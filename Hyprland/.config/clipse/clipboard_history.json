{"clipboardHistory":[{"value":"/home/karna/Downloads/.gitignore","recorded":"2025-01-03 13:24:18.724850350","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/gitignore","recorded":"2025-01-03 13:23:41.341112811","filePath":"null","pinned":false},{"value":"Venkat Branch Created\n","recorded":"2025-01-03 12:26:43.691566431","filePath":"null","pinned":false},{"value":"/mnt/Shared/CJPR/CJPR-Report/albert-base-v2\n/mnt/Shared/CJPR/CJPR-Report/albert-large-v2\n/mnt/Shared/CJPR/CJPR-Report/bert-base-uncased\n/mnt/Shared/CJPR/CJPR-Report/bert-large-uncased\n/mnt/Shared/CJPR/CJPR-Report/deberta\n/mnt/Shared/CJPR/CJPR-Report/distilbert-base-uncased\n/mnt/Shared/CJPR/CJPR-Report/Graphical Abstract\n/mnt/Shared/CJPR/CJPR-Report/roberta-base\n/mnt/Shared/CJPR/CJPR-Report/roberta-large\n/mnt/Shared/CJPR/CJPR-Report/xlnet-base-cased\n/mnt/Shared/CJPR/CJPR-Report/xlnet-large-cased\n/mnt/Shared/CJPR/CJPR-Report/ALBERT_Accuracy.pdf\n/mnt/Shared/CJPR/CJPR-Report/ALBERT_Loss.pdf\n/mnt/Shared/CJPR/CJPR-Report/Case_Distribution.pdf\n/mnt/Shared/CJPR/CJPR-Report/CJPR_1.bib\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.aux\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.bbl\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.blg\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.fdb_latexmk\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.fls\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.log\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.out\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.pdf\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.spl\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.synctex.gz\n/mnt/Shared/CJPR/CJPR-Report/CJPR_Main_NEW.tex\n/mnt/Shared/CJPR/CJPR-Report/DeBERTa_Accuracy.pdf\n/mnt/Shared/CJPR/CJPR-Report/DeBERTa_Loss.pdf\n/mnt/Shared/CJPR/CJPR-Report/Docker.pdf\n/mnt/Shared/CJPR/CJPR-Report/elsarticle.cls\n/mnt/Shared/CJPR/CJPR-Report/elsarticle-harv.bst\n/mnt/Shared/CJPR/CJPR-Report/ESWA-D-24-12938.pdf\n/mnt/Shared/CJPR/CJPR-Report/HXBR.pdf\n/mnt/Shared/CJPR/CJPR-Report/LICENSE\n/mnt/Shared/CJPR/CJPR-Report/numcompress.sty\n/mnt/Shared/CJPR/CJPR-Report/README.md\n/mnt/Shared/CJPR/CJPR-Report/RFCF.pdf\n/mnt/Shared/CJPR/CJPR-Report/RFROC.pdf\n/mnt/Shared/CJPR/CJPR-Report/R_GPU.pdf\n/mnt/Shared/CJPR/CJPR-Report/RoBERTa_Architecture.pdf\n/mnt/Shared/CJPR/CJPR-Report/rotating.sty\n/mnt/Shared/CJPR/CJPR-Report/rough.aux\n/mnt/Shared/CJPR/CJPR-Report/rough.fdb_latexmk\n/mnt/Shared/CJPR/CJPR-Report/rough.fls\n/mnt/Shared/CJPR/CJPR-Report/rough.log\n/mnt/Shared/CJPR/CJPR-Report/rough.out\n/mnt/Shared/CJPR/CJPR-Report/rough.pdf\n/mnt/Shared/CJPR/CJPR-Report/rough.synctex.gz\n/mnt/Shared/CJPR/CJPR-Report/rough.tex\n/mnt/Shared/CJPR/CJPR-Report/R_TPU.pdf\n/mnt/Shared/CJPR/CJPR-Report/Sigmoid_Function.pdf\n/mnt/Shared/CJPR/CJPR-Report/XGCF.pdf\n/mnt/Shared/CJPR/CJPR-Report/XGROC.pdf","recorded":"2025-01-03 12:15:53.920677516","filePath":"null","pinned":false},{"value":"git@github.com:Chaganti-Reddy/cjpr-report-latest.git","recorded":"2025-01-03 12:00:14.170156957","filePath":"null","pinned":false},{"value":"https://github.com/Chaganti-Reddy/cjpr-report-latest/","recorded":"2025-01-03 11:58:18.588767749","filePath":"null","pinned":false},{"value":"Aa2005233711@@","recorded":"2025-01-03 11:55:49.585676036","filePath":"null","pinned":false},{"value":"https://meet.google.com/wdi-oecz-jes","recorded":"2025-01-03 11:42:17.893874946","filePath":"null","pinned":false},{"value":"https://meet.google.com/wdi-oecz-jes?pli=1","recorded":"2025-01-03 11:42:11.464570865","filePath":"null","pinned":false},{"value":"Day-06 of 40 Days of Kubernetes with blog","recorded":"2025-01-03 10:59:48.962260509","filePath":"null","pinned":false},{"value":"csrftoken=K86QQ5Tyzu8JMC5yTT8wvQxvDvTk32RQ2sgVCgOZpos7tqzLXHjQV0MReCqUgeP3; cf_clearance=IT.rzrM85wMD2CYfYMI08flMGWWVlWRjlcTEJxOhjnM-1734689244-1.2.1.1-LcvPMKm0Lf09OpLicA_RfDQxmPPZ94imYfQ6fOoQ5kLkmVjrIwL.F6ydHAswmNhHVYKGvbDDu1NIv09RtRHNDbbxFEM9ZdbJVgVesThNGfR5X3GSqAT6JG7m6ktgP6tZbQ0W0YLKdwYZs0UsNl1AOg4LLXf.ddM7q.gmJAMVlXk9aNwvpfsiv8S_hmDVHFpfU4MrkbOjjfkGxKruIJ3dUgTRIgY2IQ0d6zuk0OHO41551LnTFjkyb.i.SbYT5scS89XRIXbwkdnW00EjP0.3OUqjHAJBVPhicD97Q_XstuhYMHxUkvEl378t6rm_vajWCBND5qT.Ty_5bRRGiKO5QGcCI4SoPPJlJefvRDhGwVzeA5QRK9HJiffztiYvattA; LEETCODE_SESSION=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJfYXV0aF91c2VyX2lkIjoiNTg1NzE1OCIsIl9hdXRoX3VzZXJfYmFja2VuZCI6ImFsbGF1dGguYWNjb3VudC5hdXRoX2JhY2tlbmRzLkF1dGhlbnRpY2F0aW9uQmFja2VuZCIsIl9hdXRoX3VzZXJfaGFzaCI6ImIyYjRmNDExMzY0Zjg3ZDQwYTQ2ZmI3NDlhYzlkODg5YjViODhkMzYzYmRhMTkzZTc2YzZiODMzNDE5NjAzZmEiLCJzZXNzaW9uX3V1aWQiOiJjYWFmM2MwNCIsImlkIjo1ODU3MTU4LCJlbWFpbCI6ImNoYWdhbnRpdmVua2F0YXJhbWlyZWRkeTFAZ21haWwuY29tIiwidXNlcm5hbWUiOiJjaGFnYW50aXZlbmthdGFyYW1pcmVkZHkxIiwidXNlcl9zbHVnIjoiY2hhZ2FudGl2ZW5rYXRhcmFtaXJlZGR5MSIsImF2YXRhciI6Imh0dHBzOi8vYXNzZXRzLmxlZXRjb2RlLmNvbS91c2Vycy9hdmF0YXJzL2F2YXRhcl8xNjU0MTY3NDY0LnBuZyIsInJlZnJlc2hlZF9hdCI6MTczNTgxOTI4MiwiaXAiOiIxNzUuMTAxLjY4LjU1IiwiaWRlbnRpdHkiOiI2Y2IzYTcxZmUxY2RkN2Y0OTEyMTkyM2E5Mzk4MDA2ZSIsImRldmljZV93aXRoX2lwIjpbImExNTYyYWQzODk2M2MxYmVhMWE5NDcxZjFhMTU4NTc0IiwiMTc1LjEwMS42OC41NSJdLCJfc2Vzc2lvbl9leHBpcnkiOjEyMDk2MDB9.MHC0CS7zzD0TeDPCEnE8Mt47IN2DATfdxotirYwVPLw; INGRESSCOOKIE=04ea8f59b8f8291a012bc1031c29569c|8e0876c7c1464cc0ac96bc2edceabd27; c_a_u=\"Y2hhZ2FudGl2ZW5rYXRhcmFtaXJlZGR5MQ==:1tKxin:vGyV6-eewYv5z8nBH7rZL_fJ9cJMUPPvbMhhmN_Cfng\"; messages=W1siX19qc29uX21lc3NhZ2UiLDAsMjUsIlN1Y2Nlc3NmdWxseSBzaWduZWQgaW4gYXMgQ2hhZ2FudGkgUmVkZHkuIl1d:1tOZun:PESXPYNYvimt-aVGeQGE6XLV080arL3EdqbaC20VfuU; _dd_s=rum=0\u0026expire=1735820190008; ip_check=(false, \"175.101.68.55\")","recorded":"2025-01-02 17:32:07.759700547","filePath":"null","pinned":false},{"value":"cf_clearance=IT.rzrM85wMD2CYfYMI08flMGWWVlWRjlcTEJxOhjnM-1734689244-1.2.1.1-LcvPMKm0Lf09OpLicA_RfDQxmPPZ94imYfQ6fOoQ5kLkmVjrIwL.F6ydHAswmNhHVYKGvbDDu1NIv09RtRHNDbbxFEM9ZdbJVgVesThNGfR5X3GSqAT6JG7m6ktgP6tZbQ0W0YLKdwYZs0UsNl1AOg4LLXf.ddM7q.gmJAMVlXk9aNwvpfsiv8S_hmDVHFpfU4MrkbOjjfkGxKruIJ3dUgTRIgY2IQ0d6zuk0OHO41551LnTFjkyb.i.SbYT5scS89XRIXbwkdnW00EjP0.3OUqjHAJBVPhicD97Q_XstuhYMHxUkvEl378t6rm_vajWCBND5qT.Ty_5bRRGiKO5QGcCI4SoPPJlJefvRDhGwVzeA5QRK9HJiffztiYvattA; LEETCODE_SESSION=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJfYXV0aF91c2VyX2lkIjoiNTg1NzE1OCIsIl9hdXRoX3VzZXJfYmFja2VuZCI6ImFsbGF1dGguYWNjb3VudC5hdXRoX2JhY2tlbmRzLkF1dGhlbnRpY2F0aW9uQmFja2VuZCIsIl9hdXRoX3VzZXJfaGFzaCI6ImIyYjRmNDExMzY0Zjg3ZDQwYTQ2ZmI3NDlhYzlkODg5YjViODhkMzYzYmRhMTkzZTc2YzZiODMzNDE5NjAzZmEiLCJzZXNzaW9uX3V1aWQiOiJjYWFmM2MwNCIsImlkIjo1ODU3MTU4LCJlbWFpbCI6ImNoYWdhbnRpdmVua2F0YXJhbWlyZWRkeTFAZ21haWwuY29tIiwidXNlcm5hbWUiOiJjaGFnYW50aXZlbmthdGFyYW1pcmVkZHkxIiwidXNlcl9zbHVnIjoiY2hhZ2FudGl2ZW5rYXRhcmFtaXJlZGR5MSIsImF2YXRhciI6Imh0dHBzOi8vYXNzZXRzLmxlZXRjb2RlLmNvbS91c2Vycy9hdmF0YXJzL2F2YXRhcl8xNjU0MTY3NDY0LnBuZyIsInJlZnJlc2hlZF9hdCI6MTczNTgxOTI4MiwiaXAiOiIxNzUuMTAxLjY4LjU1IiwiaWRlbnRpdHkiOiI2Y2IzYTcxZmUxY2RkN2Y0OTEyMTkyM2E5Mzk4MDA2ZSIsImRldmljZV93aXRoX2lwIjpbImExNTYyYWQzODk2M2MxYmVhMWE5NDcxZjFhMTU4NTc0IiwiMTc1LjEwMS42OC41NSJdLCJfc2Vzc2lvbl9leHBpcnkiOjEyMDk2MDB9.MHC0CS7zzD0TeDPCEnE8Mt47IN2DATfdxotirYwVPLw; messages=W1siX19qc29uX21lc3NhZ2UiLDAsMjUsIlN1Y2Nlc3NmdWxseSBzaWduZWQgaW4gYXMgQ2hhZ2FudGkgUmVkZHkuIl1d:1tOZun:PESXPYNYvimt-aVGeQGE6XLV080arL3EdqbaC20VfuU","recorded":"2025-01-02 17:31:51.454491691","filePath":"null","pinned":false},{"value":"bind = $mainMod SHIFT, W, exec, zen-browser --private-window\n","recorded":"2025-01-02 17:25:19.898928873","filePath":"null","pinned":false},{"value":"kubectl config get-contexts","recorded":"2025-01-02 17:13:45.721904238","filePath":"null","pinned":false},{"value":"kubectl cluster-info --context kind-test-single\n\nNot sure what to do next? üòÖ  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n(base) \u003e\u003e\u003e  ÔÅª  ~  kubectl cluster-info --context kind-test-single\nKubernetes control plane is running at https://127.0.0.1:34267\nCoreDNS is running at https://127.0.0.1:34267/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n(base) \u003e\u003e\u003e  ÔÅª  ~  ","recorded":"2025-01-02 17:13:18.942472733","filePath":"null","pinned":false},{"value":"kubectl cluster-info --context kind-test-single","recorded":"2025-01-02 17:12:19.737611026","filePath":"null","pinned":false},{"value":"Error response from daemon: conflict: unable to remove repository reference \"hello-world:latest\" (must force) - container 407722ab4431 is using its referenced image d2c94e258dcb","recorded":"2025-01-02 17:12:04.918340953","filePath":"null","pinned":false},{"value":"bind = $mainMod SHIFT, right, resizeactive, 10 0\nbind = $mainMod SHIFT, left, resizeactive, -10 0\nbind = $mainMod SHIFT, up, resizeactive, 0 -10\nbind = $mainMod SHIFT, down, resizeactive, 0 10","recorded":"2025-01-02 17:05:56.156548818","filePath":"null","pinned":false},{"value":"bind = $mainMod+CTRL, down, movewindow, d\n","recorded":"2025-01-02 17:03:31.200934372","filePath":"null","pinned":false},{"value":"bind = $mainMod+CTRL, up, layoutmsg, swapnext\nbind = $mainMod+CTRL, down, layoutmsg, swapnext prevbind","recorded":"2025-01-02 17:01:26.467445546","filePath":"null","pinned":false},{"value":"ü§£","recorded":"2025-01-02 16:59:30.981869870","filePath":"null","pinned":false},{"value":"# üåê Kubernetes API Server: The Cluster's Gateway\n\nThe **API Server** is the heart of Kubernetes, acting as the primary interface for all cluster operations. It processes requests from users, internal components, and external systems, ensuring the cluster operates smoothly and securely.\n\n---\n\n## üöÄ Key Roles and Responsibilities\n\n1. **Central Communication Hub**:  \n   - Serves as the single point of interaction for cluster management.  \n   - Handles requests from `kubectl`, dashboards, and automated systems.  \n\n2. **Authentication and Authorization**:  \n   - Verifies who you are (**authentication**) and what you're allowed to do (**authorization**).  \n\n3. **Validation**:  \n   - Ensures incoming requests are correctly structured and logical.  \n\n4. **Persistence**:  \n   - Stores and retrieves the cluster‚Äôs state in **etcd**, maintaining a single source of truth.  \n\n5. **Cluster State Management**:  \n   - Manages the desired and current states of the cluster resources.  \n\n6. **Real-Time Monitoring**:  \n   - Supports \"watch\" requests, enabling clients to track resource changes in real time.  \n\n7. **Load Balancing and High Availability**:  \n   - Distributes requests across multiple API Servers in high-availability setups.  \n\n---\n\n## üîÑ API Server Workflow\n\n1. **Receive Request**:  \n   - Processes requests from `kubectl`, external apps, or other components.  \n\n2. **Authentication**:  \n   - Validates the requester using tokens, certificates, or plugins.  \n\n3. **Authorization**:  \n   - Checks permissions via RBAC or other authorization mechanisms.  \n\n4. **Admission Control**:  \n   - Applies policies like resource quotas and security constraints.  \n\n5. **Validation**:  \n   - Ensures requests conform to the API schema.  \n\n6. **Persistence and Update**:  \n   - Updates **etcd** for any state changes and notifies relevant components.  \n\n7. **Response**:  \n   - Returns the result (e.g., resource details or operation status).  \n\n---\n\n## ‚ú® Features of the API Server\n\n1. **RESTful Interface**:  \n   - Offers a RESTful API for seamless programmatic interaction.  \n\n2. **Extensibility**:  \n   - Supports **Custom Resource Definitions (CRDs)** to extend Kubernetes functionality.  \n\n3. **Scalability**:  \n   - Handles high request volumes efficiently in large clusters.  \n\n4. **Self-Healing**:  \n   - Works with controllers and the scheduler to maintain cluster health.  \n\n---\n\n## üåü Example Scenarios\n\n1. **Pod Deployment**:  \n   - A `kubectl apply` command sends a request to the API Server, which validates and stores the Pod's configuration in **etcd**.  \n\n2. **Cluster Monitoring**:  \n   - A monitoring system uses the \"watch\" feature to get real-time updates on resource changes.  \n\n3. **Security Enforcement**:  \n   - The API Server ensures only authorized users can modify critical resources.  \n\n---\n\n## üõ°Ô∏è Best Practices\n\n- **Monitor Traffic**: Use tools like `kube-apiserver` metrics to analyze request patterns.  \n- **Secure Communications**: Enforce TLS and authenticate using secure tokens or certificates.  \n- **Optimize RBAC**: Regularly review and refine Role-Based Access Controls.  \n- **Scale API Servers**: Add replicas in high-traffic environments for improved performance.  \n","recorded":"2025-01-02 16:53:40.991145232","filePath":"null","pinned":false},{"value":"In many nations, the life-span of people has increased significantly. Few\nindividuals argue that old-age population create hurdles to the governments while,\nothers believe that they are of an advantage to the society. In my opinion, the\nbenefits of having senior citizens outweigh its drawbacks.\nOlder people are well experienced. They must have witnessed all the highs and\nlows of life. Hence, they are wise enough to imbibe the young generations good\nmorals, values and ethics which not only moulds them to be a good person but\nalso, makes them a better person in the society. For instance, Swami\nVivekananda grew up listening to moral stories told by his grand-mother which\nhad a great impact on his success. Additionally, senior citizens tend to suffer from\nvarious health problems. This results in increased profits for the health-care\nindustry. For example, due to over population, especially the old, Telangana\nGovernment has increased the medical hospitals in the state which are now\nrunning successfully making good profits.\nOn the other hand, the Government has to invest a lot of funds for the welfare of\nelderly people because, it has to work on schemes like, monthly pensions, rations\nand generic medicines that are available at a lower cost. For instance, huge\nproportion of the governments income is spent for the well-being of senior o\ncitizens. Furthermore, issues like shortage of medical-professionals have to be\naddressed very quickly which is an expensive affair to the government. For\ninstance, during COVID-19 pandemic, Italy having highest number of old age\npopulation had to hire medical professionals from other countries to provide\nquality health care facilities.\nIn conclusion, like the two sides of a coin having over populated old-people is both\nboon and a bane. Nonetheless, in my perspective, advantages like having them\nas a moral support to the families and increased profits in the health-care industry\nis way beneficial than the odds like increased burden on government in allocation\nof funds for welfare schemes and need for medical-professionals.","recorded":"2025-01-02 16:51:18.277974679","filePath":"null","pinned":false},{"value":"- **API SERVER** : This is the main part of the Kubernetes which takes the requests and send the data or requests from users. Using this server, clients interact with the cluster.\n\t\n\t- ### Key Roles and Responsibilities of the API Server:\n\n\t\t1. **Central Communication Hub**:\n\t\t    \n\t\t    - The API Server acts as the single point of interaction for all cluster operations. It receives requests from users (e.g., `kubectl` commands), internal cluster components, and external applications.\n\t\t2. **Authentication and Authorization**:\n\t\t    \n\t\t    - Ensures that users or components making API requests are authenticated (who you are) and authorized (what you can do).\n\t\t3. **Validation**:\n\t\t    \n\t\t    - Validates API requests to ensure they conform to the required schema and logic before processing them.\n\t\t4. **Persistence**:\n\t\t    \n\t\t    - Interacts with **etcd** (the cluster's key-value store) to store and retrieve the state of the cluster. Any updates made via the API Server are persisted in `etcd`.\n\t\t5. **Cluster State Management**:\n\t\t    \n\t\t    - Serves as the source of truth for the current state of the cluster and manages desired state declarations.\n\t\t6. **Handling Watch Requests**:\n\t\t    \n\t\t    - Supports the \"watch\" mechanism for monitoring changes in resources. Clients can watch for updates (e.g., new Pods, changes to ConfigMaps) in real time.\n\t\t7. **Load Balancing and High Availability**:\n\t\t    \n\t\t    - In a multi-master setup, API Servers are load-balanced to ensure high availability and fault tolerance.\n\t\t\n\t\t### API Server Workflow:\n\t\n\t\t1. **Receive Request**:\n\t\t\t\n\t\t\t- A request arrives at the API Server (e.g., via `kubectl`, a dashboard, or an automated system).\n\t\t2. **Authentication**:\n\t\t\t\n\t\t\t- The server verifies the identity of the requester using mechanisms like tokens, certificates, or authentication plugins.\n\t\t3. **Authorization**:\n\t\t\t\n\t\t\t- Checks if the requester has permission to perform the requested action using Role-Based Access Control (RBAC) or other mechanisms.\n\t\t4. **Admission Control**:\n\t\t\t\n\t\t\t- Admission controllers validate and modify requests according to policies (e.g., resource quotas, security policies).\n\t\t5. **Validation**:\n\t\t\t\n\t\t\t- The request is checked against the API's schema to ensure it‚Äôs correctly structured.\n\t\t6. **Persistence and Update**:\n\t\t\t\n\t\t\t- If the request modifies the cluster, the API Server updates the state in **etcd** and notifies relevant components.\n\t\t7. **Response**:\n\t\t\t\n\t\t\t- Returns the result of the request (e.g., resource details, status of operation).\n\t\n\t\t### Features:\n\t\n\t\t1. **RESTful Interface**:\n\t\t\t\n\t\t\t- The Kubernetes API Server provides a RESTful interface, making it easy to interact programmatically.\n\t\t2. **Extensibility**:\n\t\t\t\n\t\t\t- Supports custom resources (Custom Resource Definitions - CRDs), enabling you to extend Kubernetes with your own APIs.\n\t\t3. **Scalability**:\n\t\t\t\n\t\t\t- Designed to handle a large number of API requests efficiently in large-scale clusters.\n\t\t4. **Self-Healing**:\n\t\t\t\n\t\t\t- Works with the controller-manager and scheduler to reconcile the cluster's current state with its desired state.","recorded":"2025-01-02 16:44:25.662234942","filePath":"null","pinned":false},{"value":"# ‚ö° Kubernetes Scheduler: The Cluster Matchmaker\n\nThe **scheduler** is a vital part of Kubernetes' control plane, responsible for assigning Pods to the most suitable worker nodes. It ensures optimal resource utilization and enforces scheduling policies to maintain a balanced and efficient cluster.\n\n---\n\n## üöÄ Key Functions of the Scheduler\n\n1. **Node Selection**:  \n   - Chooses the best node for a Pod based on resource availability, constraints, and affinity rules.  \n\n2. **Resource Awareness**:  \n   - Ensures nodes have enough CPU, memory, and other resources to run the Pod.  \n\n3. **Policy Enforcement**:  \n   - Implements policies like:  \n     - **Node Affinity/Anti-Affinity**: Determines co-location or separation of Pods.  \n     - **Taints and Tolerations**: Matches Pods with compatible nodes.  \n     - **Pod Affinity/Anti-Affinity**: Governs scheduling relative to other Pods.  \n\n4. **Priority and Preemption**:  \n   - Honors Pod priorities and may preempt lower-priority Pods for higher-priority workloads.  \n\n5. **Topology Awareness**:  \n   - Considers zones and regions to ensure fault tolerance and reduce latency.  \n\n---\n\n## üîÑ Workflow of the Scheduler\n\n1. **Input**:  \n   - Identifies Pods that need scheduling (no assigned node).  \n\n2. **Filtering**:  \n   - Eliminates unsuitable nodes based on:  \n     - Node conditions (e.g., memory pressure, disk issues).  \n     - Resource requests vs. available capacity.  \n     - Policies like taints, tolerations, and affinity rules.  \n\n3. **Scoring**:  \n   - Scores remaining nodes by evaluating factors like:  \n     - Resource balance.  \n     - Pod affinity and anti-affinity preferences.  \n\n4. **Binding**:  \n   - Assigns the Pod to the highest-scoring node and updates the API Server.  \n\n---\n\n## üõ† Extensibility\n\nThe Kubernetes Scheduler is highly customizable:  \n\n1. **Custom Schedulers**:  \n   - Create a scheduler tailored to specific needs.  \n\n2. **Scheduling Framework**:  \n   - Use plugins for custom filtering, scoring, or additional scheduling logic.  \n\n---\n\n## üåü Example Scenarios\n\n1. **Resource Constraints**:  \n   - A Pod requesting 4 CPUs and 8GB memory is scheduled to a node with adequate resources.  \n\n2. **Affinity Rules**:  \n   - A Pod with **Node Affinity** is placed on a preferred node (e.g., in a specific zone).  \n\n3. **Priority Handling**:  \n   - A high-priority Pod preempts a low-priority Pod when resources are scarce.  \n\n---\n\n## üõ°Ô∏è Best Practices\n\n- Monitor scheduler performance and logs for bottlenecks or errors.  \n- Use **taints** and **tolerations** to optimize resource usage.  \n- Implement **custom plugins** for unique scheduling requirements.  \n- Regularly review **Pod priorities** to ensure critical workloads are prioritized.  \n","recorded":"2025-01-02 16:43:36.385170987","filePath":"null","pinned":false},{"value":"**SCHEDULAR** : In Kubernetes, the **scheduler** is a critical component of the control plane, running on the master node. It is responsible for assigning workloads, encapsulated as **pods, to appropriate worker nodes in the cluster. The scheduler ensures that Pods are allocated to nodes that meet the requirements specified in the Pod's definition and optimizes resource utilization across the cluster.\n\t\n\t- ### Key Functions of the Kubernetes Scheduler:\n\n\t\t1. **Node Selection**:\n\t\t\t\n\t\t\t- The scheduler selects a node for a Pod based on various criteria such as resource availability (CPU, memory), constraints, and affinity/anti-affinity rules.\n\t\t2. **Resource Awareness**:\n\t\t\t\n\t\t\t- It evaluates whether a node has sufficient resources (e.g., CPU, memory) to run the Pod.\n\t\t3. **Policy Enforcement**:\n\t\t\t\n\t\t\t- It respects policies such as:\n\t\t\t\t- **Node Affinity/Anti-Affinity**: Rules that specify whether Pods should be co-located or separated.\n\t\t\t\t- **Taints and Tolerations**: Mechanisms to avoid or tolerate nodes with specific taints.\n\t\t\t\t- **Pod Affinity/Anti-Affinity**: Rules regarding how Pods should be scheduled relative to each other.\n\t\t4. **Priority and Preemption**:\n\t\t\t\n\t\t\t- It considers Pod priority classes and may preempt lower-priority Pods to make room for higher-priority ones if the cluster is running out of resources.\n\t\t5. **Topology Awareness**:\n\t\t\t\n\t\t\t- It can consider topology constraints (e.g., zones, regions) to ensure fault tolerance and optimize network latency.\n\t\n\t\t### Workflow of the Scheduler:\n\n\t\t1. **Input**:\n\t\t\t\n\t\t\t- A Pod without an assigned node.\n\t\t2. **Filtering**:\n\t\t\t\n\t\t\t- The scheduler filters nodes that cannot run the Pod based on:\n\t\t\t\t- Node conditions (e.g., out of memory, disk pressure).\n\t\t\t\t- Resource requests vs. available capacity.\n\t\t\t\t- Constraints like taints, tolerations, and affinity rules.\n\t\t3. **Scoring**:\n\t\t\t\n\t\t\t- The scheduler scores the remaining nodes based on various factors such as resource balance, affinity, and anti-affinity preferences.\n\t\t4. **Binding**:\n\t\t\t\n\t\t\t- The scheduler assigns the Pod to the node with the highest score and updates the API server with the decision.\n\n\t\t### Extensibility:\n\n\t\tKubernetes Scheduler is highly extensible:\n\t\t\n\t\t- **Custom Schedulers**: You can implement a custom scheduler if the default one doesn't meet specific requirements.\n\t\t- **Scheduling Framework**: Allows plugins to be used for custom filtering, scoring, and other scheduling logic.","recorded":"2025-01-02 16:42:54.594929562","filePath":"null","pinned":false},{"value":"**LAYMAN üòÅ** : Generally it is a database to store the values, but its specialty is to store in JSON(key: value) format which helps to make custom keys for custom values unlike in the RDBMS we have to specify a key for every value. ","recorded":"2025-01-02 16:40:27.717556259","filePath":"null","pinned":false},{"value":"# üóùÔ∏è etcd: The Backbone of Kubernetes\n\n**etcd** is Kubernetes' **key-value store** and the **source of truth** for cluster data. It's distributed, highly available, and ensures data consistency.  \n\n---\n\n## üöÄ What Makes etcd Special?\n\n1. **Strong Consistency**:  \n   Uses the **Raft consensus algorithm** for consistent data across nodes.  \n\n2. **High Availability**:  \n   Distributed design ensures no single point of failure.  \n\n3. **Durable**:  \n   Data is persisted to disk, surviving restarts or failures.  \n\n4. **Low Latency**:  \n   Optimized for real-time cluster state updates.  \n\n5. **Scalable**:  \n   Handles high request volumes, perfect for large clusters.  \n\n---\n\n## üõ† Role of etcd in Kubernetes\n\n1. **Cluster State Storage**:  \n   Stores everything: Nodes, Pods, ConfigMaps, Secrets, CRDs, etc.  \n\n2. **Leader Election**:  \n   Helps control plane components elect a leader (e.g., Scheduler).  \n\n3. **Watch Mechanism**:  \n   Enables components to react instantly to state changes.  \n\n4. **Backup \u0026 Restore**:  \n   Critical for disaster recovery.\n\n---\n\n## üîç How etcd Works\n\n1. **Key-Value Store**:  \n   Stores data like:  \n   - Key: `/registry/pods/default/my-pod`  \n   - Value: Pod's JSON/YAML configuration.  \n\n2. **API Server Interaction**:  \n   - API Server writes state to etcd.  \n   - Other components read from it and act.  \n\n3. **Replication \u0026 Consensus**:  \n   - Data is replicated across nodes.  \n   - Writes go to the leader; followers stay in sync.\n\n---\n\n## üîí Deployment Tips\n\n1. **HA Setup**:  \n   - Use 3 or 5 nodes for quorum-based fault tolerance.  \n\n2. **Secure Communication**:  \n   - Encrypt traffic with TLS.  \n\n3. **Fast Storage**:  \n   - Use SSDs for performance.  \n\n4. **Regular Backups**:  \n   - Tools: `etcdctl`, Kubernetes backup mechanisms.  \n\n---\n\n## üåü Best Practices\n\n- Monitor health: Disk latency, memory, and leader elections.  \n- Regular updates to avoid vulnerabilities.  \n- Secure access with strong authentication.  \n\n---\n\n# ‚öôÔ∏è Kubernetes Controller Manager\n\nThe **Controller Manager** ensures Kubernetes runs like a well-oiled machine! It coordinates controllers to maintain the cluster's **desired state**.  \n\n---\n\n## üß© Key Roles\n\n1. **State Reconciliation**:  \n   Keeps the actual state aligned with the desired state (e.g., reschedules crashed Pods).  \n\n2. **Controller Aggregation**:  \n   Combines multiple controllers into a single process for simplicity.  \n\n3. **Resource Management**:  \n   Automates tasks like scaling, scheduling, and repairs.  \n\n---\n\n## üõ† Controllers Overview\n\n1. **Node Controller**:  \n   Handles node health and reschedules Pods from failed nodes.  \n\n2. **Replication Controller**:  \n   Ensures the desired number of Pod replicas are running.  \n\n3. **Deployment Controller**:  \n   Manages rollouts, updates, and rollbacks of Deployments.  \n\n4. **DaemonSet Controller**:  \n   Ensures a Pod copy runs on all/specific nodes.  \n\n5. **Job Controller**:  \n   Ensures Pods complete tasks for Jobs.  \n\n6. **Endpoint Controller**:  \n   Updates IP addresses for Services.  \n\n7. **Service Account Controller**:  \n   Manages default ServiceAccounts and their secrets.  \n\n8. **Resource Quota Controller**:  \n   Enforces resource usage limits in namespaces.  \n\n9. **PV Controller**:  \n   Manages Persistent Volumes and bindings to PVCs.  \n\n---\n\n## üîÑ Workflow of the Controller Manager\n\n1. **Watch**:  \n   Monitors desired state via API Server.  \n\n2. **Reconcile**:  \n   Fixes mismatches (e.g., recreates deleted Pods).  \n\n3. **Update State**:  \n   Updates the cluster state and persists changes in etcd.  \n\n---\n\n## üîë Features\n\n- **Leader Election**: Ensures only one active instance in HA setups.  \n- **Modularity**: Independent controllers simplify debugging.  \n- **Extendability**: Add custom controllers for specialized tasks.  \n\n---\n\n## üåü Example Scenarios\n\n1. **Node Failure**:  \n   Node Controller detects a failure, marks it \"NotReady,\" and reschedules Pods.  \n\n2. **Replica Management**:  \n   If a Pod in a Deployment crashes, the Replication Controller creates a new one.  \n\n3. **Resource Binding**:  \n   The PV Controller binds a PVC to an available PV.  \n\n---\n\n## üõ† Best Practices\n\n- **Monitor Logs**: Ensure controllers are functioning properly.  \n- **Enable Leader Election**: Avoid conflicts in HA setups.  \n- **Develop Custom Controllers**: Extend Kubernetes for unique workflows.  \n","recorded":"2025-01-02 16:39:48.955308755","filePath":"null","pinned":false},{"value":"## üöÄ What Makes etcd Special?\n\n1. **Strong Consistency**:  \n   Uses the **Raft consensus algorithm** for consistent data across nodes.  \n\n2. **High Availability**:  \n   Distributed design ensures no single point of failure.  \n\n3. **Durable**:  \n   Data is persisted to disk, surviving restarts or failures.  \n\n4. **Low Latency**:  \n   Optimized for real-time cluster state updates.  \n\n5. **Scalable**:  \n   Handles high request volumes, perfect for large clusters.  \n\n---\n\n## üõ† Role of etcd in Kubernetes\n\n1. **Cluster State Storage**:  \n   Stores everything: Nodes, Pods, ConfigMaps, Secrets, CRDs, etc.  \n\n2. **Leader Election**:  \n   Helps control plane components elect a leader (e.g., Scheduler).  \n\n3. **Watch Mechanism**:  \n   Enables components to react instantly to state changes.  \n\n4. **Backup \u0026 Restore**:  \n   Critical for disaster recovery.\n\n---\n\n## üîç How etcd Works\n\n1. **Key-Value Store**:  \n   Stores data like:  \n   - Key: `/registry/pods/default/my-pod`  \n   - Value: Pod's JSON/YAML configuration.  \n\n2. **API Server Interaction**:  \n   - API Server writes state to etcd.  \n   - Other components read from it and act.  \n\n3. **Replication \u0026 Consensus**:  \n   - Data is replicated across nodes.  \n   - Writes go to the leader; followers stay in sync.\n\n---\n\n## üîí Deployment Tips\n\n1. **HA Setup**:  \n   - Use 3 or 5 nodes for quorum-based fault tolerance.  \n\n2. **Secure Communication**:  \n   - Encrypt traffic with TLS.  \n\n3. **Fast Storage**:  \n   - Use SSDs for performance.  \n\n4. **Regular Backups**:  \n   - Tools: `etcdctl`, Kubernetes backup mechanisms.  \n\n---\n\n## üåü Best Practices\n\n- Monitor health: Disk latency, memory, and leader elections.  \n- Regular updates to avoid vulnerabilities.  \n- Secure access with strong authentication.  ","recorded":"2025-01-02 16:39:41.339159090","filePath":"null","pinned":false},{"value":"- **ETCD** : **etcd** is a distributed, highly available, and consistent key-value store that serves as the **backbone** of Kubernetes' control plane. It stores the entire state and configuration of the Kubernetes cluster, making it a critical component for cluster operation and reliability.\n\n\t**LAYMAN üòÅ** : Generally it is a database to store the values, but its specialty is to store in JSON(key: value) format which helps to make custom keys for custom values unlike in the RDBMS we have to specify a key for every value. \n\n\t### **Key Features of etcd:**\n\n\t1. **Strong Consistency**:\n\t\t\n\t\t- etcd uses the **Raft consensus algorithm** to ensure that data is strongly consistent across all nodes in the etcd cluster.\n\t2. **High Availability**:\n\t\t\n\t\t- By running as a distributed system, etcd ensures high availability through leader election and replication. If one node fails, other nodes in the etcd cluster continue to operate.\n\t3. **Durability**:\n\t\t\n\t\t- All data in etcd is persisted to disk, ensuring durability even in the event of node restarts or failures.\n\t4. **Low Latency**:\n\t\t\n\t\t- Optimized for fast reads and writes, making it suitable for managing cluster state in real time.\n\t5. **Scalability**:\n\t\t\n\t\t- Can handle high volumes of requests, making it ideal for large-scale clusters.\n\n\t### **Role of etcd in Kubernetes:**\n\n\t1. **Cluster State Storage**:\n\t\t\n\t\t- etcd acts as the **source of truth** for the Kubernetes cluster, storing all cluster data, including:\n\t\t\t- Node and Pod information\n\t\t\t- Configurations (e.g., ConfigMaps, Secrets)\n\t\t\t- Persistent Volume Claims (PVCs)\n\t\t\t- Networking information (e.g., Services, Endpoints)\n\t\t\t- Custom Resource Definitions (CRDs)\n\t2. **Leader Election**:\n\t\t\n\t\t- Used for leader election among control plane components like the Kubernetes Controller Manager and Scheduler.\n\t3. **Watch Mechanism**:\n\t\t\n\t\t- etcd supports a **watch** feature, enabling Kubernetes components to subscribe to changes in cluster state and react in real time.\n\t4. **Backup and Restore**:\n\t\t\n\t\t- Since it stores the cluster‚Äôs state, regular backups of etcd are crucial for disaster recovery.\n\n\t### **How etcd Works in Kubernetes:**\n\n\t1. **Data Storage**:\n\t\t\n\t\t- All objects and configurations are stored as key-value pairs. For example:\n\t\t\t- Key: `/registry/pods/default/my-pod`\n\t\t\t- Value: Detailed JSON/YAML representation of the Pod's configuration.\n\t2. **Interaction with API Server**:\n\t\t\n\t\t- The API Server interacts with etcd to retrieve or persist cluster state.\n\t\t- For example:\n\t\t\t- When a new Pod is created, the API Server writes the Pod's desired state to etcd.\n\t\t\t- Other control plane components read this state and act accordingly (e.g., the scheduler assigns the Pod to a node).\n\t3. **Replication and Consensus**:\n\t\t\n\t\t- etcd replicates data across all its nodes and uses the Raft consensus algorithm to ensure consistency. Only one node acts as the leader, and write requests are sent to the leader.\n\n\t### **Deployment in Kubernetes:**\n\n\t- **Embedded in the Control Plane**:\n\t\t\n\t\t- etcd is typically deployed alongside other control plane components in a highly available configuration.\n\t\t- A typical Kubernetes HA setup includes multiple etcd instances (e.g., 3 or 5) for fault tolerance.\n\t- **Secured Communication**:\n\t\t\n\t\t- Communication between etcd and other components (like the API Server) is secured using Transport Layer Security (TLS).\n\n\t### **Best Practices for etcd:**\n\n\t1. **Regular Backups**:\n\t\t\n\t\t- Use tools like `etcdctl` or Kubernetes' built-in backup mechanisms to regularly back up etcd data.\n\t2. **Cluster Size**:\n\t\t\n\t\t- Deploy an odd number of etcd nodes (e.g., 3, 5) to ensure quorum-based consensus.\n\t3. **Monitor Health**:\n\t\t\n\t\t- Monitor metrics like disk latency, memory usage, and leader election frequency to ensure etcd is healthy.\n\t4. **Use Fast Storage**:\n\t\t\n\t\t- Store etcd data on high-performance SSDs to minimize latency and ensure reliable performance.\n\t5. **Secure Configuration**:\n\t\t\n\t\t- Use strong authentication and encryption (TLS) to secure etcd communications and data.\n\n- **CONTROLLER MANAGER**: The **Controller Manager** is a core component of the Kubernetes control plane responsible for managing various controllers that regulate the state of the cluster. Each controller is a loop that monitors the cluster's desired state (as defined in etcd via the API Server) and works to reconcile it with the actual state.\n\n\t### Key Roles of the Kubernetes Controller Manager:\n\t\n\t1. **State Reconciliation**:\n\t    \n\t    - Ensures the actual state of the cluster matches the desired state defined by users and stored in etcd. For example, if a Pod crashes, a controller ensures it's rescheduled.\n\t2. **Controller Coordination**:\n\t    \n\t    - The Controller Manager aggregates several individual controllers into a single binary, simplifying operations and resource management.\n\t3. **Resource Management**:\n\t    \n\t    - Manages resources like nodes, Pods, ReplicationControllers, and more by automating operations such as scaling, scheduling, and repairing.\n\t\n\t### **Types of Controllers in the Controller Manager:**\n\t\n\t1. **Node Controller**:\n\t    \n\t    - Monitors the health of nodes.\n\t    - Detects node failures and triggers actions like marking the node as \"NotReady\" and rescheduling Pods elsewhere.\n\t2. **Replication Controller**:\n\t    \n\t    - Ensures that the desired number of Pod replicas for a ReplicationController or ReplicaSet exists at any given time.\n\t3. **Deployment Controller**:\n\t    \n\t    - Manages the rollout and scaling of Deployments.\n\t    - Handles rolling updates and rollbacks.\n\t4. **DaemonSet Controller**:\n\t    \n\t    - Ensures that a copy of a specific Pod runs on all (or a subset of) nodes.\n\t5. **Job Controller**:\n\t    \n\t    - Manages Job objects, ensuring Pods complete their tasks successfully.\n\t6. **Endpoint Controller**:\n\t    \n\t    - Populates the Endpoints object with the IP addresses of Pods associated with a Service.\n\t7. **Service Account Controller**:\n\t    \n\t    - Creates default ServiceAccounts for namespaces and manages their secrets.\n\t8. **Resource Quota Controller**:\n\t    \n\t    - Ensures that resource usage within a namespace does not exceed predefined quotas.\n\t9. **Persistent Volume (PV) Controller**:\n\t    \n\t    - Manages the lifecycle of Persistent Volumes and their binding to Persistent Volume Claims (PVCs).\n\t\n\t### Workflow of the Controller Manager:\n\t\n\t1. **Watch the Cluster State**:\n\t    \n\t    - The Controller Manager communicates with the API Server to \"watch\" for changes in the cluster's desired state.\n\t2. **Reconcile State**:\n\t    \n\t    - If there‚Äôs a mismatch between the desired state and the actual state (e.g., a Pod is deleted or a node becomes unhealthy), the corresponding controller takes action.\n\t3. **Update Cluster State**:\n\t    \n\t    - The controller performs the necessary operations (e.g., creating a new Pod, unbinding a PVC) and updates the API Server, which then persists changes to etcd.\n\t\n\t### Deployment:\n\t\n\t- The Kubernetes Controller Manager runs as a single process but includes multiple controllers.\n\t- In High Availability (HA) setups, it runs on multiple master nodes, but **leader election** ensures only one instance is active at a time.\n\t\n\t### Features:\n\t\n\t1. **Leader Election**:\n\t    \n\t    - Ensures high availability by electing a single active Controller Manager in an HA setup.\n\t2. **Extendability**:\n\t    \n\t    - Kubernetes allows custom controllers to extend cluster functionality (e.g., for managing custom resources or workflows).\n\t3. **Modular Design**:\n\t    \n\t    - Individual controllers work independently, making the system modular and easier to debug or extend.\n\t\n\t### Example Scenarios:\n\t\n\t1. **Node Failure**:\n\t    \n\t    - The Node Controller detects that a node is down.\n\t    - It marks the node as \"NotReady\" and ensures all Pods on that node are rescheduled on healthy nodes.\n\t2. **Replica Management**:\n\t    \n\t    - A Deployment specifies that 5 replicas of a Pod should run.\n\t    - If one Pod crashes, the Replication Controller ensures a new Pod is created to maintain 5 replicas.\n\t3. **Resource Binding**:\n\t    \n\t    - A user creates a Persistent Volume Claim (PVC).\n\t    - The Persistent Volume Controller binds the PVC to an available Persistent Volume (PV).\n\t\n\t### Best Practices:\n\t\n\t1. **Monitoring**:\n\t    \n\t    - Monitor logs and metrics from the Controller Manager to ensure controllers are reconciling state effectively.\n\t2. **Leader Election**:\n\t    \n\t    - Use leader election to avoid conflicts in HA setups.\n\t3. **Custom Controllers**:\n\t    \n\t    - For specialized workflows, develop custom controllers using Kubernetes' controller-runtime library.","recorded":"2025-01-02 16:38:19.826718371","filePath":"null","pinned":false},{"value":"# üöÄ Worker Node in Kubernetes\n\nA **Worker Node** is where your **apps live and thrive** in Kubernetes! It's the powerhouse that runs Pods, hosts containers, and manages resources. Let's break it down!  \n\n---\n\n## üß© Key Components\n\n1. **Kubelet**:  \n   - The worker node's **brain**!  \n   - Talks to the API server, ensures Pods are running, and reports node status.  \n\n2. **Container Runtime**:  \n   - The **engine** that runs your containers.  \n   - Examples: `Docker`, `containerd`, `CRI-O`.\n\n3. **Kube-Proxy**:  \n   - The **networking wizard**!  \n   - Routes traffic to/from Pods and handles load balancing.  \n\n4. **Operating System**:  \n   - The base for all Kubernetes magic!  \n   - Popular choices: Ubuntu, CentOS, CoreOS.\n\n---\n\n## üéØ Responsibilities\n\n1. **Host Pods**: Runs app workloads.  \n2. **Networking**: Enables inter-Pod and external communication.  \n3. **Monitor Health**: Reports resource usage \u0026 status.  \n4. **Scale \u0026 Load Balance**: Shares workloads with other nodes.\n\n---\n\n## ‚ö° Workflow in Action\n\n1. **Pod Scheduling**: Scheduler assigns Pods ‚Üí Kubelet sets them up.  \n2. **Run Containers**: Container runtime pulls images and launches containers.  \n3. **Allocate Resources**: Pods get CPU, memory, etc., as per their needs.  \n4. **Networking**: Kube-proxy ensures smooth traffic flow.  \n5. **Health Check**: Kubelet monitors containers and restarts them if needed.\n\n---\n\n## üåü Features\n\n- **Scalable**: Add more nodes to handle more workloads.  \n- **Fault-Tolerant**: Failing nodes? Pods are rescheduled elsewhere.  \n- **Resource Isolation**: Keeps Pods secure using namespaces and cgroups.  \n- **Smart Resource Management**: Efficient and stable resource usage.  \n\n---\n\n## üõ† Deployment \u0026 Maintenance Tips\n\n- **Node Labels \u0026 Taints**: Organize workloads intelligently (e.g., GPU nodes).  \n- **Monitoring**: Use tools like Prometheus/Grafana for insights.  \n- **Upgrades**: Keep OS and Kubernetes updated for security and stability.  \n- **Autoscaling**: Ensure high availability with auto-scaling and health checks.  \n\n---\n\n## üí° Best Practices\n\n1. **Allocate Sufficient Resources**: Avoid bottlenecks.  \n2. **Stay Updated**: Patch vulnerabilities and improve performance.  \n3. **Use Quotas**: Prevent Pods from hogging resources.  \n4. **Label Nodes**: Optimize workload distribution.  \n\n---\n\nKubernetes Worker Nodes: **Simple, Scalable, Powerful!** üõ†\n","recorded":"2025-01-02 16:35:42.324277008","filePath":"null","pinned":false},{"value":"Now this is the **MASTER NODE**, coming to **WORKER NODE**, it also contains 2 different components \n1. KUBE-PROXY\n2. KUBELET\n\n**A worker node in Kubernetes is where the actual application workloads (containers) run. It is a critical part of the Kubernetes cluster that provides the compute, memory, storage, and networking resources necessary to host and manage Pods. Each worker node communicates with the Kubernetes control plane to perform its assigned tasks.**\n\n### Key Components of a Worker Node:\n\n1. **Kubelet**:\n    \n    - The primary agent on the worker node that communicates with the Kubernetes API Server.\n    - It ensures that the Pods scheduled to the node are running as expected.\n    - Periodically reports the status of the node and its running Pods back to the control plane.\n2. **Container Runtime**:\n    \n    - Responsible for running containers. Kubernetes supports multiple container runtimes, including:\n        - Docker\n        - containerd\n        - CRI-O\n    - The runtime pulls container images, starts containers, and manages their lifecycle.\n3. **Kube-Proxy**:\n    \n    - A network proxy that manages networking rules on the node.\n    - It ensures that network traffic is correctly routed to and from Pods running on the node.\n    - Supports Services by load-balancing traffic between Pods.\n4. **Operating System**:\n    \n    - The base layer that hosts the Kubernetes components and container runtime. Common OS choices include Linux distributions like Ubuntu, CentOS, or specialized Kubernetes OSes like CoreOS.\n\n### Responsibilities of a Worker Node:\n\n1. **Hosting Pods**:\n    \n    - Worker nodes run the application Pods as scheduled by the Kubernetes Scheduler.\n2. **Networking**:\n    \n    - Ensures inter-Pod communication and manages traffic between external clients and the cluster.\n3. **Monitoring and Reporting**:\n    \n    - Continuously reports its health and resource usage (e.g., CPU, memory, disk) to the control plane.\n4. **Scaling and Load Distribution**:\n    \n    - Shares the cluster's workload with other worker nodes, allowing Kubernetes to scale applications across the nodes.\n\n### Workflow of a Worker Node:\n\n1. **Pod Scheduling**:\n    \n    - The Kubernetes Scheduler assigns Pods to the worker node.\n    - The kubelet on the node receives instructions to create the Pods.\n2. **Container Runtime**:\n    \n    - The container runtime pulls the required container images from a container registry and starts the containers.\n3. **Resource Allocation**:\n    \n    - The node's resources (CPU, memory, disk) are allocated to running containers as per the Pod specifications.\n4. **Networking**:\n    \n    - The kube-proxy configures networking rules to route traffic to and from Pods.\n    - Pods can communicate with other Pods on the same or different nodes.\n5. **Monitoring**:\n    \n    - The kubelet monitors the health of the containers and ensures they remain in the desired state. If a container fails, kubelet attempts to restart it.\n\n### Features of a Worker Node:\n\n1. **Scalability**:\n    \n    - New worker nodes can be added to the cluster to handle increased workloads.\n2. **Fault Tolerance**:\n    \n    - If a worker node fails, the control plane reschedules the affected Pods on other available nodes.\n3. **Resource Isolation**:\n    \n    - Ensures that Pods running on the same node are isolated from one another using container runtime features like namespaces and cgroups.\n4. **Flexible Resource Management**:\n    \n    - Nodes support overcommitting resources, allowing efficient utilization while maintaining stability.\n\n### Deployment and Configuration:\n\n1. **Node Registration**:\n    \n    - Each worker node registers itself with the control plane using the kubelet.\n2. **Labels and Taints**:\n    \n    - Nodes can have labels to specify their capabilities (e.g., `region=us-east`).\n    - Taints can prevent certain workloads from being scheduled on specific nodes unless the workloads tolerate those taints.\n3. **Node Pooling**:\n    \n    - In managed Kubernetes services (e.g., GKE, EKS, AKS), worker nodes are often organized into \"node pools\" for easier management.\n\n### Monitoring and Maintenance:\n\n1. **Monitoring Tools**:\n    \n    - Tools like Prometheus, Grafana, or Kubernetes Dashboard can be used to monitor node health and performance.\n2. **Resource Usage**:\n    \n    - Continuously monitor CPU, memory, and disk usage to prevent resource exhaustion.\n3. **Upgrades**:\n    \n    - Worker nodes should be upgraded to keep their Kubernetes components and OS up to date.\n4. **Fault Recovery**:\n    \n    - Use node autoscaling and health checks to ensure continuous availability in case of node failures.\n\n### Best Practices:\n\n1. **Ensure Sufficient Resources**:\n    \n    - Allocate adequate CPU, memory, and storage to handle expected workloads.\n2. **Regular Updates**:\n    \n    - Keep the node OS and Kubernetes components updated to avoid security vulnerabilities and compatibility issues.\n3. **Resource Quotas**:\n    \n    - Use resource quotas to prevent any Pod from monopolizing node resources.\n4. **Use Node Labels**:\n    \n    - Label nodes to facilitate intelligent workload scheduling (e.g., running GPU-intensive workloads on GPU nodes).","recorded":"2025-01-02 16:34:24.337855949","filePath":"null","pinned":false},{"value":"There are other commands to run and for documentation visit the official website. ","recorded":"2025-01-02 16:32:14.475006799","filePath":"null","pinned":false},{"value":"# a cluster with 3 control-plane nodes and 3 workers\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker\n- role: worker","recorded":"2025-01-02 16:18:18.738701743","filePath":"null","pinned":false},{"value":"https://kind.sigs.k8s.io/docs/user/quick-start/#multi-node-clusters","recorded":"2025-01-02 16:17:35.374241024","filePath":"null","pinned":false},{"value":"[[ $commands[kubectl] ]] \u0026\u0026 source \u003c(kubectl completion zsh)\n","recorded":"2025-01-02 16:16:14.695393179","filePath":"null","pinned":false},{"value":"echo '[[ $commands[kubectl] ]] \u0026\u0026 source \u003c(kubectl completion zsh)' \u003e\u003e ~/.zshrc","recorded":"2025-01-02 16:15:39.805483673","filePath":"null","pinned":false},{"value":"source \u003c(kubectl completion zsh)","recorded":"2025-01-02 16:15:23.092260167","filePath":"null","pinned":false},{"value":"source \u003c(kubectl completion zsh)  # set up autocomplete in zsh into the current shell\necho '[[ $commands[kubectl] ]] \u0026\u0026 source \u003c(kubectl completion zsh)' \u003e\u003e ~/.zshrc ","recorded":"2025-01-02 16:15:16.597438222","filePath":"null","pinned":false},{"value":"https://kubernetes.io/docs/reference/kubectl/quick-reference/","recorded":"2025-01-02 16:14:26.060577369","filePath":"null","pinned":false},{"value":"# this config file contains all config fields with comments\n# NOTE: this is not a particularly useful config file\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\n# patch the generated kubeadm config with some extra settings\nkubeadmConfigPatches:\n- |\n  apiVersion: kubelet.config.k8s.io/v1beta1\n  kind: KubeletConfiguration\n  evictionHard:\n    nodefs.available: \"0%\"\n# patch it further using a JSON 6902 patch\nkubeadmConfigPatchesJSON6902:\n- group: kubeadm.k8s.io\n  version: v1beta3\n  kind: ClusterConfiguration\n  patch: |\n    - op: add\n      path: /apiServer/certSANs/-\n      value: my-hostname\n# 1 control plane node and 3 workers\nnodes:\n# the control plane node config\n- role: control-plane\n# the three workers\n- role: worker\n- role: worker\n- role: worker\n","recorded":"2025-01-02 16:13:44.842615083","filePath":"null","pinned":false},{"value":"sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl","recorded":"2025-01-02 16:09:37.066909611","filePath":"null","pinned":false},{"value":"   curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"","recorded":"2025-01-02 16:08:54.585814767","filePath":"null","pinned":false},{"value":"kubectl version --client --output=yaml","recorded":"2025-01-02 16:08:08.392742709","filePath":"null","pinned":false},{"value":"kubectl version --client","recorded":"2025-01-02 16:08:05.137223670","filePath":"null","pinned":false},{"value":"echo \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check","recorded":"2025-01-02 16:07:38.958952476","filePath":"null","pinned":false},{"value":"   curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"","recorded":"2025-01-02 16:07:32.032294690","filePath":"null","pinned":false},{"value":"https://kubernetes.io/docs/tasks/tools/#kubectl","recorded":"2025-01-02 16:06:47.049196918","filePath":"null","pinned":false},{"value":"One more important note is that in CKA examination - **Below domains and all of its sub-domains are allowed to be referred in the exam**, but the thing is we have to know how to search and what to search üòÇ","recorded":"2025-01-02 16:05:04.245672621","filePath":"null","pinned":false},{"value":"üòÇ","recorded":"2025-01-02 16:05:03.615158751","filePath":"null","pinned":false},{"value":"https://kubernetes.io/docs","recorded":"2025-01-02 16:04:32.051499860","filePath":"null","pinned":false},{"value":"https://kubernetes.io/blog/","recorded":"2025-01-02 16:04:22.768332841","filePath":"null","pinned":false},{"value":"\n    https://kubernetes.io/docs\n    https://kubernetes.io/blog/\n    Kubernetes cheat sheet : https://kubernetes.io/docs/reference/kubectl/quick-reference/\n","recorded":"2025-01-02 16:03:48.802762944","filePath":"null","pinned":false},{"value":"Below domains and all of its sub-domains are allowed to be referred in the exam","recorded":"2025-01-02 16:03:41.903237630","filePath":"null","pinned":false},{"value":"https://kind.sigs.k8s.io/docs/user/quick-start/#creating-a-cluster","recorded":"2025-01-02 16:00:40.912856405","filePath":"null","pinned":false},{"value":"https://kind.sigs.k8s.io/docs/user/quick-start/","recorded":"2025-01-02 15:58:50.163917958","filePath":"null","pinned":false},{"value":"## Day-05: Kubernetes Architecture","recorded":"2025-01-02 15:51:22.181433345","filePath":"null","pinned":false},{"value":"# sudo systemctl enable --now libvirtd\n","recorded":"2025-01-02 15:03:47.500294870","filePath":"null","pinned":false},{"value":"CONTAINER ID   IMAGE         COMMAND    CREATED         STATUS                     PORTS     NAMES\n407722ab4431   hello-world   \"/hello\"   3 seconds ago   Exited (0) 2 seconds ago             hello-world","recorded":"2025-01-02 14:16:58.245140410","filePath":"null","pinned":false},{"value":"elated_swartz","recorded":"2025-01-02 14:16:23.818866456","filePath":"null","pinned":false},{"value":"821c34da15e2","recorded":"2025-01-02 14:15:34.999932913","filePath":"null","pinned":false},{"value":"821c34da15e2   hello-world   \"/hello\"   31 minutes ago   Exited (0) 31 minutes ago             elated_swartz","recorded":"2025-01-02 14:15:34.140809457","filePath":"null","pinned":false},{"value":"https://docs.docker.com/get-started/docker_cheatsheet.pdf","recorded":"2025-01-02 14:12:02.771946689","filePath":"null","pinned":false},{"value":"docker-login-cli","recorded":"2025-01-02 14:06:55.880038892","filePath":"null","pinned":false},{"value":"Docker Login using Pass in CLI","recorded":"2025-01-02 14:06:48.931800265","filePath":"null","pinned":false},{"value":"Login to docker","recorded":"2025-01-02 14:05:51.618077995","filePath":"null","pinned":false},{"value":"sed -i '0,/{/s/{/{\\n\\t\"credsStore\": \"pass\",/' ~/.docker/config.json","recorded":"2025-01-02 14:05:43.684455006","filePath":"null","pinned":false},{"value":"Add credsStore to your docker config. This can be done with sed if you don't already have credStore added to your config or you can manually add \"credStore\":\"pass\" to the config.json.","recorded":"2025-01-02 14:05:39.314439221","filePath":"null","pinned":false},{"value":"pass init \"\u003cYour Name\u003e\"","recorded":"2025-01-02 14:05:26.511111328","filePath":"null","pinned":false},{"value":"Initialize pass using the newly created key","recorded":"2025-01-02 14:05:19.089749416","filePath":"null","pinned":false},{"value":"Follow prompts from gpg2 utility","recorded":"2025-01-02 14:05:10.454539668","filePath":"null","pinned":false},{"value":"gpg2 --gen-key","recorded":"2025-01-02 14:05:02.254916406","filePath":"null","pinned":false},{"value":"Create a new gpg2 key.","recorded":"2025-01-02 14:04:18.034474877","filePath":"null","pinned":false},{"value":"wget https://github.com/docker/docker-credential-helpers/releases/download/v0.6.0/docker-credential-pass-v0.6.0-amd64.tar.gz \u0026\u0026 tar -xf docker-credential-pass-v0.6.0-amd64.tar.gz \u0026\u0026 chmod +x docker-credential-pass \u0026\u0026 sudo mv docker-credential-pass /usr/local/bin/","recorded":"2025-01-02 14:04:01.415754137","filePath":"null","pinned":false},{"value":"Download, extract, make executable, and move docker-credential-pass","recorded":"2025-01-02 14:03:52.417095537","filePath":"null","pinned":false},{"value":"https://www.passwordstore.org/","recorded":"2025-01-02 14:02:59.217336582","filePath":"null","pinned":false},{"value":"üòÖ","recorded":"2025-01-02 14:01:07.976111473","filePath":"null","pinned":false},{"value":"https://github.com/docker/docker-credential-helpers/issues/102#issuecomment-388974092","recorded":"2025-01-02 13:58:51.594008422","filePath":"null","pinned":false},{"value":"Generally when we are using CLI version of docker preferably in linux (I use Arch BTW  )","recorded":"2025-01-02 13:57:44.284256684","filePath":"null","pinned":false},{"value":"üëø","recorded":"2025-01-02 13:57:34.457681653","filePath":"null","pinned":false},{"value":"VZCL-XNKC","recorded":"2025-01-02 13:55:29.283277513","filePath":"null","pinned":false},{"value":"sed -i '0,/{/s/{/{\\n\\t\"credsStore\": \"pass\",/' ~/.docker/config.json\n","recorded":"2025-01-02 13:55:00.906519802","filePath":"null","pinned":false},{"value":"347DAB753B400EF50ED5E1103DBB25722D672CD8","recorded":"2025-01-02 13:54:43.784345526","filePath":"null","pinned":false},{"value":"wget https://github.com/docker/docker-credential-helpers/releases/download/v0.6.0/docker-credential-pass-v0.6.0-amd64.tar.gz \u0026\u0026 tar -xf docker-credential-pass-v0.6.0-amd64.tar.gz \u0026\u0026 chmod +x docker-credential-pass \u0026\u0026 sudo mv docker-credential-pass /usr/local/bin/\n","recorded":"2025-01-02 13:54:03.927072471","filePath":"null","pinned":false},{"value":"\n    Install pass\n\nsudo apt-get install pass\n\n    Download, extract, make executable, and move docker-credential-pass\n\nwget https://github.com/docker/docker-credential-helpers/releases/download/v0.6.0/docker-credential-pass-v0.6.0-amd64.tar.gz \u0026\u0026 tar -xf docker-credential-pass-v0.6.0-amd64.tar.gz \u0026\u0026 chmod +x docker-credential-pass \u0026\u0026 sudo mv docker-credential-pass /usr/local/bin/\n\n    Create a new gpg2 key.\n\ngpg2 --gen-key\n\n    Follow prompts from gpg2 utility\n\n    Initialize pass using the newly created key\n\npass init \"\u003cYour Name\u003e\"\n\n    Add credsStore to your docker config. This can be done with sed if you don't already have credStore added to your config or you can manually add \"credStore\":\"pass\" to the config.json.\n\nsed -i '0,/{/s/{/{\\n\\t\"credsStore\": \"pass\",/' ~/.docker/config.json\n\n    Login to docker\n\ndocker login\n","recorded":"2025-01-02 13:53:05.074986100","filePath":"null","pinned":false},{"value":"\"credsStore\": \"osxkeychain\"","recorded":"2025-01-02 13:49:56.858914974","filePath":"null","pinned":false},{"value":"WARNING! Your password will be stored unencrypted in /home/karna/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credential-stores","recorded":"2025-01-02 13:49:04.228423901","filePath":"null","pinned":false},{"value":"Aa2005233711@","recorded":"2025-01-02 13:46:45.069517921","filePath":"null","pinned":false},{"value":"chagantivenkataramireddy1@gmail.com","recorded":"2025-01-02 13:46:14.593772226","filePath":"null","pinned":false},{"value":"sudo usermod -aG docker $USER","recorded":"2025-01-02 13:42:19.653689670","filePath":"null","pinned":false},{"value":"Day-05 of 40 Days of Kubernetes with blog (Part2)","recorded":"2025-01-02 13:26:33.395574553","filePath":"null","pinned":false},{"value":"  background: rgba(255, 77, 77, 0.15);\n","recorded":"2025-01-02 13:08:30.375560539","filePath":"null","pinned":false},{"value":"nwg-clipman ","recorded":"2024-12-31 14:12:58.361974201","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/posts/karan-saxena-466b07190_if-you-dont-want-to-waste-100s-of-hours-activity-7275849490740219904-dXbC/?utm_source=share\u0026utm_medium=member_ios","recorded":"2024-12-31 14:09:12.902821408","filePath":"null","pinned":false},{"value":"/mnt/Shared/SOCTA PPT - Update.pptx","recorded":"2024-12-31 14:00:13.257255023","filePath":"null","pinned":false},{"value":"Hyderabad@2003","recorded":"2024-12-30 15:27:48.847434465","filePath":"null","pinned":false},{"value":"venkataramireddychaganti41@gmail.com","recorded":"2024-12-30 15:27:44.579325491","filePath":"null","pinned":false},{"value":"/run/media/karna/Files/Agnito.zip","recorded":"2024-12-30 14:54:13.090454501","filePath":"null","pinned":false}]}