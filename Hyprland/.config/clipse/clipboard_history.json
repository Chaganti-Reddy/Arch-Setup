{"clipboardHistory":[{"value":"idle_inhibitor","recorded":"2024-11-05 12:58:07.467985029","filePath":"null","pinned":false},{"value":"\"idle_inhibitor\": {\n    \"format\": \"ÔÉß \",\n    \"tooltip\": true,\n    \"tooltip-format-activated\": \"Presentation Mode\",\n    \"tooltip-format-deactivated\": \"Idle Mode\",\n    \"start-activated\": false,\n    \"timeout\": 5\n  },","recorded":"2024-11-05 12:57:58.474908769","filePath":"null","pinned":false},{"value":"#custom-arch {\n    color: black;\n    background: @arch;\n    font-size: 11pt;\n    margin-right: -1px;\n    margin-bottom: -2px;\n    padding-right: 0px;\n    padding-left: 3px;\n    text-shadow: 0px 0px 1.5px rgba(0, 0, 0, 1);\n}","recorded":"2024-11-05 12:57:26.152847998","filePath":"null","pinned":false},{"value":"ÔÉß","recorded":"2024-11-05 12:55:28.668251976","filePath":"null","pinned":false},{"value":"\"format\": \"ÔÉß \",\n    \"tooltip\": true,\n    \"tooltip-format-activated\": \"Presentation Mode\",\n    \"tooltip-format-deactivated\": \"Idle Mode\",\n    \"start-activated\": false,\n    \"timeout\": 5","recorded":"2024-11-05 12:54:14.058359828","filePath":"null","pinned":false},{"value":"custom/arch","recorded":"2024-11-05 12:54:06.957266889","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Books/DSA.png","recorded":"2024-11-05 12:29:06.622648123","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/DSA.png","recorded":"2024-11-05 12:28:37.517235438","filePath":"null","pinned":false},{"value":"( 6/10) upgrading gnupg                                                                                            [--------------------------------------------------------------------] 100%","recorded":"2024-11-05 12:12:31.678867240","filePath":"null","pinned":false},{"value":"sudo pacman -S tesseract","recorded":"2024-11-05 11:45:21.604233452","filePath":"null","pinned":false},{"value":"tesseract","recorded":"2024-11-05 11:44:57.162303123","filePath":"null","pinned":false},{"value":"Personal History Statement (PHS)\nRequired of all applicants:\n‚Ä¢ Describe how your background and life experiences contribute to your ability to be both persistent and resourceful in graduate school.\n‚Ä¢ Describe how your life experiences have prepared you to contribute to an academic community where scholars with diverse research\ninterests, abilities, backgrounds and experiences are supported, respected, and valued.\nOptional:\n‚Ä¢ Please address concerns that you may have that your academic record does not reflect your true capabilities and discuss mitigating factors\nthat have affected your academic record. Reviewers will be interested in understanding your accomplishments relative to your opportunities.","recorded":"2024-11-04 21:42:29.865673617","filePath":"null","pinned":false},{"value":"Describe how your skills, preparation, and interests are a match for the program to which you are applying. Identify faculty who share your\nresearch and scholarly interests. Reviewers will want to know that you have researched the program, faculty, and key focus areas.","recorded":"2024-11-04 21:42:08.879757278","filePath":"null","pinned":false},{"value":"Discuss how your experiences, skills and abilities have prepared you for graduate study. Relevant topics may include coursework, work and\nresearch experiences, internships, presentations, exhibits, publications and community service. If you describe a research or scholarly\nexperience, include information on the topic, research mentor, vour role and outcomes.","recorded":"2024-11-04 21:39:27.653095282","filePath":"null","pinned":false},{"value":"Describe your research, scholarly, or creative interests. What topics are of particular interest to you? Reviewers know that interests change\nover time but try to be as specific as possible.","recorded":"2024-11-04 21:18:21.302902010","filePath":"null","pinned":false},{"value":"My journey in marketing began through academic exposure towards \"marketing management,\" and there, the development of understanding on a lot of key topics in this arena like consumer behavior, product life cycle, product, and price management etc. From that point onward, these basics developed the strong interest to have the quest of further insight regarding the very dynamic world of marketing. Through such research papers and academic projects along with an internship that included hands-on experience working directly with social media marketing strategy and understanding the nature of consumer behavior, I could explore the interest I have further in these areas. Through the experiences so far, I can take away valuable market research skills and some practical insights about strategy but do know I need to make more progress technically, specifically on the analytics end, in order to become competitive for this role as a marketer working more so in data-driven fashion. This included especially working on market trends analysis and deriving actionable insights in order to ensure effective marketing strategies. I was greatly inspired by this experience to become a marketing consultant at a top-tier firm using my advanced analytical skills to help businesses develop data-backed marketing strategies. Attending Purdue University for a Master's in Marketing Analytics is a vital step toward the realization of these career aspirations. The focus of the program on Marketing Analytics, Data-Driven Marketing will give me the technical capabilities for transitioning into a marketing consulting role.","recorded":"2024-11-04 21:13:01.904684970","filePath":"null","pinned":false},{"value":"My journey in marketing began with academic exposure to ‚Äúmarketing management‚Äù, where I developed understanding of key topics like consumer behaviour, product life cycle, product and price management etc. This foundational knowledge sparked a strong interest and a quest for deeper insights into the dynamic world of marketing. I pursued this interest further by engaging in academic projects, research papers, and internship experiences that allowed me to work hands-on with social media marketing strategies and analyse consumer behaviour. While my practical experience has provided me with valuable skills in areas like market research and strategy, I recognize that I need to strengthen my technical skills, particularly in data analytics, to excel in a data-focused marketing role. During my internship as a market research analyst, I enjoyed analysing market trends and deriving actionable insights for effective marketing strategies. This experience inspired me to aspire to become a marketing consultant in a top-tier firm, where I can use advanced analytical skills to guide businesses in developing data-backed marketing strategies. Pursuing a Master‚Äôs in Marketing Analytics at Purdue University is a critical step toward achieving my career goals. The program‚Äôs emphasis on Marketing Analytics, Data Driven Marketing will equip me with the technical expertise needed to transition into a marketing consulting role.","recorded":"2024-11-04 21:08:09.939625486","filePath":"null","pinned":false},{"value":"My journey in marketing began with academic exposure to subjects like marketing management, where I developed an understanding of key concepts such as consumer behavior, the product life cycle, and marketing strategy. This foundational knowledge sparked a strong interest and a quest for deeper insights into the dynamic world of marketing. I pursued this interest further by engaging in academic projects, research papers, and internship experiences that allowed me to work hands-on with social media marketing strategies and analyze consumer behavior. These experiences not only solidified my passion for marketing but also underscored the importance of data-driven insights in making impactful business decisions.\n\nWhile my practical experience has provided me with valuable skills in areas like market research and strategy, I recognize that I need to strengthen my technical skills, particularly in data analytics, to excel in a data-focused marketing role. During my internship as a market research analyst, I enjoyed analyzing market trends and deriving actionable insights for effective marketing strategies. This experience inspired me to aspire to become a marketing consultant in a top-tier firm, where I can use advanced analytical skills to guide businesses in developing data-backed marketing strategies.\n\nPursuing a Master‚Äôs in Marketing Analytics at Purdue University is a critical step toward achieving my career goals. The program‚Äôs emphasis on analytical tools, statistical methods, and data-driven decision-making will equip me with the technical expertise needed to transition into a consulting role. Moreover, learning from experienced faculty and collaborating with a diverse cohort will help me build a well-rounded skill set that combines both strategic thinking and technical proficiency.","recorded":"2024-11-04 21:05:33.534377886","filePath":"null","pinned":false},{"value":"My professional goal is to become a data-driven marketing strategist, capable of making informed, creative, and consumer-focused decisions. I envision myself working in a global brand management role, where I can harness data analytics to shape impactful brand narratives and drive consumer engagement. The dynamic field of marketing excites me, especially the intersection of technology, data, and psychology, where understanding consumer behavior is both an art and a science. I am particularly drawn to how brands build trust and authenticity in an increasingly digital landscape.\n\nPurdue University‚Äôs MS in Marketing program stands out to me because of its emphasis on data analytics and consumer insights, both of which are critical to becoming a marketing leader in today‚Äôs world. Courses in predictive analytics and brand management align closely with my aspirations, as I aim to develop not only analytical skills but also strategic thinking abilities. I am especially impressed by Purdue‚Äôs experiential learning approach, which will allow me to work on real-world projects, learn from accomplished faculty members, and collaborate with a diverse cohort of driven peers.\n\nGraduate school will provide the structured learning environment and practical experiences I need to gain a competitive edge in the industry. By building a solid foundation in both theory and application, Purdue will prepare me to confidently tackle marketing challenges, adapt to industry shifts, and make a meaningful impact in the field. Ultimately, I am committed to a career where I can contribute to the growth of brands in meaningful ways, and I believe Purdue University is the perfect place to bring my ambitions to life.","recorded":"2024-11-04 20:51:52.827530723","filePath":"null","pinned":false},{"value":"To write a strong response for your statement of purpose, you could consider the following points:\n\n    Career Vision and Ambitions in Marketing\n    Share a clear, future-oriented vision of your career. Mention specific areas within marketing‚Äîlike digital marketing, consumer insights, brand strategy, or data analytics‚Äîthat excite you. Highlight your passion for leveraging data to understand consumer behavior or creating impactful brand strategies.\n\n    Why Marketing?\n    Explain what drew you to marketing, such as a desire to understand consumer psychology, the creative aspect of building brands, or the technical side of data analysis. Show that you understand the dynamic nature of the marketing field and emphasize your commitment to staying innovative.\n\n    Long-Term Goals and Industry Impact\n    Describe where you see yourself after graduation. Whether it‚Äôs leading marketing initiatives in a multinational corporation, founding a data-driven marketing firm, or working in a niche industry, share how you aim to make an impact. Show that you aspire to become a marketing leader who makes informed, innovative, and strategic decisions.\n\n    Why Purdue University?\n    Highlight specific aspects of Purdue‚Äôs marketing program that align with your goals. For example, mention faculty members you admire, relevant coursework (such as analytics or consumer behavior), or any unique opportunities Purdue offers, like internships or case competitions. This demonstrates that you‚Äôve thoroughly researched the program.\n\n    How Graduate School Will Prepare You\n    Emphasize that Purdue‚Äôs program will equip you with both the technical skills and theoretical knowledge needed to excel in marketing. Discuss how you‚Äôll benefit from Purdue‚Äôs focus on data-driven marketing, hands-on projects, and collaborative learning, which will refine your expertise and help you build a strong professional network.\n\n    Future Aspirations and Commitment to Growth\n    Lastly, underline your commitment to continuous learning and how the skills you acquire at Purdue will be foundational as you adapt to the evolving landscape of marketing.","recorded":"2024-11-04 20:46:50.067218034","filePath":"null","pinned":false},{"value":"What are your professional plans and career goals? How will attending graduate school assist you in reaching those goals?","recorded":"2024-11-04 20:45:38.670628352","filePath":"null","pinned":false},{"value":"üòâ","recorded":"2024-11-04 20:37:13.655806511","filePath":"null","pinned":false},{"value":"https://meet.google.com/fvc-wwjf-mvz","recorded":"2024-11-04 20:23:34.484532879","filePath":"null","pinned":false},{"value":"28,900\nRMB","recorded":"2024-11-04 11:05:23.913524523","filePath":"null","pinned":false},{"value":"percentile","recorded":"2024-11-04 09:45:59.640603769","filePath":"null","pinned":false},{"value":"default imagenet weights were provided to the model. The output layer consist of global\naverage filter and softmax is used as the activation function. The Training and validation\naccuracy, Training and validation loss are shown in Figure 4.4 below.","recorded":"2024-11-03 22:19:40.810490105","filePath":"null","pinned":false},{"value":"The EfficientNet B0 model has roughly 5.9 million parameters, it is consist of exactly 3\ninput channels, it is a transfer learning model already pretrained on Imagenet dataset. By","recorded":"2024-11-03 22:19:28.422983901","filePath":"null","pinned":false},{"value":"EfficientNet B0 Model","recorded":"2024-11-03 22:19:09.486374852","filePath":"null","pinned":false},{"value":"Training, Validation loss and Accuracy of Sequential model","recorded":"2024-11-03 22:18:15.400917885","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/cnn seq.png","recorded":"2024-11-03 22:17:56.683780372","filePath":"null","pinned":false},{"value":"tf.keras.preprocessing.image.ImageDataGenerator","recorded":"2024-11-03 22:00:37.955041017","filePath":"null","pinned":false},{"value":"module 'keras.api.preprocessing.image' has no attribute 'ImageDataGenerator'","recorded":"2024-11-03 22:00:12.273428584","filePath":"null","pinned":false},{"value":"!wget https://www.dropbox.com/s/qv3fwuzp18p4s3s/OralCancer.rar","recorded":"2024-11-03 21:54:59.343955012","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Updated_CNN_Sequential_with_correct_paths.ipynb","recorded":"2024-11-03 21:48:33.465703404","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Updated_CNN_Sequential.ipynb","recorded":"2024-11-03 21:46:40.968975940","filePath":"null","pinned":false},{"value":"/kaggle/input/oralcancer/Oral Cancer/oral_scc","recorded":"2024-11-03 21:45:00.531376342","filePath":"null","pinned":false},{"value":"/kaggle/input/oralcancer/Oral Cancer/oral_normal","recorded":"2024-11-03 21:44:56.172893005","filePath":"null","pinned":false},{"value":"/kaggle/input/oralcancer/Oral Cancer","recorded":"2024-11-03 21:44:47.158308264","filePath":"null","pinned":false},{"value":" module 'keras.api.preprocessing.image' has no attribute 'ImageDataGenerator'","recorded":"2024-11-03 21:43:44.894420503","filePath":"null","pinned":false},{"value":"/content/Oral Cancer","recorded":"2024-11-03 21:41:17.169140147","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/naruto-uzumaki.png","recorded":"2024-11-01 18:16:23.945864950","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/naruto uzumaki.png","recorded":"2024-11-01 18:05:17.507738529","filePath":"null","pinned":false},{"value":"If you don't like your destiny, don't accept it. Instead, have the courage to change it the way you want it to be!","recorded":"2024-11-01 18:02:41.717505270","filePath":"null","pinned":false},{"value":"Topological Deep Learning: Going Beyond Graph Data","recorded":"2024-11-01 17:58:41.605616548","filePath":"null","pinned":false},{"value":"Deep Double Descent: Where Bigger Models and More Data Hurt","recorded":"2024-11-01 17:56:08.611963257","filePath":"null","pinned":false},{"value":"Just Give up....\non trying to make me give up","recorded":"2024-11-01 17:54:28.914470135","filePath":"null","pinned":false},{"value":"#8F6325","recorded":"2024-11-01 17:14:08.531318507","filePath":"null","pinned":false},{"value":"\n\n\n\n\n\n\n\n\n","recorded":"2024-11-01 17:13:56.139314626","filePath":"null","pinned":false},{"value":"Yocket‚Äôs University Predictor tool","recorded":"2024-11-01 16:57:01.750392826","filePath":"null","pinned":false},{"value":"8639746193","recorded":"2024-11-01 16:55:56.509715122","filePath":"null","pinned":false},{"value":"Game of Thrones","recorded":"2024-10-30 23:16:16.811093025","filePath":"null","pinned":false},{"value":"The sequential CNN model comprises four convolutional layers, each including a filter size of 3x3 and a filter count of 32, 64, 128, and 128, utilizing the ReLU activation function. Subsequently, two 2x2 MaxPool layers are incorporated, concluding the model with a fully linked layer and a sigmoid layer. An early check was implemented on the model to halt training if the model's accuracy does not improve. Figure 4.3 below illustrates the training and validation accuracy, as well as the training and validation loss.","recorded":"2024-10-30 14:53:29.882741011","filePath":"null","pinned":false},{"value":"The sequential CNN model is constructed using four convolutional layers, each with a\nfilter size of 3x3 and a number of filters of 32, 64, 128 and 128 and an activation function\nof ReLU. Next, two 2x2 MaxPool layers are added, and the model is finished with a fully\nconnected layer and a sigmoid layer. An early check was applied on the model to stop\ntraining if the accuracy of model is not increased. The Training and validation accuracy,\nTraining and validation loss are shown in Figure 4.3 below.","recorded":"2024-10-30 14:53:14.281303085","filePath":"null","pinned":false},{"value":"Sequential CNN Model","recorded":"2024-10-30 14:53:01.888852572","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/canny.png","recorded":"2024-10-30 13:56:23.648986906","filePath":"null","pinned":false},{"value":"Canny edge detection is one of the most precisely defined approaches that provides accurate and reliable detection. The three prerequisites for edge detection are a low error rate, precise localization, and minimal response. CED was utilized to delineate the cancerous region of the image from the remainder. The photos were initially converted to grayscale, followed by the use of Gaussian blur. Canny edge detection was used to the images following the acquisition of the structure. Figure 4.2 illustrates the application of Canny edge detection on images of oral cancer. Canny edge detection yielded suboptimal accuracy when utilized on clinical images.","recorded":"2024-10-30 13:55:52.676819812","filePath":"null","pinned":false},{"value":"One of the most rigorously defined techniques that offers accurate and trustworthy detec-\ntion is canny edge detection. Low error rate, accurate localization, and minimal response\nare the three requirements for edge detection. CED was applied to segment the cancer\npart of the image from the rest. The images were first converted to grayscale, then gaus-\nsian blur was applied. After getting the structure canny edge detection was applied on the\nimages. Figure 4.2 shows canny edge detection applied on oral cancer images. Canny\nedge detection did not provide good accuracy when applied on Clinical Images.","recorded":"2024-10-30 13:55:43.824500892","filePath":"null","pinned":false},{"value":"Canny Edge Detection","recorded":"2024-10-30 13:55:32.093732101","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/thresold.png","recorded":"2024-10-30 13:54:49.244471815","filePath":"null","pinned":false},{"value":"The image is converted into a binary format using thresholding, with the shape pixels rendered in white and the background pixels in black. By doing so, we can swiftly differentiate the shapes from the background and conduct further analysis on them. The photos underwent several threshold settings between 50 and 200 to determine the ideal threshold for the deep learning model. Figure 4.1 illustrates the outcome of thresholding applied to oral cancer images.","recorded":"2024-10-30 13:54:11.099674247","filePath":"null","pinned":false},{"value":"The image is turned into a binary image via thresholding, where the pixels that make up\nthe shapes are white and the pixels that make up the backdrop are black. By doing so, we\ncan quickly distinguish the shapes from the background and carry out additional analysis\non them. The images were subjected to different threshold values ranging from 50 to 200\nto find the optimal threshold for the Deep learning model. Figure 4.1 depicts the result of\nthresholding on oral cancer images.","recorded":"2024-10-30 13:53:53.510940187","filePath":"null","pinned":false},{"value":"Thresholding of images","recorded":"2024-10-30 13:51:02.019801085","filePath":"null","pinned":false},{"value":"This section contains the findings of a number of experiments, including a comparison of\nvarious pretrained models and a comparison of suggested models. Using bar graphs and\ntables, each subsection is thoroughly described.","recorded":"2024-10-30 13:50:50.649743752","filePath":"null","pinned":false},{"value":"The oral cancer clinical image dataset was obtained from Kaggle and is divided into two categories: malignant and non-cancerous. The dataset comprises 87 photos of malignant tumors and 44 images of benign oral tissue, all employed for training purposes. The photographs encompass dimensions ranging from 258x217 to 1258x758 pixels and are supplied in JPG format. The dataset includes samples from male and female patients obtained from hospitals around Karnataka. This varied representation guarantees that the dataset mirrors actual clinical variability, enhancing model training and assessment.","recorded":"2024-10-30 13:19:52.420690218","filePath":"null","pinned":false},{"value":"The dataset of oral cancer clinical images was sourced from Kaggle and is categorized into two classes: cancerous and non-cancerous. It includes a total of 87 images of cancerous lesions and 44 images of non-cancerous oral tissue, all utilized for training. The images capture a range of dimensions from 258x217 to 1258x758 pixels and are provided in JPG format. The dataset comprises samples from both male and female patients, collected from hospitals across Karnataka. This diverse representation helps ensure that the dataset reflects real-world clinical variability for improved model training and evaluation.","recorded":"2024-10-30 13:18:58.986375261","filePath":"null","pinned":false},{"value":"Dataset of oral cancer clinical images was obtained from Kaggle. The images are di-\nvided into 2 parts cancer and non-cancer. There were 87 cancerous images and 44 non-\ncancerous images used in training. The dataset consist of both oral cancer and oral normal\nimages. The dimensions of these oral cancer images range from 258x 217 to 1258x 758\nunits and they were accessible in JPG format. The dataset consist od both male and female\nand was collected from hospitals of Karnataka.","recorded":"2024-10-30 13:10:53.994717591","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\small\t\n    \\caption{The experimental setup} \\label{Table_02}\n    \\addtolength{\\tabcolsep}{0.0pt}\n    \\begin{tabular}{l l l}\n    \\hline\n    %\\rowcolor[HTML]{EFEFEF}\n        Component  \u0026 GPU Specifications           \u0026 TPU   \\\\ \\hline\n        CPU        \u0026 $2$ AMD Rome, $128$ Cores Total  \u0026 Intel$^\\text{\\textregistered}$ Xeon$^\\text{\\textregistered}$ @$2.20$ GHz \\\\\n        RAM        \u0026 $256$ GB                   \u0026 $13$ GB  \\\\\n        GPU        \u0026 NVIDIA DGX A$100$--$40$ GB \u0026 NVIDIA T4 GPU--$16$ GB \\\\\n        TPU        \u0026 $-$                        \u0026 Google TPU v2      \\\\\n        Storage    \u0026 $100$ GB SSD               \u0026 $100$ GB SSD       \\\\\n        OS         \u0026 Ubuntu $20.04$ LTS         \u0026 Ubuntu $22.04$ LTS \\\\\n        Python version  \u0026 $3.10.8$              \u0026 $3.10.12$     \\\\ \\hline\n    \\end{tabular}\n\\end{table}","recorded":"2024-10-30 13:09:12.515643391","filePath":"null","pinned":false},{"value":"NVIDIA DGX A 100 high-performance computing(HPC)","recorded":"2024-10-30 13:08:17.709374497","filePath":"null","pinned":false},{"value":"We employed neural networks to develop the research code in Python for our study utilizing frameworks such as Keras and TensorFlow. Keras is an open-source neural network API that serves as an interface for TensorFlow.It prioritizes the concept over the implementation. The Keras API within the TensorFlow framework is compatible with both CPU and GPU architectures. We utilize Google Colab, which is conducive to user-friendly solutions, to train the model. Google Colab is a complimentary Jupyter notebook environment that functions entirely in the cloud. Colab's sufficient processing capacity makes it particularly suitable for machine and deep learning implementations. It provides multiple runtime options, including TPU (TensorFlow Processing Unit) and GPU (Graphics Processing Unit). Google Colab offers twelve GB of RAM and facilitates GPU-accelerated picture processing. Images are composed of pixel values utilized by high-capacity GPUs.","recorded":"2024-10-30 13:07:07.411469817","filePath":"null","pinned":false},{"value":"We used neural networks to implement the research code in Python for our study using\nnetworks such as Keras and TensorFlow. An interface of TensorFlow, Keras is an open-\nsource neural network API.It places more emphasis on the idea than the execution. The\nKeras API on the Tensorflow platform supports both the CPU and GPU. We employ\nGoogle Colab, which is user-friendly for implementations, to train the model. A free\nJupyter notebook environment, Google-Colab operates totally in the cloud. Due to its\nadequate processing capability, Colab is ideally suited for implementations of machines\nand deep learning. It offers a variety of runtime alternatives, including TPU (Tensorflow\nProcessing Unit) and GPU (Graphics Processing Unit). The Google Colab has twelve GB\nof RAM and supports GPU-based image processing. Pixel values used by high-capacity\nGPUs make up images.","recorded":"2024-10-30 13:06:21.110724413","filePath":"null","pinned":false},{"value":"Error loading webview: Error: Could not register service worker: InvalidStateError: Failed to register a ServiceWorker: The document is in an invalid state..","recorded":"2024-10-30 12:52:55.206620181","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/CNN.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/CNN_Sequential.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/DenseNet.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Edge_Detection.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Efficient_net_and_VGG.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Metaheuristic.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Oral Cancer (Lips and Tongue) images\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/README.md\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Resnet50.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Resnet_With_Fitness.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Results\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Sequential.ipynb\n/home/karna/Downloads/Oral-cancer-detection-using-deep-learning-main/Thresholding.ipynb","recorded":"2024-10-30 12:52:16.182788998","filePath":"null","pinned":false},{"value":"Somersault foraging behavior in MRFO.","recorded":"2024-10-30 12:25:15.408436833","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/somersault.png","recorded":"2024-10-30 12:24:51.057081066","filePath":"null","pinned":false},{"value":"This is the pinnacle foraging strategy. This is among the most breathtaking vistas in nature. Manta rays execute many backward spins near plankton as they locate a food source to lure it. Manta rays optimize their food intake by doing somersaults, which are unexpected, frequent, localized movements that attract plankton to their mouths.","recorded":"2024-10-30 12:24:10.274356089","filePath":"null","pinned":false},{"value":"This is the ultimate foraging strategy. This is among the most breathtaking vistas in nature. Manta rays execute many backward spins near plankton as they locate a food source to lure it. Manta rays optimize their food intake by doing somersaults, which are unexpected, frequent, localized movements that attract plankton to their mouths.","recorded":"2024-10-30 12:23:53.575610856","filePath":"null","pinned":false},{"value":"It is the final foraging tactic. This is one of the most stunning scenes in nature. Manta rays\nwill perform a number of backward spins near the plankton when they find a supply of\nfood in order to attract it to them. Manta rays maximise their intake of food by performing\nsomersaults, which are surprising, frequent, localised movements that draws the plankton\nto the mouth of manta ray.","recorded":"2024-10-30 12:23:38.531814380","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Optimized Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n        \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n        \\STATE Set maximum iterations $T_{\\text{max}}$ and initialize $t = 0$\n        \\STATE Evaluate fitness $f(\\mathbf{X}_i)$ for each manta ray and set $\\mathbf{X}_{\\text{best}} = \\arg \\min f(\\mathbf{X}_i)$\n\n        \\WHILE{$t \u003c T_{\\text{max}}$}\n            \\FOR{each manta ray $i$}\n                \\STATE Generate a random number $r_{\\text{choice}} = \\text{rand()}$\n                \n                \\IF{$r_{\\text{choice}} \u003c 0.5$ \\textbf{and} $\\frac{t}{T_{\\text{max}}} \u003c \\text{rand()}$}\n                    \\STATE Select a random position $\\mathbf{X}_{\\text{rand}}$ \\COMMENT{Cyclone Foraging}\n                    \\STATE $\\mathbf{X}_i = \\mathbf{X}_i + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i)$\n                \\ELSE\n                    \\STATE \\COMMENT{Chain Foraging}\n                    \\STATE $\\mathbf{X}_i = \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i)$\n                \\ENDIF\n            \\ENDFOR\n\n            \\STATE Compute fitness $f(\\mathbf{X}_i)$ for each updated position\n            \\STATE Update $\\mathbf{X}_{\\text{best}}$ if a better solution is found\n\n            \\FOR{each manta ray $i$} \\COMMENT{Somersault Foraging}\n                \\STATE $\\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i)$\n            \\ENDFOR\n\n            \\STATE Increment iteration $t = t + 1$\n        \\ENDWHILE\n\n        \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-30 12:22:08.690147452","filePath":"null","pinned":false},{"value":"See the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.49                 \\STATE\n                            $\\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdo...\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:49: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.49                 \\STATE\n                            $\\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdo...\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:49: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.49                 \\STATE\n                            $\\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdo...\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:50: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.50             \\ENDFOR","recorded":"2024-10-30 12:21:04.902201492","filePath":"null","pinned":false},{"value":"See the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.54                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:54: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.54                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:54: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.54                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:58: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.58             \\ENDFOR","recorded":"2024-10-30 12:20:05.206164827","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Optimized Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n        \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n        \\STATE Set maximum iterations $T_{\\text{max}}$ and initialize $t = 0$\n        \\STATE Evaluate fitness $f(\\mathbf{X}_i)$ for each manta ray and set $\\mathbf{X}_{\\text{best}} = \\arg \\min f(\\mathbf{X}_i)$\n\n        \\WHILE{$t \u003c T_{\\text{max}}$}\n            \\FOR{each manta ray $i$}\n                \\STATE Generate a random number $r_{\\text{choice}} = \\text{rand()}$\n                \n                \\IF{$r_{\\text{choice}} \u003c 0.5$ \\textbf{and} $\\frac{t}{T_{\\text{max}}} \u003c \\text{rand()}$}\n                    \\STATE Select a random position $\\mathbf{X}_{\\text{rand}}$ \\COMMENT{Cyclone Foraging}\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_i + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i)\n                    \\]\n                \\ELSE\n                    \\STATE Update position: \\COMMENT{Chain Foraging}\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i)\n                    \\]\n                \\ENDIF\n            \\ENDFOR\n\n            \\STATE Compute fitness $f(\\mathbf{X}_i)$ for each updated position\n            \\STATE Update $\\mathbf{X}_{\\text{best}}$ if a better solution is found\n\n            \\FOR{each manta ray $i$} \\COMMENT{Somersault Foraging}\n                \\STATE Update position:\n                \\[\n                \\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i)\n                \\]\n            \\ENDFOR\n\n            \\STATE Increment iteration $t = t + 1$\n        \\ENDWHILE\n\n        \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-30 12:19:29.672229404","filePath":"null","pinned":false},{"value":"Something's wrong--perhaps a missing \\item.LaTeX\n","recorded":"2024-10-30 12:19:04.196136913","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Optimized Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n        \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n        \\STATE Set maximum iterations $T_{\\text{max}}$ and initialize $t = 0$\n        \\STATE Evaluate fitness $f(\\mathbf{X}_i)$ for each manta ray and set $\\mathbf{X}_{\\text{best}} = \\arg \\min f(\\mathbf{X}_i)$\n\n        \\WHILE{$t \u003c T_{\\text{max}}$}\n            \\FOR{each manta ray $i$}\n                \\STATE Set a random number $r_{\\text{choice}} = \\text{rand()}$\n                \n                \\IF{$r_{\\text{choice}} \u003c 0.5$ \\textbf{and} $\\frac{t}{T_{\\text{max}}} \u003c \\text{rand()}$} \\COMMENT{Cyclone Foraging}\n                    \\STATE Select a random position $\\mathbf{X}_{\\text{rand}}$\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_i + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i)\n                    \\]\n                \\ELSE \\COMMENT{Chain Foraging}\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i)\n                    \\]\n                \\ENDIF\n            \\ENDFOR\n\n            \\STATE Compute fitness $f(\\mathbf{X}_i)$ for each updated position\n            \\STATE Update $\\mathbf{X}_{\\text{best}}$ if a better solution is found\n\n            \\FOR{each manta ray $i$} \\COMMENT{Somersault Foraging}\n                \\STATE Update position:\n                \\[\n                \\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i)\n                \\]\n            \\ENDFOR\n\n            \\STATE Increment iteration $t = t + 1$\n        \\ENDWHILE\n\n        \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-30 12:18:19.441481330","filePath":"null","pinned":false},{"value":"See the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.37                     \\STATE\n                                Select a random position $\\mathbf{X}_{\\text{...\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:37: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.37                     \\STATE\n                                Select a random position $\\mathbf{X}_{\\text{...\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:37: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.37                     \\STATE\n                                Select a random position $\\mathbf{X}_{\\text{...\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:38: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.38                     \\STATE\n                                Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:38: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.38                     \\STATE\n                                Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:38: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.38                     \\STATE\n                                Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:42: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.42                 \\ELSE \\hfill\n                                  \\COMMENT{Chain Foraging}\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:43: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.43                     \\STATE\n                                Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:43: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.43                     \\STATE\n                                Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:43: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.43                     \\STATE\n                                Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:47: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.47                 \\ENDIF\n                           \n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:54: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.54                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:54: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.54                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:54: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.54                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:58: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.58             \\ENDFOR","recorded":"2024-10-30 12:17:57.451333333","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Optimized Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n        \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n        \\STATE Set maximum iterations $T_{\\text{max}}$ and initialize $t = 0$\n        \\STATE Evaluate fitness $f(\\mathbf{X}_i)$ for each manta ray and set $\\mathbf{X}_{\\text{best}} = \\arg \\min f(\\mathbf{X}_i)$\n\n        \\WHILE{$t \u003c T_{\\text{max}}$}\n            \\FOR{each manta ray $i$}\n                \\STATE Set a random number $r_{\\text{choice}} = \\text{rand()}$ \n                \n                \\IF{$r_{\\text{choice}} \u003c 0.5$ \\textbf{and} $\\frac{t}{T_{\\text{max}}} \u003c \\text{rand()}$} \\hfill \\COMMENT{Cyclone Foraging}\n                    \\STATE Select a random position $\\mathbf{X}_{\\text{rand}}$ \n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_i + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i)\n                    \\]\n                \\ELSE \\hfill \\COMMENT{Chain Foraging}\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i)\n                    \\]\n                \\ENDIF\n            \\ENDFOR\n\n            \\STATE Compute fitness $f(\\mathbf{X}_i)$ for each updated position\n            \\STATE Update $\\mathbf{X}_{\\text{best}}$ if a better solution is found\n\n            \\FOR{each manta ray $i$} \\hfill \\COMMENT{Somersault Foraging}\n                \\STATE Update position:\n                \\[\n                \\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i)\n                \\]\n            \\ENDFOR\n\n            \\STATE Increment iteration $t = t + 1$\n        \\ENDWHILE\n\n        \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-30 12:17:21.180078387","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Optimized Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n        \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n        \\STATE Set maximum iterations $T_{\\text{max}}$ and initialize $t = 0$\n        \\STATE Evaluate fitness $f(\\mathbf{X}_i)$ for each manta ray and set $\\mathbf{X}_{\\text{best}} = \\arg \\min f(\\mathbf{X}_i)$\n\n        \\WHILE{$t \u003c T_{\\text{max}}$}\n            \\FOR{each manta ray $i$}\n                \\STATE Set a random number $r_{\\text{choice}} = \\text{rand}$ \n                \n                \\IF{$r_{\\text{choice}} \u003c 0.5$ \\textbf{and} $\\frac{t}{T_{\\text{max}}} \u003c \\text{rand}$} \\hfill \\COMMENT{Cyclone Foraging}\n                    \\STATE Select a random position $\\mathbf{X}_{\\text{rand}}$ \n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_i + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i)\n                    \\]\n                \\ELSE \\hfill \\COMMENT{Chain Foraging}\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i = \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i)\n                    \\]\n                \\ENDIF\n            \\ENDFOR\n\n            \\STATE Compute fitness $f(\\mathbf{X}_i)$ for each updated position\n            \\STATE Update $\\mathbf{X}_{\\text{best}}$ if a better solution is found\n\n            \\FOR{each manta ray $i$} \\hfill \\COMMENT{Somersault Foraging}\n                \\STATE Update position:\n                \\[\n                \\mathbf{X}_i = \\mathbf{X}_i + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i)\n                \\]\n            \\ENDFOR\n\n            \\STATE Increment iteration $t = t + 1$\n        \\ENDWHILE\n\n        \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-30 12:16:48.831447193","filePath":"null","pinned":false},{"value":"\\begin{itemize}\n    \\item \\textbf{\\( N \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Number of manta rays (population size).\n        \\item \\textbf{Role}: Defines the number of candidate solutions that are evolved during the optimization process.\n    \\end{itemize}\n\n    \\item \\textbf{\\( \\mathbf{X}_i \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Position of the \\( i \\)-th manta ray in the solution space.\n        \\item \\textbf{Role}: Represents the current solution vector for each manta ray. Each position is updated iteratively based on the foraging techniques.\n    \\end{itemize}\n\n    \\item \\textbf{\\( T_{\\text{max}} \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Maximum number of iterations (stopping criterion).\n        \\item \\textbf{Role}: Determines the number of iterations or steps the algorithm will run before stopping.\n    \\end{itemize}\n\n    \\item \\textbf{\\( t \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Current iteration number.\n        \\item \\textbf{Role}: Tracks the progress of the algorithm and is incremented at each iteration until \\( T_{\\text{max}} \\) is reached.\n    \\end{itemize}\n\n    \\item \\textbf{\\( f(\\mathbf{X}_i) \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Fitness function evaluated at position \\( \\mathbf{X}_i \\).\n        \\item \\textbf{Role}: Measures the quality of the solution at position \\( \\mathbf{X}_i \\). Used to determine if a new position is better than the previous best.\n    \\end{itemize}\n\n    \\item \\textbf{\\( \\mathbf{X}_{\\text{best}} \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Best solution found so far.\n        \\item \\textbf{Role}: Stores the best solution (manta ray position) found during the optimization process, updated if a better solution is found.\n    \\end{itemize}\n\n    \\item \\textbf{\\( \\text{rand} \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Random number between 0 and 1.\n        \\item \\textbf{Role}: Used in stochastic decisions, like determining if a manta ray will use cyclone or chain foraging.\n    \\end{itemize}\n\n    \\item \\textbf{\\( \\mathbf{X}_{\\text{rand}} \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Randomly chosen manta ray‚Äôs position for movement.\n        \\item \\textbf{Role}: Used in cyclone foraging to create randomness in the movement of the manta rays.\n    \\end{itemize}\n\n    \\item \\textbf{\\( r \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Random scaling factor.\n        \\item \\textbf{Role}: Introduces variability in position updates, encouraging exploration of the solution space.\n    \\end{itemize}\n\n    \\item \\textbf{\\( \\alpha \\), \\( \\beta \\), \\( \\gamma \\)}:\n    \\begin{itemize}\n        \\item \\textbf{Description}: Control parameters (specific to the algorithm, e.g., somersault, chain, or cyclone foraging).\n        \\item \\textbf{Role}: These parameters adjust the step size and direction during updates, helping to balance exploration and exploitation.\n    \\end{itemize}\n\n    \\item \\textbf{\\( S \\)}: \n    \\begin{itemize}\n        \\item \\textbf{Description}: Somersault factor.\n        \\item \\textbf{Role}: Determines the range of movement during the somersault foraging phase, influencing the extent to which manta rays \"jump\" toward the best solution.\n    \\end{itemize}\n\n    \\item \\textbf{\\( r_2, r_3 \\)}:\n    \\begin{itemize}\n        \\item \\textbf{Description}: Additional random scaling factors in the somersault foraging.\n        \\item \\textbf{Role}: Used to control the magnitude and direction of the position updates relative to the best solution, adding variability in the somersault phase.\n    \\end{itemize}\n\\end{itemize}","recorded":"2024-10-30 12:14:30.035072055","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n    \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n    \\STATE Define maximum iterations $T_{\\text{max}}$, set iteration counter $t = 0$\n    \\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n    \\STATE Identify the best solution $\\mathbf{X}_{\\text{best}} = \\arg \\min f(\\mathbf{X}_i)$\n\n    \\WHILE{stopping criterion is not satisfied}\n        \\FOR{each manta ray $i$ from $1$ to $N$}\n            \\IF{$\\text{rand} \u003c 0.5$} \\COMMENT{Cyclone foraging}\n                \\IF{$\\frac{t}{T_{\\text{max}}} \u003c \\text{rand}$}\n                    \\STATE $\\mathbf{X}_{\\text{rand}} = \\mathbf{X}_i + \\text{rand} \\cdot (\\mathbf{X}_u - \\mathbf{X}_i)$\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i(t+1) = \n                    \\begin{cases} \n                      \\mathbf{X}_{\\text{rand}} + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                      \\mathbf{X}_{\\text{rand}} + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\ldots, N \n                    \\end{cases}\n                    \\]\n                \\ELSE\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i(t+1) = \n                    \\begin{cases} \n                      \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                      \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\ldots, N \n                    \\end{cases}\n                    \\]\n                \\ENDIF\n            \\ELSE \\COMMENT{Chain foraging}\n                \\STATE Update position:\n                \\[\n                \\mathbf{X}_i(t+1) = \n                \\begin{cases} \n                  \\mathbf{X}_i(t) + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                  \\mathbf{X}_i(t) + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\ldots, N \n                \\end{cases}\n                \\]\n            \\ENDIF\n        \\ENDFOR\n        \n        \\STATE Compute fitness $f(\\mathbf{X}_i(t+1))$ for each individual\n        \\IF{$f(\\mathbf{X}_i(t+1)) \u003c f(\\mathbf{X}_{\\text{best}})$}\n            \\STATE $\\mathbf{X}_{\\text{best}} = \\mathbf{X}_i(t+1)$\n        \\ENDIF\n\n        \\STATE \\textbf{Somersault foraging:}\n        \\FOR{each manta ray $i$ from $1$ to $N$}\n            \\STATE Update position:\n            \\[\n            \\mathbf{X}_i(t+1) = \\mathbf{X}_i(t) + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i(t))\n            \\]\n        \\ENDFOR\n        \n        \\STATE Compute fitness $f(\\mathbf{X}_i(t+1))$ for each individual\n        \\IF{$f(\\mathbf{X}_i(t+1)) \u003c f(\\mathbf{X}_{\\text{best}})$}\n            \\STATE $\\mathbf{X}_{\\text{best}} = \\mathbf{X}_i(t+1)$\n        \\ENDIF\n        \\STATE Increment iteration counter $t = t + 1$\n    \\ENDWHILE\n    \n    \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}","recorded":"2024-10-30 12:11:25.386866777","filePath":"null","pinned":false},{"value":"\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.36 ...{$\\frac{t}{T_{\\text{max}}} \u003c \\text{rand}$}\n                                                  \n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:36: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.36 ...{$\\frac{t}{T_{\\text{max}}} \u003c \\text{rand}$}\n                                                  \n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:36: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.36 ...{$\\frac{t}{T_{\\text{max}}} \u003c \\text{rand}$}\n                                                  \n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:37: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.37                     \\STATE\n                                $\\mathbf{X}_{\\text{rand}} = \\mathbf{X}_i + \\...\n\nOverfull \\hbox (63.68492pt too wide) detected at line 45\n\\OT1/cmr/bx/n/10 X[]\\OT1/cmr/m/n/10 (\\OML/cmm/m/it/10 t \\OT1/cmr/m/n/10 + 1) = []\n\nOverfull \\hbox (60.75713pt too wide) detected at line 54\n\\OT1/cmr/bx/n/10 X[]\\OT1/cmr/m/n/10 (\\OML/cmm/m/it/10 t \\OT1/cmr/m/n/10 + 1) = []\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:57: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.57                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:57: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.57                 \\STATE\n                            Update position:\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:57: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.57                 \\STATE\n                            Update position:\n\nOverfull \\hbox (50.7447pt too wide) detected at line 64\n\\OT1/cmr/bx/n/10 X[]\\OT1/cmr/m/n/10 (\\OML/cmm/m/it/10 t \\OT1/cmr/m/n/10 + 1) = []\n\n/home/karna/Downloads/CJPR-Report-main/rough.tex:65: LaTeX Error: Something's wrong--perhaps a missing \\item.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H \u003creturn\u003e  for immediate help.\n ...                                              \n                                                  \nl.65             \\ENDIF\n                       \n\nLaTeX Warning: Float too large for page by 47.60081pt on input line 90.","recorded":"2024-10-29 21:57:24.324971447","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n    \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n    \\STATE Define maximum iterations $T_{\\text{max}}$, set iteration counter $t = 0$\n    \\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n    \\STATE Identify the best solution $\\mathbf{X}_{\\text{best}} = \\arg \\min f(\\mathbf{X}_i)$\n\n    \\WHILE{stopping criterion is not satisfied}\n        \\FOR{each manta ray $i$ from $1$ to $N$}\n            \\IF{$\\text{rand} \u003c 0.5$} \\COMMENT{Cyclone foraging}\n                \\IF{$\\frac{t}{T_{\\text{max}}} \u003c \\text{rand}$}\n                    \\STATE $\\mathbf{X}_{\\text{rand}} = \\mathbf{X}_i + \\text{rand} \\cdot (\\mathbf{X}_u - \\mathbf{X}_i)$\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i(t+1) = \n                    \\begin{cases} \n                      \\mathbf{X}_{\\text{rand}} + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                      \\mathbf{X}_{\\text{rand}} + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\ldots, N \n                    \\end{cases}\n                    \\]\n                \\ELSE\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i(t+1) = \n                    \\begin{cases} \n                      \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                      \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\ldots, N \n                    \\end{cases}\n                    \\]\n                \\ENDIF\n            \\ELSE \\COMMENT{Chain foraging}\n                \\STATE Update position:\n                \\[\n                \\mathbf{X}_i(t+1) = \n                \\begin{cases} \n                  \\mathbf{X}_i(t) + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                  \\mathbf{X}_i(t) + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\ldots, N \n                \\end{cases}\n                \\]\n            \\ENDIF\n        \\ENDFOR\n        \n        \\STATE Compute fitness $f(\\mathbf{X}_i(t+1))$ for each individual\n        \\IF{$f(\\mathbf{X}_i(t+1)) \u003c f(\\mathbf{X}_{\\text{best}})$}\n            \\STATE $\\mathbf{X}_{\\text{best}} = \\mathbf{X}_i(t+1)$\n        \\ENDIF\n\n        \\STATE \\textbf{Somersault foraging:}\n        \\FOR{each manta ray $i$ from $1$ to $N$}\n            \\STATE Update position:\n            \\[\n            \\mathbf{X}_i(t+1) = \\mathbf{X}_i(t) + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i(t))\n            \\]\n        \\ENDFOR\n        \n        \\STATE Compute fitness $f(\\mathbf{X}_i(t+1))$ for each individual\n        \\IF{$f(\\mathbf{X}_i(t+1)) \u003c f(\\mathbf{X}_{\\text{best}})$}\n            \\STATE $\\mathbf{X}_{\\text{best}} = \\mathbf{X}_i(t+1)$\n        \\ENDIF\n        \\STATE Increment iteration counter $t = t + 1$\n    \\ENDWHILE\n    \n    \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-29 21:56:51.664200089","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n    \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n    \\STATE Define maximum iterations $T_{max}$, set iteration counter $t = 0$\n    \\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n    \\STATE Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$\n\n    \\WHILE{stopping criterion is not satisfied}\n        \\FOR{each manta ray $i$ from $1$ to $N$}\n            \\IF{$rand \u003c 0.5$} \\hfill \\COMMENT{Cyclone foraging}\n                \\IF{$t / T_{max} \u003c rand$}\n                    \\STATE $\\mathbf{X}_{\\text{rand}} = \\mathbf{X}_i + rand \\cdot (\\mathbf{X}_u - \\mathbf{X}_i)$\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i(t+1) = \n                    \\begin{cases} \n                      \\mathbf{X}_{\\text{rand}} + r \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                      \\mathbf{X}_{\\text{rand}} + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{rand}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\cdots, N \n                    \\end{cases}\n                    \\]\n                \\ELSE\n                    \\STATE Update position:\n                    \\[\n                    \\mathbf{X}_i(t+1) = \n                    \\begin{cases} \n                      \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                      \\mathbf{X}_{\\text{best}} + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\beta \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\cdots, N \n                    \\end{cases}\n                    \\]\n                \\ENDIF\n            \\ELSE \\hfill \\COMMENT{Chain foraging}\n                \\STATE Update position:\n                \\[\n                \\mathbf{X}_i(t+1) = \n                \\begin{cases} \n                  \\mathbf{X}_i(t) + r \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 1 \\\\ \n                  \\mathbf{X}_i(t) + r \\cdot (\\mathbf{X}_{i-1}(t) - \\mathbf{X}_i(t)) + \\alpha \\cdot (\\mathbf{X}_{\\text{best}} - \\mathbf{X}_i(t)) \u0026 \\text{if } i = 2, \\cdots, N \n                \\end{cases}\n                \\]\n            \\ENDIF\n        \\ENDFOR\n        \n        \\STATE Compute fitness $f(\\mathbf{X}_i(t+1))$ for each individual\n        \\IF{$f(\\mathbf{X}_i(t+1)) \u003c f(\\mathbf{X}_{\\text{best}})$}\n            \\STATE $\\mathbf{X}_{\\text{best}} = \\mathbf{X}_i(t+1)$\n        \\ENDIF\n\n        \\STATE \\textbf{Somersault foraging:}\n        \\FOR{each manta ray $i$ from $1$ to $N$}\n            \\STATE Update position:\n            \\[\n            \\mathbf{X}_i(t+1) = \\mathbf{X}_i(t) + S \\cdot (r_2 \\cdot \\mathbf{X}_{\\text{best}} - r_3 \\cdot \\mathbf{X}_i(t))\n            \\]\n        \\ENDFOR\n        \n        \\STATE Compute fitness $f(\\mathbf{X}_i(t+1))$ for each individual\n        \\IF{$f(\\mathbf{X}_i(t+1)) \u003c f(\\mathbf{X}_{\\text{best}})$}\n            \\STATE $\\mathbf{X}_{\\text{best}} = \\mathbf{X}_i(t+1)$\n        \\ENDIF\n        \\STATE Increment iteration counter $t = t + 1$\n    \\ENDWHILE\n    \n    \\STATE \\textbf{Return} best solution $\\mathbf{X}_{\\text{best}}$ and corresponding fitness $f(\\mathbf{X}_{\\text{best}})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-29 21:55:09.314091693","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n    \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n    \\STATE Define maximum iterations $T_{max}$, and set iteration counter $t = 0$\n    \\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n    \\STATE Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$\n\n    \\WHILE{$t \u003c T_{max}$}\n        \\FOR{each manta ray $i$}\n            \\STATE \\textbf{Chain Foraging:}\n            \\STATE \\hspace{0.5cm} Select a random manta ray $j$ from the population\n            \\STATE \\hspace{0.5cm} Update position using:\n            \\[\n            \\mathbf{X}_i = \\mathbf{X}_i + r \\cdot (\\mathbf{X}_{j} - \\mathbf{X}_i)\n            \\]\n            where $r$ is a random number in $(0, 1)$\n            \\STATE \\hspace{0.5cm} If new $\\mathbf{X}_i$ improves fitness, retain position; otherwise, revert\n\n            \\STATE \\textbf{Somersault Foraging:}\n            \\STATE \\hspace{0.5cm} Update position using:\n            \\[\n            \\mathbf{X}_i = \\mathbf{X}_{best} + \\beta \\cdot (\\mathbf{X}_i - \\mathbf{X}_{best})\n            \\]\n            where $\\beta$ is a random somersault factor in $(0, 2)$\n\n            \\STATE \\textbf{Cyclone Foraging:}\n            \\STATE \\hspace{0.5cm} Update position based on:\n            \\[\n            \\mathbf{X}_i = \\mathbf{X}_i + \\gamma \\cdot (\\mathbf{X}_{best} - \\mathbf{X}_i)\n            \\]\n            where $\\gamma$ is a convergence factor in $(0, 1)$\n        \\ENDFOR\n        \\STATE Update $\\mathbf{X}_{best}$ if a better solution is found\n        \\STATE Increment iteration counter $t = t + 1$\n    \\ENDWHILE\n    \n    \\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and its fitness $f(\\mathbf{X}_{best})$\n    \\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-29 21:52:18.802000552","filePath":"null","pinned":false},{"value":"Chain foraging is the primary foraging strategy wherein manta rays form an orderly line, usually consisting of fifty or more individuals, to seek sustenance. They identify plankton in the water and navigate towards regions with elevated quantities. Smaller male manta rays frequently swim above the females, aligning their movements. Each manta ray trails the one ahead, advancing towards both the food source and the preceding ray. This collaborative alignment enables manta rays to seize any overlooked plankton, optimizing their food consumption as they direct the plankton into their gills. ","recorded":"2024-10-29 21:49:46.553052128","filePath":"null","pinned":false},{"value":"Chain foraging is the initial foraging technique in which manta rays align in an organized line, typically in groups of fifty or more, to search for food. They detect plankton in the water and swim toward areas with higher concentrations. Smaller male manta rays often swim above the females, synchronizing with their movements. Each manta ray follows the one in front, moving toward both the food source and the preceding ray. This cooperative alignment allows the manta rays to capture any missed plankton, maximizing their food intake as they channel the plankton into their gills. ","recorded":"2024-10-29 21:49:37.454896031","filePath":"null","pinned":false},{"value":"Chain foraging is the first foraging technique. Manta rays queue up one after another to\nform an organised line when they begin to forage in groups of Fifty or more. Manta rays\ncan spot plankton in the water and swim towards it. The more plankton there is in an\narea, the better it is. Smaller male manta rays swim on top of female ones and piggyback\non their backs in time with the females‚Äô pectoral fin beats. the concentrated plankton\nthat manta rays are drawn to and want to feed plankton. Manta rays align themselves\nin a foraging chain from head to tail. Everyone but the first person moves towards the\nmeal and the person in front of it. In other words, each person gets updated throughout\neach iteration Manta rays following them will therefore catch whatever plankton that the\nones preceding them missed. They can increase their food yields and channel the most\nplankton into their gills by working together in this fashion. Figure 3.9 Depicts Manta\nRays Chain Foragin behaviour.","recorded":"2024-10-29 21:49:13.953438627","filePath":"null","pinned":false},{"value":"Chain foraging represents the initial foraging approach. Manta rays align sequentially to create an organized formation when foraging in groups of fifty or more. Manta rays can detect plankton in the water and swim towards it. An increased abundance of plankton in a region is advantageous. Smaller male manta rays ascend atop female manta rays, synchronously riding on their backs in accordance with the rhythm of the females' pectoral fin movements. The dense plankton that attracts manta rays for feeding. Manta rays position themselves in a foraging sequence from head to tail. All individuals, except for the first, advance towards the meal and the individual positioned in front of it. In other words, every individual receives updates during each repetition. Manta rays trailing after will capture any plankton that their predecessors overlooked. They can enhance their food production and direct the maximum amount of plankton into their gills by collaborating in this manner.","recorded":"2024-10-29 21:48:58.080650521","filePath":"null","pinned":false},{"value":"\\begin{figure}[H]\n        \\centering\n        \\includegraphics[width=0.8\\textwidth, height=0.35\\textwidth]{images/chain forage.png}\n        \\caption{Chain Foraging behavious in a 2-D space}\n        \\label{fig:03_02}\n    \\end{figure}","recorded":"2024-10-29 21:48:02.429167687","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/cyclone.png","recorded":"2024-10-29 21:47:54.101582509","filePath":"null","pinned":false},{"value":"Cyclone foraging is the second method of foraging. When plankton levels are significantly elevated, multiple manta rays will gather. In a spiral, their tails and heads interconnect.A rotating vortex forms in the cyclone's eye as the purified water ascends to the surface. Consequently, the plankton is ingested by their expansive mouths.A group of manta rays will form an elongated foraging chain and spiral towards a patch of plankton in deep water upon detection.","recorded":"2024-10-29 21:47:25.541784259","filePath":"null","pinned":false},{"value":"Cyclone foraging is second method of foraging. When plankton‚Äôs amount is extremely\nhigher, the numerous manta rays will congregate. In a spiral, their tail ends and heads\nconnect.A whirling vertex develops in the cyclone‚Äôs eye as the filtered water rises to the\nsurface. As a result, the plankton gets drawn into their large jaws.A cluster of manta rays\n38\nwill create a long foraging chain and travel in a spiral towards the food when they spot a\nplankton\"s patch in the deep water.","recorded":"2024-10-29 21:47:04.234680681","filePath":"null","pinned":false},{"value":"\\item \\textbf{Cyclone Foraging}: Directs manta rays to aggregate around the optimal solution identified to date by emulating cyclone-like motions.","recorded":"2024-10-29 21:46:53.027630395","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/chain forage.png","recorded":"2024-10-29 21:38:45.020138249","filePath":"null","pinned":false},{"value":"10.1016/j.engappai.2019.103300","recorded":"2024-10-29 21:38:00.937719332","filePath":"null","pinned":false},{"value":"Chain Foraging\n(Weiguo Zhao, Zhang, and L. Wang 2020)","recorded":"2024-10-29 21:37:36.298163539","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102912.png","recorded":"2024-10-29 21:36:29.875572883","filePath":"null","pinned":false},{"value":"Chain foraging represents the initial foraging approach. Manta rays align sequentially to create an organized formation when foraging in groups of fifty or more. Manta rays can detect plankton in the water and swim towards it. An increased abundance of plankton in a region is advantageous. Smaller male manta rays ascend atop female manta rays, synchronously riding on their backs in accordance with the rhythm of the females' pectoral fin movements. The dense plankton that attracts manta rays for feeding. Manta rays position themselves in a foraging sequence from head to tail. All individuals, except for the first, advance towards the meal and the individual positioned in front of it. In other words, every individual receives updates during each repetition. Manta rays trailing after will capture any plankton that their predecessors overlooked. They can enhance their food production and direct the maximum amount of plankton into their gills by collaborating in this manner. Figure 3.9 Illustrates the chain foraging behavior of manta rays.","recorded":"2024-10-29 21:34:37.061799396","filePath":"null","pinned":false},{"value":"\\subsection{Parameter Descriptions}\n\\begin{itemize}\n    \\item $\\mathbf{X}_i$: Position of the $i$-th manta ray in the solution space.\n    \\item $\\mathbf{X}_{best}$: Position of the best solution found so far.\n    \\item $rand$: Random variable that helps in stochastic movement.\n    \\item $\\beta$: Somersault factor to control the impact of the somersault movement.\n    \\item $\\gamma$: Convergence factor used in cyclone foraging.\n\\end{itemize}","recorded":"2024-10-29 21:30:51.412692210","filePath":"null","pinned":false},{"value":"\\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and corresponding fitness $f(\\mathbf{X}_{best})$","recorded":"2024-10-29 21:30:04.639502322","filePath":"null","pinned":false},{"value":"Update $\\mathbf{X}_{best}$ if a better solution is found\n    \\STATE Increment iteration $t = t + 1$","recorded":"2024-10-29 21:29:52.662080647","filePath":"null","pinned":false},{"value":"\\STATE Update position $\\mathbf{X}_i$ based on chain foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + rand \\cdot (\\mathbf{X}_{j} - \\mathbf{X}_i)$, where $j$ is a randomly chosen manta ray\n        \\STATE If $\\mathbf{X}_i$ improves fitness, retain new position, else revert\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on somersault foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_{best} + \\beta \\cdot (\\mathbf{X}_i - \\mathbf{X}_{best})$, where $\\beta$ is a random somersault factor\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on cyclone foraging (towards $\\mathbf{X}_{best}$):\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + \\gamma \\cdot (\\mathbf{X}_{best} - \\mathbf{X}_i)$, where $\\gamma$ is a convergence factor","recorded":"2024-10-29 21:29:12.660153422","filePath":"null","pinned":false},{"value":"each manta ray $i$","recorded":"2024-10-29 21:29:04.335642881","filePath":"null","pinned":false},{"value":"t \u003c T_{max}$","recorded":"2024-10-29 21:28:51.557354350","filePath":"null","pinned":false},{"value":"Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$","recorded":"2024-10-29 21:28:40.105006675","filePath":"null","pinned":false},{"value":"Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray","recorded":"2024-10-29 21:28:34.851121776","filePath":"null","pinned":false},{"value":"Define maximum iterations $T_{max}$, and set $t = 0$","recorded":"2024-10-29 21:28:25.511750488","filePath":"null","pinned":false},{"value":"Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$","recorded":"2024-10-29 21:28:01.855486964","filePath":"null","pinned":false},{"value":"\\caption{Manta Ray Foraging Optimization (MRFO)}","recorded":"2024-10-29 21:26:55.817418901","filePath":"null","pinned":false},{"value":"\\begin{algorithmic}[1]\n\\State \\textbf{Initialize} population of solutions $X_i$ for $i = 1, 2, \\ldots, N$ and parameters\n\\State \\textbf{Evaluate} the fitness of each solution\n\\State \\textbf{Set} the best solution found as $X_{best}$\n\\While{termination criteria not met}\n    \\For{each solution $X_i$}\n        \\State \\textbf{Chain Foraging:} Update $X_i$ using chain foraging strategy\n        \\[\n        X_i^{t+1} = X_i^t + r \\cdot (X_j^t - X_i^t)\n        \\]\n        where $X_j$ is a random solution, and $r$ is a random number in $[0,1]$\n        \n        \\State \\textbf{Cyclone Foraging:} Update $X_i$ using cyclone foraging strategy\n        \\[\n        X_i^{t+1} = X_{best}^t + r \\cdot (X_i^t - X_{best}^t)\n        \\]\n        where $X_{best}$ is the current best solution\n        \n        \\State \\textbf{Somersault Foraging:} Update $X_i$ using somersault foraging strategy if certain criteria are met\n        \\[\n        X_i^{t+1} = X_i^{t} + S \\cdot (X_{best}^t - X_i^t)\n        \\]\n        where $S$ is the somersault factor\n    \\EndFor\n    \\State \\textbf{Evaluate} fitness of updated solutions and update $X_{best}$ if a better solution is found\n\\EndWhile\n\\end{algorithmic}\n\\end{algorithm}","recorded":"2024-10-29 21:24:55.589564486","filePath":"null","pinned":false},{"value":"\\section{Manta Ray Foraging Optimization Algorithm}\n\nThe Manta Ray Foraging Optimization (MRFO) algorithm is inspired by the foraging behaviors of manta rays in the ocean. This metaheuristic algorithm mimics three key foraging strategies: chain foraging, cyclone foraging, and somersault foraging. The algorithm is typically represented as follows:\n\n\\begin{algorithm}[H]\n\\caption{Manta Ray Foraging Optimization (MRFO) Algorithm}\n\\begin{algorithmic}[1]\n\\State \\textbf{Initialize} population of solutions $X_i$ for $i = 1, 2, \\ldots, N$ and parameters\n\\State \\textbf{Evaluate} the fitness of each solution\n\\State \\textbf{Set} the best solution found as $X_{best}$\n\\While{termination criteria not met}\n    \\For{each solution $X_i$}\n        \\State \\textbf{Chain Foraging:} Update $X_i$ using chain foraging strategy\n        \\[\n        X_i^{t+1} = X_i^t + r \\cdot (X_j^t - X_i^t)\n        \\]\n        where $X_j$ is a random solution, and $r$ is a random number in $[0,1]$\n        \n        \\State \\textbf{Cyclone Foraging:} Update $X_i$ using cyclone foraging strategy\n        \\[\n        X_i^{t+1} = X_{best}^t + r \\cdot (X_i^t - X_{best}^t)\n        \\]\n        where $X_{best}$ is the current best solution\n        \n        \\State \\textbf{Somersault Foraging:} Update $X_i$ using somersault foraging strategy if certain criteria are met\n        \\[\n        X_i^{t+1} = X_i^{t} + S \\cdot (X_{best}^t - X_i^t)\n        \\]\n        where $S$ is the somersault factor\n    \\EndFor\n    \\State \\textbf{Evaluate} fitness of updated solutions and update $X_{best}$ if a better solution is found\n\\EndWhile\n\\end{algorithmic}\n\\end{algorithm}\n\n\\noindent\nThe parameters and strategies are defined as:\n\\begin{itemize}\n    \\item $X_i$: Position of the $i$-th manta ray (solution)\n    \\item $X_{best}$: Best position found so far\n    \\item $r$: Random number within $[0, 1]$\n    \\item $S$: Somersault factor, a predefined parameter to adjust somersaulting distance\n\\end{itemize}\n","recorded":"2024-10-29 21:24:18.984042529","filePath":"null","pinned":false},{"value":"State","recorded":"2024-10-29 21:22:11.811899297","filePath":"null","pinned":false},{"value":"Manta ray Foraging Algorithm","recorded":"2024-10-29 21:21:12.032089359","filePath":"null","pinned":false},{"value":"\\begin{algorithm}[H]\n\\caption{Convolutional Neural Network for Image Classification}\n\\label{algo:CNN}\n\\begin{algorithmic}[1]\n\\Require Input image $I$ of dimension $H \\times W \\times D$\n\\Require Trained CNN model $M$ with $L$ layers: $\\{L_1, L_2, \\dots, L_L\\}$\n\\Ensure Classification label $\\hat{y}$\n\n\\Procedure{CNN\\_Classification}{$I, M$}\n    \\State Initialize feature map $F = I$\n    \n    \\For{each layer $l = 1$ to $L$}\n        \\If{$L_l$ is a \\textbf{Convolutional Layer}}\n            \\State Apply filter $K_l$ of size $k \\times k$ with stride $s$ and padding $p$ on $F$\n            \\State Compute feature map $F_l$ using $F_l = ReLU(F \\ast K_l + b_l)$\n        \n        \\ElsIf{$L_l$ is a \\textbf{Pooling Layer}}\n            \\State Apply pooling function $P_l$ (e.g., max or average pooling)\n            \\State Compute pooled feature map $F_l = P_l(F)$\n        \n        \\ElsIf{$L_l$ is a \\textbf{Fully Connected (FC) Layer}}\n            \\State Flatten $F$ into a vector $v$\n            \\State Compute output vector $v = Softmax(W_l v + b_l)$\n        \\EndIf\n        \\State Update feature map $F = F_l$\n    \\EndFor\n\n    \\State \\Return $\\hat{y} = \\arg \\max(v)$ \\Comment{Predicted class label}\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}","recorded":"2024-10-29 21:20:49.367157631","filePath":"null","pinned":false},{"value":"https://meet.google.com/hna-misn-scf","recorded":"2024-10-29 20:41:15.935031034","filePath":"null","pinned":false},{"value":"\\end{document}","recorded":"2024-10-29 13:15:11.199188280","filePath":"null","pinned":false},{"value":"\\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\\usepackage{amsmath}\n\n\\begin{document}","recorded":"2024-10-29 13:15:01.117835640","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n\\caption{Manta Ray Foraging Optimization (MRFO)}\n\\begin{algorithmic}[1]\n\\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n\\STATE Define maximum iterations $T_{max}$, and set $t = 0$\n\\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n\\STATE Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$\n\n\\WHILE{$t \u003c T_{max}$}\n    \\FOR{each manta ray $i$}\n        \\STATE Update position $\\mathbf{X}_i$ based on chain foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + rand \\cdot (\\mathbf{X}_{j} - \\mathbf{X}_i)$, where $j$ is a randomly chosen manta ray\n        \\STATE If $\\mathbf{X}_i$ improves fitness, retain new position, else revert\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on somersault foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_{best} + \\beta \\cdot (\\mathbf{X}_i - \\mathbf{X}_{best})$, where $\\beta$ is a random somersault factor\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on cyclone foraging (towards $\\mathbf{X}_{best}$):\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + \\gamma \\cdot (\\mathbf{X}_{best} - \\mathbf{X}_i)$, where $\\gamma$ is a convergence factor\n    \\ENDFOR\n    \\STATE Update $\\mathbf{X}_{best}$ if a better solution is found\n    \\STATE Increment iteration $t = t + 1$\n\\ENDWHILE\n\n\\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and corresponding fitness $f(\\mathbf{X}_{best})$\n\\end{algorithmic}\n\\end{algorithm}","recorded":"2024-10-29 13:14:46.063471672","filePath":"null","pinned":false},{"value":"\\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and corresponding fitness $f(\\mathbf{X}_{best})$\n]","recorded":"2024-10-29 13:12:32.282504575","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n    \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n    \\STATE Define maximum iterations $T_{max}$, and set $t = 0$\n    \\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n    \\STATE Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$\n    \n    \\WHILE{$t \u003c T_{max}$}\n        \\FOR{each manta ray $i$}\n            \\STATE Update position $\\mathbf{X}_i$ based on chain foraging:\n            \\STATE $\\mathbf{X}_i = \\mathbf{X}_i + rand \\cdot (\\mathbf{X}_{j} - \\mathbf{X}_i)$, where $j$ is a randomly chosen manta ray\n            \\STATE If $\\mathbf{X}_i$ improves fitness, retain new position, else revert\n            \n            \\STATE Update position $\\mathbf{X}_i$ based on somersault foraging:\n            \\STATE $\\mathbf{X}_i = \\mathbf{X}_{best} + \\beta \\cdot (\\mathbf{X}_i - \\mathbf{X}_{best})$, where $\\beta$ is a random somersault factor\n            \n            \\STATE Update position $\\mathbf{X}_i$ based on cyclone foraging (towards $\\mathbf{X}_{best}$):\n            \\STATE $\\mathbf{X}_i = \\mathbf{X}_i + \\gamma \\cdot (\\mathbf{X}_{best} - \\mathbf{X}_i)$, where $\\gamma$ is a convergence factor\n        \\ENDFOR\n        \\STATE Update $\\mathbf{X}_{best}$ if a better solution is found\n        \\STATE Increment iteration $t = t + 1$\n    \\ENDWHILE\n    \n    \\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and corresponding fitness $f(\\mathbf{X}_{best})$\n    \\end{algorithmic}\n\\end{algorithm}","recorded":"2024-10-29 13:11:36.272894081","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n    \\caption{Manta Ray Foraging Optimization (MRFO)}\n    \\begin{algorithmic}[1]\n    \\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n    \\STATE Define maximum iterations $T_{max}$, and set $t = 0$\n    \\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n    \\STATE Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$\n    \n    \\WHILE{$t \u003c T_{max}$}\n        \\FOR{each manta ray $i$}\n            \\STATE Update position $\\mathbf{X}_i$ based on chain foraging:\n            \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + rand \\cdot (\\mathbf{X}_{j} - \\mathbf{X}_i)$, where $j$ is a randomly chosen manta ray\n            \\STATE If $\\mathbf{X}_i$ improves fitness, retain new position, else revert\n            \n            \\STATE Update position $\\mathbf{X}_i$ based on somersault foraging:\n            \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_{best} + \\beta \\cdot (\\mathbf{X}_i - \\mathbf{X}_{best})$, where $\\beta$ is a random somersault factor\n            \n            \\STATE Update position $\\mathbf{X}_i$ based on cyclone foraging (towards $\\mathbf{X}_{best}$):\n            \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + \\gamma \\cdot (\\mathbf{X}_{best} - \\mathbf{X}_i)$, where $\\gamma$ is a convergence factor\n        \\ENDFOR\n        \\STATE Update $\\mathbf{X}_{best}$ if a better solution is found\n        \\STATE Increment iteration $t = t + 1$\n    \\ENDWHILE\n    \n    \\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and corresponding fitness $f(\\mathbf{X}_{best})$\n    \\end{algorithmic}\n\\end{algorithm}","recorded":"2024-10-29 13:10:40.776654536","filePath":"null","pinned":false},{"value":"\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\\usepackage{amsmath}","recorded":"2024-10-29 13:09:45.470917501","filePath":"null","pinned":false},{"value":"\\begin{algorithm}\n\\caption{Manta Ray Foraging Optimization (MRFO)}\n\\begin{algorithmic}[1]\n\\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n\\STATE Define maximum iterations $T_{max}$, and set $t = 0$\n\\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n\\STATE Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$\n\n\\WHILE{$t \u003c T_{max}$}\n    \\FOR{each manta ray $i$}\n        \\STATE Update position $\\mathbf{X}_i$ based on chain foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + rand \\cdot (\\mathbf{X}_{j} - \\mathbf{X}_i)$, where $j$ is a randomly chosen manta ray\n        \\STATE If $\\mathbf{X}_i$ improves fitness, retain new position, else revert\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on somersault foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_{best} + \\beta \\cdot (\\mathbf{X}_i - \\mathbf{X}_{best})$, where $\\beta$ is a random somersault factor\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on cyclone foraging (towards $\\mathbf{X}_{best}$):\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + \\gamma \\cdot (\\mathbf{X}_{best} - \\mathbf{X}_i)$, where $\\gamma$ is a convergence factor\n    \\ENDFOR\n    \\STATE Update $\\mathbf{X}_{best}$ if a better solution is found\n    \\STATE Increment iteration $t = t + 1$\n\\ENDWHILE\n\n\\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and corresponding fitness $f(\\mathbf{X}_{best})$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Parameter Descriptions}\n\\begin{itemize}\n    \\item $\\mathbf{X}_i$: Position of the $i$-th manta ray in the solution space.\n    \\item $\\mathbf{X}_{best}$: Position of the best solution found so far.\n    \\item $rand$: Random variable that helps in stochastic movement.\n    \\item $\\beta$: Somersault factor to control the impact of the somersault movement.\n    \\item $\\gamma$: Convergence factor used in cyclone foraging.\n\\end{itemize}","recorded":"2024-10-29 13:09:07.762544162","filePath":"null","pinned":false},{"value":"(A) Manta Ray (B) Structure of Manta ray","recorded":"2024-10-29 13:07:39.410207398","filePath":"null","pinned":false},{"value":"The Manta Ray Foraging Optimization (MRFO) algorithm is derived from the distinctive foraging techniques employed by manta rays in their natural habitat. Manta rays display activities including chain foraging and somersault foraging during their search for sustenance in the ocean. These strategies improve the pursuit of optimal solutions by equilibrating exploration (diverse search) and exploitation (local refinement) within a solution space.\n\nThe MRFO algorithm is particularly efficacious for high-dimensional and intricate optimization challenges. The process has three primary stages:\n\\begin{itemize} \n\\item \\textbf{Chain Foraging:} Simulates the synchronized movement of manta rays in a chain formation to explore an extensive area for sustenance.\n    \\item \\textbf{Somersault Foraging:} Enables manta rays to perform somersault maneuvers to enhance their seek in promising areas.\n    \\item \\textbf{Cyclone Foraging}: Directs manta rays to aggregate around the optimal solution identified to date by emulating cyclone-like motions.\n\\end{itemize}","recorded":"2024-10-29 13:03:22.040514482","filePath":"null","pinned":false},{"value":"The Manta Ray Foraging Optimization (MRFO) algorithm is derived from the distinctive foraging techniques employed by manta rays in their natural habitat. Manta rays display activities including chain foraging and somersault foraging during their search for sustenance in the ocean. These strategies improve the pursuit of optimal solutions by equilibrating exploration (diverse search) and exploitation (local refinement) within a solution space.\n\nThe MRFO algorithm is particularly efficacious for high-dimensional and intricate optimization challenges. The process has three primary stages:\nBegin{itemize} \\item \\textbfChain Foraging: Simulates the synchronized movement of manta rays in a chain formation to explore an extensive area for sustenance.\n    \\item \\textbf{}Somersault Foraging: Enables manta rays to perform somersault maneuvers to enhance their seek in promising areas.\n    \\item \\textbf{}{Cyclone Foraging}: Directs manta rays to aggregate around the optimal solution identified to date by emulating cyclone-like motions.\n\\end{itemize}","recorded":"2024-10-29 13:01:38.280475552","filePath":"null","pinned":false},{"value":"The Manta Ray Foraging Optimization (MRFO) algorithm is inspired by the unique foraging strategies of manta rays in nature. Manta rays exhibit behaviors such as chain foraging and somersault foraging while searching for food in the ocean. These techniques enhance the search for optimal solutions by balancing exploration (diverse search) and exploitation (local refinement) in a solution space.\n\nThe MRFO algorithm is especially effective for high-dimensional and complex optimization problems. Its process involves three main stages:\n\\begin{itemize}\n    \\item \\textbf{Chain Foraging}: Simulates the coordinated movement of manta rays in a chain to search a broader area for food.\n    \\item \\textbf{Somersault Foraging}: Allows manta rays to make somersault movements to refine their search near promising regions.\n    \\item \\textbf{Cyclone Foraging}: Guides manta rays to converge towards the best solution found so far by mimicking cyclone-like movements.\n\\end{itemize}","recorded":"2024-10-29 13:01:12.858142770","filePath":"null","pinned":false},{"value":"\\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\section{Manta Ray Foraging Optimization (MRFO) Algorithm}\n\nThe Manta Ray Foraging Optimization (MRFO) algorithm is inspired by the unique foraging strategies of manta rays in nature. Manta rays exhibit behaviors such as chain foraging and somersault foraging while searching for food in the ocean. These techniques enhance the search for optimal solutions by balancing exploration (diverse search) and exploitation (local refinement) in a solution space.\n\nThe MRFO algorithm is especially effective for high-dimensional and complex optimization problems. Its process involves three main stages:\n\\begin{itemize}\n    \\item \\textbf{Chain Foraging}: Simulates the coordinated movement of manta rays in a chain to search a broader area for food.\n    \\item \\textbf{Somersault Foraging}: Allows manta rays to make somersault movements to refine their search near promising regions.\n    \\item \\textbf{Cyclone Foraging}: Guides manta rays to converge towards the best solution found so far by mimicking cyclone-like movements.\n\\end{itemize}\n\n\\subsection{Algorithm for MRFO}\n\nThe pseudocode for MRFO is provided below.\n\n\\begin{algorithm}\n\\caption{Manta Ray Foraging Optimization (MRFO)}\n\\begin{algorithmic}[1]\n\\STATE Initialize population of $N$ manta rays with random positions $\\mathbf{X}_i$ for $i = 1, \\ldots, N$\n\\STATE Define maximum iterations $T_{max}$, and set $t = 0$\n\\STATE Evaluate the fitness $f(\\mathbf{X}_i)$ of each manta ray\n\\STATE Identify the best solution $\\mathbf{X}_{best} = \\arg \\min f(\\mathbf{X}_i)$\n\n\\WHILE{$t \u003c T_{max}$}\n    \\FOR{each manta ray $i$}\n        \\STATE Update position $\\mathbf{X}_i$ based on chain foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + rand \\cdot (\\mathbf{X}_{j} - \\mathbf{X}_i)$, where $j$ is a randomly chosen manta ray\n        \\STATE If $\\mathbf{X}_i$ improves fitness, retain new position, else revert\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on somersault foraging:\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_{best} + \\beta \\cdot (\\mathbf{X}_i - \\mathbf{X}_{best})$, where $\\beta$ is a random somersault factor\n        \n        \\STATE Update position $\\mathbf{X}_i$ based on cyclone foraging (towards $\\mathbf{X}_{best}$):\n        \\STATE \\hspace{0.5cm} $\\mathbf{X}_i = \\mathbf{X}_i + \\gamma \\cdot (\\mathbf{X}_{best} - \\mathbf{X}_i)$, where $\\gamma$ is a convergence factor\n    \\ENDFOR\n    \\STATE Update $\\mathbf{X}_{best}$ if a better solution is found\n    \\STATE Increment iteration $t = t + 1$\n\\ENDWHILE\n\n\\STATE \\textbf{Return} best solution $\\mathbf{X}_{best}$ and corresponding fitness $f(\\mathbf{X}_{best})$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Parameter Descriptions}\n\\begin{itemize}\n    \\item $\\mathbf{X}_i$: Position of the $i$-th manta ray in the solution space.\n    \\item $\\mathbf{X}_{best}$: Position of the best solution found so far.\n    \\item $rand$: Random variable that helps in stochastic movement.\n    \\item $\\beta$: Somersault factor to control the impact of the somersault movement.\n    \\item $\\gamma$: Convergence factor used in cyclone foraging.\n\\end{itemize}\n\nThe MRFO algorithm effectively balances exploration and exploitation through its iterative process of chain, somersault, and cyclone foraging behaviors. By utilizing these movements, MRFO efficiently searches complex solution spaces to find optimal solutions.\n\n\\end{document}\n","recorded":"2024-10-29 13:00:31.473424031","filePath":"null","pinned":false},{"value":" Manta ray Foraging Algorithm","recorded":"2024-10-29 12:59:05.824328828","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/MobileNet.png","recorded":"2024-10-29 12:55:33.558349545","filePath":"null","pinned":false},{"value":"\\documentclass{article}\n\\usepackage{tikz}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usetikzlibrary{positioning, calc, shapes.geometric}\n\n\\begin{document}\n\n\\begin{figure}[h]\n\\centering\n\\begin{tikzpicture}[\n    layer/.style={draw, rectangle, minimum height=2cm, minimum width=1.5cm},\n    arrow/.style={-\u003e, thick},\n    text depth=0.5ex, text height=1.5ex,\n    font=\\footnotesize\n]\n\n% Input Layer\n\\node[layer, fill=gray!20] (input) at (0,0) {\\begin{tabular}{c} Input \\\\ $224 \\times 224 \\times 3$ \\end{tabular}};\n\n% First Conv Layer\n\\node[layer, fill=blue!20, right=1.5cm of input] (conv1) {\\begin{tabular}{c} Conv \\\\ $3 \\times 3$ \\\\ $32@112 \\times 112$ \\end{tabular}};\n\\draw[arrow] (input) -- (conv1);\n\n% Depthwise Separable Convolutions\n\\node[layer, fill=blue!10, right=1.5cm of conv1] (depthwise1) {\\begin{tabular}{c} Depthwise \\\\ Conv \\\\ $32@112 \\times 112$ \\end{tabular}};\n\\draw[arrow] (conv1) -- (depthwise1);\n\n\\node[layer, fill=blue!10, right=1.5cm of depthwise1] (pointwise1) {\\begin{tabular}{c} Pointwise \\\\ Conv \\\\ $1 \\times 1$ \\\\ $32@112 \\times 112$ \\end{tabular}};\n\\draw[arrow] (depthwise1) -- (pointwise1);\n\n\\node[layer, fill=blue!10, right=1.5cm of pointwise1] (depthwise2) {\\begin{tabular}{c} Depthwise \\\\ Conv \\\\ $3 \\times 3$ \\\\ $64@112 \\times 112$ \\end{tabular}};\n\\draw[arrow] (pointwise1) -- (depthwise2);\n\n% Pooling and Next Layers\n\\node[layer, fill=blue!20, right=2cm of depthwise2] (pointwise2) {\\begin{tabular}{c} Pointwise \\\\ Conv \\\\ $1 \\times 1$ \\\\ $128@56 \\times 56$ \\end{tabular}};\n\\draw[arrow] (depthwise2) -- (pointwise2);\n\n% Dotted line for repeating layers\n\\node[right=1.5cm of pointwise2] (dots) {$\\cdots$};\n\\draw[arrow] (pointwise2) -- (dots);\n\n% Last Convolution Block\n\\node[layer, fill=blue!10, right=1.5cm of dots] (depthwiseN) {\\begin{tabular}{c} Depthwise \\\\ Conv \\\\ $1024@7 \\times 7$ \\end{tabular}};\n\\draw[arrow] (dots) -- (depthwiseN);\n\n\\node[layer, fill=blue!10, right=1.5cm of depthwiseN] (pointwiseN) {\\begin{tabular}{c} Pointwise \\\\ Conv \\\\ $1 \\times 1$ \\\\ $1024@7 \\times 7$ \\end{tabular}};\n\\draw[arrow] (depthwiseN) -- (pointwiseN);\n\n% Global Average Pooling\n\\node[layer, fill=gray!20, right=2cm of pointwiseN] (pooling) {\\begin{tabular}{c} Global \\\\ Average \\\\ Pooling \\\\ $1024$ \\end{tabular}};\n\\draw[arrow] (pointwiseN) -- (pooling);\n\n% Fully Connected Layer\n\\node[layer, fill=blue!20, right=1.5cm of pooling] (fc) {\\begin{tabular}{c} Fully \\\\ Connected \\\\ Layer \\\\ $1024$ \\end{tabular}};\n\\draw[arrow] (pooling) -- (fc);\n\n% Output Layer\n\\node[layer, fill=gray!20, right=1.5cm of fc] (output) {\\begin{tabular}{c} Output \\\\ Classes \\end{tabular}};\n\\draw[arrow] (fc) -- (output);\n\n\\end{tikzpicture}\n\\caption{MobileNet Model Architecture}\n\\end{figure}\n\n\\end{document}\n","recorded":"2024-10-29 12:54:37.884677087","filePath":"null","pinned":false},{"value":"Mobilenet Model Architecture","recorded":"2024-10-29 12:52:58.734003428","filePath":"null","pinned":false},{"value":"The MobileNet design is based on an inverted residual structure, where both the input and output of the residuals are utilized. In contrast to traditional residual models, blocks consist of slender bottleneck layers that utilize expanded representations in the input (Sandler et al. 2018). MobileNet utilizes compact depth-wise convolutions to filter features in the subsequent expansion layer. The non-linearities in the thin layers were diminished to preserve representational integrity. MobileNet seeks to deliver an efficient and compact model for tasks such as object detection, semantic segmentation, and image classification on devices with constrained memory and processing capabilities.The primary premise of MobileNet is the utilization of depth-wise separable convolutions. Traditional convolutions need substantial computational resources, as they utilize a single filter across the whole input volume. In contrast, depthwise separable convolutions partition the convolution into two stages: a depth-dependent convolution and a pointwise convolution. This split significantly reduces computation costs while essential regional and channel-wise data is still being collected.The MobileNet design achieves a commendable equilibrium between model size and accuracy. It provides multiple versions with improved performance and efficiency, namely MobileNetV1, MobileNetV2, and MobileNetV3.","recorded":"2024-10-29 12:49:57.679028000","filePath":"null","pinned":false},{"value":"The foundation of the MobileNet design is a residual structure that is inverted, in which\nboth the input and the output of the remaining Unlike conventional residual models,\nblock are thin bottleneck layers. which use enlarged depictions in the input (Sandler\net al. 2018). MobileNet filters the characteristics in the succeeding expansion layer us-\ning compact depth-wise convolutions. Non-linearities in the thin layers were also re-\nduced to retain representational strength. MobileNet aims to provide an effective and\ncompact model for operations like object detection, semantic segmentation, and picture\nclassification on hardware with limited memory and processing capacity.Utilising depth-\nwise separable convolutions is MobileNet‚Äôs main principle. It costs a lot of computer\npower to perform traditional convolutions, which deploy just one filters to the whole in-\nput volume. As opposed to this, depthwise separable convolutions divide the convolution\nin two phases a depth-dependent convolution and a point-by-point convolution. While\nstill gathering relevant geographical and channel-wise data, this separation considerably\nlowers the computing cost.Model size and accuracy are well-balanced in the MobileNet\ndesign. It offers various iterations with enhanced performance and efficiency, including\nMobileNetV1, MobileNetV2, and MobileNetV3.","recorded":"2024-10-29 12:49:31.392707461","filePath":"null","pinned":false},{"value":"Mobilenet Model Architecture\n(W. Wang et al. 2020)","recorded":"2024-10-29 12:48:57.473882879","filePath":"null","pinned":false},{"value":"VGG Model Architecture\n(Simonyan and Zisserman 2014)","recorded":"2024-10-29 11:52:21.832187246","filePath":"null","pinned":false},{"value":"VGG16 is a 16-layer Convolutional Neural Network (CNN) architecture. A pre-processing technique known as CNN is employed with VGG-16. This technique is extensively employed for image analysis, as each convolutional layer consists of several 3x3 filters (Ferrer-S√°nchez et al. 2022). Compared to conventional multi-layer neural networks, the connectivity layer has been augmented. It employs a concurrent hierarchy of many non-linear activation layers and convolutional layers, which is superior to a singular convolution. The layer architecture facilitates improved image feature extraction, employs max pooling for downsampling, and utilizes the Rectified Linear Unit (ReLU) as the activation function, which identifies the most significant features within the image's region as pooled values. The downsampling layer is typically employed to reduce the number of features while preserving the essential attributes of the sample and enhancing the network's resilience against distortion in the image. The VGG16 CNN architecture is reported to achieve exceptional accuracy in image recognition on extensive datasets such as ImageNet. The activation function employed is the Rectified Linear Unit (ReLU), utilized at all phases. Fully connected layers consist of 4096 nodes, utilized at the conclusion of the design illustrated in Figure 3.6. Numerous applications utilize the VGG architecture.","recorded":"2024-10-29 11:50:43.447934234","filePath":"null","pinned":false},{"value":"VGG16 is a 16-layer Convolutional Neural Network (CNN) architecture. A pre-processing\ntechnique called CNN is used with the VGG-16. It is a widely used technique for image\nanalysis since each convolutional layer comprises several 3x3 filters (Ferrer-S√°nchez et al.\n2022). In comparison to traditional multi-layer neural networks, the connection layer has\nbeen increased. It uses a simultaneous hierarchy of many non-linear activation layers and\nconvolutional layers, which is superior than a single convolution. The layer configuration\nenables enhanced image feature extraction, Max=pooling down sampling, and conver-\nsion of the Reflected linear-unit (ReLU) as the activation function, which determines the\nhighest importance within the picture‚Äôs area as the pooled and value of the region. The\ndown sampling layer is generally used to decrease the amount of characteristics while\nmaintaining the sample‚Äôs critical properties and improving the network‚Äôs anti-distortion\ncapabilities for the image. On huge datasets like ImageNet, the VGG16 CNN architecture\nis claimed to produce excellent accuracy for image recognition. The activation function is\nthe Rectified Linear Unit (ReLU), which is used in all stages. 4096 nodes make up fully\nconnected layers, which are employed at the very end of the architecture seen in Figure\n3.6. Many applications employ the VGG architecture.","recorded":"2024-10-29 11:50:30.787345443","filePath":"null","pinned":false},{"value":"Resnet Model Architecture","recorded":"2024-10-29 11:43:46.165483058","filePath":"null","pinned":false},{"value":" \\item \\textbf{Stage 1:}","recorded":"2024-10-29 11:42:46.874514880","filePath":"null","pinned":false},{"value":"The ResNet50 architecture is structured as follows:\n\\begin{itemize}\n    Stage 1: Initial convolutional layer with a \\(7 \\times 7\\) kernel size and a stride of 2, succeeded by max pooling.\n    Stage 2: Three residual blocks, each including three convolutional layers with dimensions \\(1 \\times 1\\), \\(3 \\times 3\\), and \\(1 \\times 1\\).\n    Stage 3: Four residual blocks with same architecture.\n    Stage 4: Six residual blocks with a same structure.\n    Stage 5: Three residual blocks of identical architecture, succeeded by global average pooling and a fully connected layer.\n\\end{itemize}","recorded":"2024-10-29 11:42:09.976667970","filePath":"null","pinned":false},{"value":"The ResNet50 model is structured as follows:\n\\begin{itemize}\n    \\item \\textbf{Stage 1:} Initial convolutional layer with a \\(7 \\times 7\\) kernel size and stride of 2, followed by max pooling.\n    \\item \\textbf{Stage 2:} 3 residual blocks, each with three convolutional layers of sizes \\(1 \\times 1\\), \\(3 \\times 3\\), and \\(1 \\times 1\\).\n    \\item \\textbf{Stage 3:} 4 residual blocks with the same structure.\n    \\item \\textbf{Stage 4:} 6 residual blocks with the same structure.\n    \\item \\textbf{Stage 5:} 3 residual blocks with the same structure, followed by global average pooling and a fully connected layer.\n\\end{itemize}","recorded":"2024-10-29 11:41:52.550930359","filePath":"null","pinned":false},{"value":" Resnet Model Architecture\n(Fang et al. 2021)","recorded":"2024-10-29 11:41:33.057068839","filePath":"null","pinned":false},{"value":"Here, \\( y \\) denotes the output, \\( x \\) signifies the input, and \\( \\mathcal{F}(x, \\{W_i\\}) \\) indicates the residual function, which often consists of a series of two or more convolutional layers. This architecture alleviates the degradation issue faced during the training of extremely deep networks.","recorded":"2024-10-29 11:41:15.158076149","filePath":"null","pinned":false},{"value":"where \\( y \\) is the output, \\( x \\) is the input, and \\( \\mathcal{F}(x, \\{W_i\\}) \\) represents the residual function, typically a stack of two or more convolutional layers. This design mitigates the degradation problem encountered when training very deep networks.","recorded":"2024-10-29 11:41:03.493539607","filePath":"null","pinned":false},{"value":"\\begin{equation}\ny = \\mathcal{F}(x, \\{W_i\\}) + x\n\\end{equation}\nwhere \\( y \\) is the output, \\( x \\) is the input, and \\( \\mathcal{F}(x, \\{W_i\\}) \\) represents the residual function, typically a stack of two or more convolutional layers. This design mitigates the degradation problem encountered when training very deep networks.\n","recorded":"2024-10-29 11:40:56.140488640","filePath":"null","pinned":false},{"value":"\\begin{itemize}\n    \\item \\textbf{Stage 1:} Initial convolutional layer with a \\(7 \\times 7\\) kernel size and stride of 2, followed by max pooling.\n    \\item \\textbf{Stage 2:} 3 residual blocks, each with three convolutional layers of sizes \\(1 \\times 1\\), \\(3 \\times 3\\), and \\(1 \\times 1\\).\n    \\item \\textbf{Stage 3:} 4 residual blocks with the same structure.\n    \\item \\textbf{Stage 4:} 6 residual blocks with the same structure.\n    \\item \\textbf{Stage 5:} 3 residual blocks with the same structure, followed by global average pooling and a fully connected layer.\n\\end{itemize}","recorded":"2024-10-29 11:40:50.865903037","filePath":"null","pinned":false},{"value":"In a residual block, the model learns a \\textit{residual function} in relation to the layer inputs, rather than an unreferenced function. The residual function is incorporated into the layer outputs, facilitating the network's ability to learn identity mappings with more ease. The output of a residual block can be mathematically expressed as:","recorded":"2024-10-29 11:40:17.308586644","filePath":"null","pinned":false},{"value":"In a residual block, instead of learning an unreferenced function, the model learns a \\textit{residual function} with reference to the layer inputs. This residual function is added to the output of the layers, allowing the network to learn identity mappings more easily. Mathematically, the output of a residual block can be defined as:","recorded":"2024-10-29 11:40:07.176802554","filePath":"null","pinned":false},{"value":"ResNet50 is a prevalent Convolutional Neural Network (CNN) model introduced by He et al. in 2015. The hallmark of ResNet50 and its ResNet (Residual Network) lineage is the principle of \\textit{residual learning}. This architecture was designed to mitigate the issues of vanishing and expanding gradients in deep networks by integrating \\textit{skip connections} or \\textit{identity mappings} that circumvent one or more levels.\n\nResNet50 is a 50-layer deep architecture that has exhibited outstanding efficacy in image classification tasks. The architecture is organized into blocks, each comprising convolutional layers and skip connections that facilitate the efficient propagation of gradients throughout the network during training. The model consists of five stages, each featuring a sequence of convolutional and identity blocks, culminating in a global average pooling layer followed by a fully connected layer that produces class probabilities.","recorded":"2024-10-29 11:38:35.364579490","filePath":"null","pinned":false},{"value":"esNet50 is a widely used Convolutional Neural Network (CNN) model introduced by He et al. in 2015. The defining feature of ResNet50 and its ResNet (Residual Network) family is the concept of \\textit{residual learning}. This architecture was developed to address the problem of vanishing and exploding gradients in deep networks by incorporating \\textit{skip connections} or \\textit{identity mappings} that bypass one or more layers.\n\nResNet50 is a 50-layer deep model that has demonstrated exceptional performance in image classification tasks. It is structured in blocks where each block consists of convolutional layers along with skip connections that allow gradients to flow more easily through the network during training. The model is composed of five stages, each with a series of convolutional and identity blocks, ending with a global average pooling and a fully connected layer that outputs class probabilities.\n","recorded":"2024-10-29 11:37:52.766120416","filePath":"null","pinned":false},{"value":"Example of Average and Max Pooling","recorded":"2024-10-29 11:32:48.624288960","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/Ex of Avg and Max Pool.png","recorded":"2024-10-29 11:32:02.869606206","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/CNN Arch.png","recorded":"2024-10-29 11:30:43.720181316","filePath":"null","pinned":false},{"value":"Common CNN architecture\n(Sun et al. 2019)","recorded":"2024-10-29 11:29:59.470950994","filePath":"null","pinned":false},{"value":": Example of Average and Max Pooling\n(B. Wang et al. 2021)","recorded":"2024-10-29 11:29:21.530134038","filePath":"null","pinned":false},{"value":"\\section{Convolutional Neural Networks (CNNs)}\n\nConvolutional Neural Networks (CNNs), a type of deep learning model, are extensively employed for image classification tasks, including object recognition. They utilize matrix multiplication and linear algebra to identify patterns in photos. These models frequently necessitate GPUs for effective training due to their processing requirements.\n\n\\subsection{Layers in Convolutional Neural Networks}\n\n\\begin{itemize}\n    \\item \\textbf{Convolutional Layer:} This layer constitutes the foundation of CNNs and executes feature extraction by applying filters (kernels) across receptive fields in the input image. The filter, typically a 3x3 matrix, traverses the image, calculating the dot product between the filter and local pixel values, resulting in a feature map. The result of this recurrent filtering procedure is a structured feature map that emphasizes significant attributes in the image.\n    \n    \\item \\textbf{Pooling Layer:} Commonly referred to as down-sampling, pooling decreases the dimensionality of feature maps by consolidating data inside a filter zone. Prevalent pooling methodologies encompass max pooling and average pooling. This layer diminishes model complexity, mitigates overfitting, and enhances computing performance.\n    \n    \\item \\textbf{Fully Connected (FC) Layer:} In this layer, each neuron is interconnected with every neuron in the preceding layer, executing a linear transformation on the input. Fully connected layers frequently employ a softmax activation function to generate probabilities ranging from 0 to 1, facilitating classification tasks. ReLU functions are commonly utilized in convolutional and pooling layers to incorporate non-linearity.\n\\end{itemize}\n","recorded":"2024-10-29 11:26:58.004979157","filePath":"null","pinned":false},{"value":"Convolutional Neural Networks (CNNs), a type of deep learning model, are extensively employed for image classification tasks, including object recognition. They utilize matrix multiplication and linear algebra to identify patterns in photos. These models frequently necessitate GPUs for effective training because to their processing requirements.\n\nLayers in Convolutional Neural Networks:\n\n    Convolutional Layer: This layer constitutes the foundation of CNNs and executes feature extraction by applying filters (kernels) across receptive fields in the input image. The filter, typically a 3x3 matrix, traverses the image, calculating the dot product between the filter and local pixel values, resulting in a feature map. The result of this recurrent filtering procedure is a structured feature map that emphasizes significant attributes in the image.\n\n    Pooling Layer: Commonly referred to as down-sampling, pooling decreases the dimensionality of feature maps by consolidating data inside a filter zone. Prevalent pooling methodologies encompass max pooling and average pooling. This layer diminishes model complexity, mitigates overfitting, and enhances computing performance.\n\n    Fully Connected (FC) Layer: In this layer, each neuron is interconnected with every neuron in the preceding layer, executing a linear transformation on the input. Fully connected layers frequently employ a softmax activation function to generate probabilities ranging from 0 to 1, facilitating classification tasks. ReLU functions are commonly utilized in convolutional and pooling layers to incorporate non-linearity.","recorded":"2024-10-29 11:26:31.660343082","filePath":"null","pinned":false},{"value":"\\begin{algorithm}[H]\n\\caption{Convolutional Neural Network for Image Classification}\n\\label{algo:CNN}\n\\begin{algorithmic}[1]\n\\Require Input image $I$ of dimension $H \\times W \\times D$\n\\Require Trained CNN model $M$ with $L$ layers: $\\{L_1, L_2, \\dots, L_L\\}$\n\\Ensure Classification label $\\hat{y}$\n\n\\Procedure{CNN\\_Classification}{$I, M$}\n    \\State Initialize feature map $F = I$\n    \n    \\For{each layer $l = 1$ to $L$}\n        \\If{$L_l$ is a \\textbf{Convolutional Layer}}\n            \\State Apply filter $K_l$ of size $k \\times k$ with stride $s$ and padding $p$ on $F$\n            \\State Compute feature map $F_l$ using $F_l = ReLU(F \\ast K_l + b_l)$\n        \n        \\ElsIf{$L_l$ is a \\textbf{Pooling Layer}}\n            \\State Apply pooling function $P_l$ (e.g., max or average pooling)\n            \\State Compute pooled feature map $F_l = P_l(F)$\n        \n        \\ElsIf{$L_l$ is a \\textbf{Fully Connected (FC) Layer}}\n            \\State Flatten $F$ into a vector $v$\n            \\State Compute output vector $v = Softmax(W_l v + b_l)$\n        \\EndIf\n        \\State Update feature map $F = F_l$\n    \\EndFor\n\n    \\State \\Return $\\hat{y} = \\arg \\max(v)$ \\Comment{Predicted class label}\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n","recorded":"2024-10-29 11:26:15.044242520","filePath":"null","pinned":false},{"value":"These deep learning models, primarily Convolutional Neural Networks (CNNs), are widely used for image classification tasks such as object recognition. They leverage matrix multiplication and linear algebra to detect patterns in images. Due to their computational demand, these models often require GPUs for efficient training.\n\nLayers in CNNs:\n\n    Convolutional Layer: This layer forms the backbone of CNNs and performs feature extraction by applying filters (kernels) across receptive fields in the input image. The filter, often a 3x3 matrix, slides over the image, computing the dot product between the filter and local pixel values, producing a feature map. The output of this iterative filtering process is a structured feature map that highlights important characteristics in the image.\n\n    Pooling Layer: Also known as down-sampling, pooling reduces the dimensionality of feature maps by aggregating values within a filter region. Common pooling techniques include max pooling and average pooling. This layer reduces model complexity, helps prevent overfitting, and improves computational efficiency.\n\n    Fully Connected (FC) Layer: In this layer, every neuron connects to every neuron in the previous layer, performing a linear transformation on the input. FC layers often use a softmax activation function to output probabilities between 0 and 1, aiding in classification tasks. ReLU functions are typically applied in convolutional and pooling layers to introduce non-linearity.","recorded":"2024-10-29 11:25:35.169326191","filePath":"null","pinned":false},{"value":"https://quillbot.com","recorded":"2024-10-29 11:25:24.110260601","filePath":"null","pinned":false},{"value":"Fully-Connected Layer :- A neural network with fully connected layers is one in which\neach neuron uses a weights matrix to apply a linear transformation to the input vector. As\na result, every input of the input vector influences every output of the output vector, and all\nlayer-to-layer relationships are present. FC layers commonly employ a function known\nas softmax activation, which produces odds between 0 and 1, to correctly identify inputs.\nIn layers of convolution and pooling, ReLu functions are commonly employed.","recorded":"2024-10-29 11:24:11.607675902","filePath":"null","pinned":false},{"value":"An image is generally represented as a two-dimensional (2-D) structure where the\nweight acts as an feature detector. The filter size is typically a matrix with a dimension\nof 3x3, but they can be other sizes. The filter size also determines the size of the field\nof contact. The calculation of the dot product between the filter and the entered pixels is\nevaluated after the filter is applied to an image‚Äôs specific area. Once this is done, the dot\nproduct evaluated is provided to output array. The filter moves forward by a distance and\nrepeats the procedure when the kernel‚Äôs path has gone through the whole image. Char-\nacteristic maps, activating maps, or organised features are the end outputs of an iterative\nprocess of producing dots by utilising the information being provided and a filter.\nPooling Layer :- By performing dimensionality reduction, the pooling layer, some-\ntimes referred to as down sampling, lowers the total number of input variables. With the\nexception of the fact that filtration doesn‚Äôt contain weights, pooling distributes the filter\nthroughout the whole intake. To fill the resultant collection, the kernel applies a tech-\nnique known as aggregation to the data in the field that is receptive. Despite the fact that\n32\nthe pooling layer significantly reduces information, it also benefits the CNN in numerous\nways. They facilitate efficiency gains and complexity reduction, and lessen the chance of\nover fitting. Figure 3.4 Represent Example of Average and Max Pooling.","recorded":"2024-10-29 11:24:03.961639488","filePath":"null","pinned":false},{"value":"There are three types of layers:\nConvolutional Layer:- It makes up the spine of a CNN, is also a place where the maximum\nwork takes place. It needs data to be entered, filtering, and feature mapping, among\nother things. In addition, a characteristic detector, also known as a kernel or filter, moves\n31\nthrough the receptive fields of the image to assess if the feature is there. Fig 3.3 Show\nTypical architecture of multi layer perceptron","recorded":"2024-10-29 11:23:57.997925022","filePath":"null","pinned":false},{"value":"These are deep learning models that are used for classification of images. They uses a ac-\ncessible way for classifying the image and performing tasks like object recognition with\nthe aid of matrix multiplication and other linear algebraic concepts to identify patterns in\npictures. However, modelling them may need the use of graphics processing units (GPUs)\nbecause they can be computationally demanding.","recorded":"2024-10-29 11:23:46.663683288","filePath":"null","pinned":false},{"value":"#FDA923","recorded":"2024-10-29 11:20:45.921787117","filePath":"null","pinned":false},{"value":"#F05E15","recorded":"2024-10-29 11:16:22.616209558","filePath":"null","pinned":false},{"value":"#CD471D","recorded":"2024-10-29 11:15:28.782946566","filePath":"null","pinned":false},{"value":"vscode-json-languageserver","recorded":"2024-10-28 22:04:52.272327155","filePath":"null","pinned":false},{"value":"{\"username\":\"yomeshsharma\",\"key\":\"f0e38e20fa879070df0623fff5793fde\"}\n","recorded":"2024-10-28 22:04:37.202973865","filePath":"null","pinned":false},{"value":"In deep learning and medical image analysis, choosing the correct method is crucial for obtaining precise and efficient outcomes. Diverse methods are designed to address distinct facets of picture classification, segmentation, and feature extraction, facilitating the accurate identification and diagnosis of intricate illnesses like mouth cancer. This section offers a summary of the algorithms utilized in the present study, emphasizing their functions, roles, and contributions to the model's overall performance. These algorithms, along with optimized designs and fine-tuning methods, boost detection capabilities and improve model reliability.","recorded":"2024-10-28 22:02:26.120201318","filePath":"null","pinned":false},{"value":"In the field of deep learning and medical image analysis, selecting the appropriate algorithm is essential for achieving accurate and efficient results. Various algorithms are tailored to handle different aspects of image classification, segmentation, and feature extraction, enabling the precise detection and diagnosis of complex conditions such as oral cancer. This section provides an overview of the algorithms employed in the current study, highlighting their roles, functionalities, and contributions to the overall performance of the model. These algorithms, combined with optimized architectures and fine-tuning techniques, serve to enhance the detection capabilities and improve model reliability.","recorded":"2024-10-28 22:02:01.369016766","filePath":"null","pinned":false},{"value":"\\subsection{Utilized Algorithms}","recorded":"2024-10-28 22:01:30.308851864","filePath":"null","pinned":false},{"value":"The performance of a deep learning model can be assessed using the following parameters:\n\n\\begin{itemize}\n    \\item \\textbf{Accuracy}: The primary performance statistic to comprehend is accuracy, defined as the ratio of correctly predicted observations to the total number of observations.\n\n    \\item \\textbf{Loss}: The term \"loss\" denotes a numerical value that reflects the precision of the model's predictions. It constitutes a form of retribution for an inaccurate prediction. A loss value of 0 indicates a flawless model; any other value signifies imperfection.\n\n    \\item \\textbf{RMSE}: The Root Mean Squared Error (RMSE) is defined as the square root of the average of the squares of all errors. The RMSE is a widely acknowledged and effective error statistic for numerical forecasting. RMSE should solely be employed to compare prediction errors across different models or configurations for a single variable, as it is scale-dependent and not suitable for comparisons between variables.\n\n    \\item \\textbf{Precision}: The aggregate of positive impacts, even those not immediately apparent, is divided by the entire number of favorable outcomes. A synonym for a high anticipated value is accuracy.\n\n    \\item \\textbf{AUC}: The area under the curve (AUC) represents the discrete integral of a curve illustrating the temporal variations in human expressions.\n\\end{itemize}\n","recorded":"2024-10-28 21:57:19.073151431","filePath":"null","pinned":false},{"value":"The performance of a deep learning model can be assessed using the following parameters:\n‚Ä¢ Accuracy: The primary performance statistic to comprehend is accuracy, defined as the ratio of correctly predicted observations to the total number of observations.\n‚Ä¢ Loss: The term \"loss\" denotes a numerical value that reflects the precision of the model's predictions. It constitutes a form of retribution for an inaccurate prediction. A loss value of 0 indicates a flawless model; any other value signifies imperfection.\n‚Ä¢ RMSE: The Root Mean Squared Error (RMSE) is defined as the square root of the average of the squares of all errors. The RMSE is a widely acknowledged and effective error statistic for numerical forecasting.RMSE should solely be employed to compare prediction errors across different models or configurations for a single variable, as it is scale-dependent and not suitable for comparisons between variables.\n‚Ä¢ Precision: The aggregate of positive impacts, even those not immediately apparent, is divided by the entire number of favorable outcomes. A synonym for a high anticipated value is accuracy.\n‚Ä¢ AUC: The area under the curve (AUC) represents the discrete integral of a curve illustrating the temporal variations in human expressions.","recorded":"2024-10-28 21:56:39.785897316","filePath":"null","pinned":false},{"value":"Performance Measures","recorded":"2024-10-28 21:56:30.931721170","filePath":"null","pinned":false},{"value":"\\subsection{Model Evaluation Techniques} \\label{sec-03.04}","recorded":"2024-10-28 21:56:23.870586425","filePath":"null","pinned":false},{"value":"Numerous measurement model approaches are available, and the computerized computing workbench provides four options. The options comprise:\n\n\\begin{itemize}\n    \\item \\textbf{Percentage split} - To assess a system, randomly partition the dataset into testing and training subsets following each iteration round. Analogous to utilizing a particular sample set, this approach may provide an immediate assessment of productivity but is only recommended when adequate data is available.\n\n    \\item \\textbf{Cross-validation} - Partition the data into $K$-fold segments for cross-validation. Create $K$ unique models and allocate each fold an opportunity. In all divisions except for the one designated as the test set, the training model was concurrently utilized with the test set. Subsequently, the average efficiency of each $K$-fold model is computed. This serves as the standard for evaluating model performance; nevertheless, it requires the development of multiple additional versions. All connections entering and exiting the neuron are randomly eliminated utilizing many input or hidden layers. It undergoes several filters that alter the pixel values in the image.\n\n    \\item \\textbf{Supplied dataset} - This program facilitated the physical separation of the data. Construct the desired model utilizing the complete training dataset, subsequently assess its performance with a distinct test set. Multiple metrics, including sensitivity, specificity, validation accuracy, loss, precision, recall, and F1 score, are employed to evaluate the accuracy of categorization.\n\\end{itemize}\n","recorded":"2024-10-28 21:56:16.960460046","filePath":"null","pinned":false},{"value":"Perfoemance of Deep learning model can be measured by the following paramets :-\n‚Ä¢ Accuracy :- The most fundamental performance metric to understand is accuracy,\nwhich is just the proportion of properly predicted observations to all observations.\n‚Ä¢ Loss:- The term \"loss\" refers to a numerical figure indicating the accuracy of the\nmodel‚Äôs predictions. It‚Äôs a type of payback for a bad forecast. If the loss value is 0,\nthe model is perfect; otherwise, it is not.\n‚Ä¢ RMSE :- The Root Mean Squared Error (RMSE) is defined as the square root of the\nmean of the square of all errors. A popular error metric for numerical forecasts, the\nRMSE is highly recognised as a superior all-purpose error metric.‚Ä¢RMSE should\nonly be used to compare prediction errors of various models or model configura-\ntions for a single variable, not between variables, as it is scale-dependent.\n‚Ä¢ Precision: The total number of positive impacts, even those that aren‚Äôt immediately\nvisible, is divided by the total number of favourable outcomes. Another word for a\nhigh predicted value is accuracy.\n‚Ä¢ AUC :- The area under the curve, or AUC, is the discrete integral of a curve that\ndepicts how a human being‚Äôs expressions change with time.","recorded":"2024-10-28 21:56:02.458904547","filePath":"null","pinned":false},{"value":"Numerous measurement model approaches are available, and the computerized computing workbench provides four options. The options comprise\n‚Ä¢ Percentage split - To assess a system, randomly partition the dataset into testing and training subsets following each iteration round. Analogous to utilizing a particular sample set, this approach may provide an immediate assessment of productivity but is only recommended when adequate data is available.\n‚Ä¢ Cross-validation - partition the data into K-fold segments for cross-validation. Create K unique models and allocate each fold an opportunity. In all divisions except for the one designated as the test set, the training model was concurrently utilized with the test set. Subsequently, the average efficiency of each Kp model is computed. This serves as the standard for evaluating model performance; nevertheless, it requires the development of multiple additional versions.\nAll connections entering and exiting the neuron are randomly eliminated utilizing many input or concealed layers. It undergoes several filters that alter the pixel values in the image.\n‚Ä¢ Supplied dataset‚ÄîThis program facilitated the physical separation of the data. Construct the desired model utilizing the complete training dataset, subsequently assess its performance with a distinct test set. Multiple metrics, including as sensitivity, specificity, validation accuracy, loss, precision, recall, and F1 score, are employed to evaluate the accuracy of categorization.","recorded":"2024-10-28 21:55:49.590713045","filePath":"null","pinned":false},{"value":"Kp model‚Äôs average efficiency is calculated. This is the benchmark for measuring\nmodel performance, yet it necessitates the construction of numerous other variants.\nAll the connections coming in and going out of the neuron are randomly removed\nusing a large number of input layers or hidden layers. It goes through a number of\nfilters and distorts the pixel values in the image.\n‚Ä¢ Provided dataset‚ÄîUsing this programme, the data were separated physically. Cre-\nate the proposed model using the entire training dataset, then evaluate its perfor-\nmance using a different test set. Various measures, including sensitivity, specificity,\nvalidation accuracy, loss, precision, recall, and F1 score, are used to assess the ac-\ncuracy of classification.","recorded":"2024-10-28 21:55:38.972509533","filePath":"null","pinned":false},{"value":"There are many different measurement model methodologies from which to choose, and\nthe computerised computing work bench offers four of them. The choices include\n‚Ä¢ Percentage split- To evaluate a system, randomly divide the dataset into testing and\ntraining partitions after each round of iteration. Similar to employing a specific\nsample set, this may offer an instantaneous estimate of productivity but is only\nadvised when there is a lot of data.\n‚Ä¢ Cross-validation- separate the data in K-fold units for cross-validation. Make K\ndistinct models, then give each fold a chance. On all divisions save the one that is\nused as the test set, the training model was jointly remained the test set. Then, each","recorded":"2024-10-28 21:55:31.479497121","filePath":"null","pinned":false},{"value":"Data augmentation refers to the technique of enlarging the dataset utilized for testing and training. Data augmentation can be achieved through rotation, flipping, translation, shifting, and various other operations \\cite{Bhandari2020}. CNN necessitates a substantial volume of real-world data, which is notably difficult to obtain. Images generated via data augmentation include an invariance quality, indicating that CNN perceives these images as originating from a distinct environment when they exhibit diverse shapes, contours, sizes, and rotations. The target dataset has a limited number of photos, which are also unbalanced; specifically, the quantity of cancer images is double that of non-cancer images. Data augmentation is a prevalent method for addressing the class imbalance issue. Techniques such as rotation, bidirectional flipping, shifting, altering saturation and brightness, zooming, and others have been employed. By combining various augmentation strategies, multiple types of testing and training cohorts are established. For data augmentation, we also utilize functions such as adaptive gamma correction and Gaussian noise.\n\n\\begin{itemize}\n    \\item \\textbf{Gaussian blur:} It is a lens effect that blurs photographs utilizing the Gaussian function, which determines the statistically normal distribution to determine the modifications done to each pixel in the image. It is a common function in graphic design software, mostly employed to diminish visual noise and detail. The Gaussian blurring effect produces a smooth blur that creates the illusion of viewing through a clear screen.\n\n    \\item \\textbf{Train-test Split:} This can be utilized with any supervised learning methodology, as well as issues pertaining to regression or classification. A dataset is divided into two subsets as part of the procedure. A training dataset was constructed utilizing 80\\% of the data, and a testing dataset was formed from the remaining 20\\% of the data.\n\\end{itemize}\n","recorded":"2024-10-28 21:53:57.502704342","filePath":"null","pinned":false},{"value":"Data augmentation refers to the technique of enlarging the dataset utilized for testing and training. Data augmentation can be achieved through rotation, flipping, translation, shifting, and various other operations (Bhandari et al. 2020). CNN necessitates a substantial volume of real-world data, which is notably difficult to get. Images generated via data augmentation include an invariance quality, indicating that CNN perceives these images as originating from a distinct environment when they exhibit diverse shapes, contours, sizes, and rotations. The target dataset has a limited number of photos, which are also unbalanced; specifically, the quantity of cancer images is double that of non-cancer images. Data augmentation is a prevalent method for addressing the class imbalance issue. Techniques like as rotation, bidirectional flipping, shifting, altering saturation and brightness, zooming, and others have been employed. By combining various augmentation strategies, multiple types of testing and training cohorts are established. For data augmentation, we also utilize functions such as adaptive gamma correction and Gaussian noise.\n‚Ä¢ Gaussian blur: It is a lens effect that blurs photographs utilizing the Gaussian function, which determines the statistically normal distribution to determine the modifications done to each pixel in the image. It is a common function in graphic design software, mostly employed to diminish visual noise and detail. The Gaussian blurring effect produces a smooth blur that creates the illusion of viewing through a clear screen.\n‚Ä¢ Train-test Split: - This can be utilized with any supervised learning methodology, as well as issues pertaining to regression or classification. A dataset is divided into two subsets as part of the procedure. A training dataset was constructed utilizing 80% of the data, and a testing dataset was formed from the remaining 20% of the data.","recorded":"2024-10-28 21:31:50.953125775","filePath":"null","pinned":false},{"value":"images as coming from a different environment if they have various shapes, contours,\nsizes and rotations. Our target dataset has fewer images and those images are also un-\nbalanced, the number of cancer images are twice as many than the number of non-cancer\nimages Data augmentation is one of the most popular ways to solve the class imbalance\nproblem. Rotation, flipping in both directions, shifting, adjusting saturation and bright-\nness, zooming, and other techniques have all been used. By mixing different augmenting\ntactics, several kinds of testing and training groups are created. For data augmentation,\nwe additionally employ various functions like adaptive gamma correction and Gaussian\nnoise.\n‚Ä¢ Gaussian blur : - It is a type of lens that blur images and uses the Gaussian feature,\nand defines the statically normal distribution, to decide the change to be applied to\neach pixel in the image. It is a frequent feature in graphics software and is typically\nused to reduce visual noise and detail. Using Gaussian blurring effect, you get a\nsmooth blur that gives the impression like you are gazing through a transparent\nscreen.\n‚Ä¢ Train-test Split : - This is able to be used with any controlled learning approach,\nin addition to problems involving regression or classification. A set of data is split\ninto two parts subsets as part of the process. A training dataset was created using\n80% of the data, while a testing dataset was created using 20% of the data.","recorded":"2024-10-28 21:31:35.966904428","filePath":"null","pinned":false},{"value":"The art of expanding the dataset used for testing and training purposes is known as data\naugmentation. You can increase the data by rotating, flipping, translating, shifting, and\nother operations (Bhandari et al. 2020. CNN requires a significant amount of data in the\nactual world, which is quite challenging to gather. Images created through data augmen-\ntation are considered to have an in-variance property, which means CNN interprets these","recorded":"2024-10-28 21:31:28.724149103","filePath":"null","pinned":false},{"value":"The pre-processing of photos involves modifying them to meet the model's specifications. It entails the enhancement and resolution modification of photographs, which can be vital for model development.\n\\begin{itemize}\n    \\item \\textbf{Data loading and cleaning:} The initial phase involves importing the requisite libraries for the program. Initially, numerous images of oral cancer and non-cancerous conditions were pre-processed utilizing the OpenCV tools. Data cleaning, subsequent to data loading, entails rectifying or removing erroneous, corrupted, misconfigured, redundant, and deficient information from a dataset. No singular strategy is universally applicable to all datasets for delineating the particular steps of the data cleaning process, as varying datasets necessitate unique approaches. Creating a template is essential to ensure the data cleansing process is executed precisely on each occasion.\n\n    \\item \\textbf{Exploratory Data Analysis:} After preprocessing the images from both datasets, the data was aggregated and visually represented using a histogram. In RGB histograms, R denotes red, G signifies green, and B represents blue. The histogram view illustrates the intensity distribution of photographs and quantifies the number of pixels needed to represent each intensity value. Likewise, histogram equalization enhances the intensity range for comparing different intensity levels, resulting in a uniform distribution of intensity values. In other words, it distributes the most prevalent intensity levels. The image's contrast is augmented.\n\n    \\item \\textbf{Feature extraction:} Feature extraction is utilized to mitigate multicollinearity and reduce high dimensionality to low dimensionality while removing correlated features. Each layer in deep learning acquires specific details and integrates them with existing knowledge to extract relevant information. In deep learning, correlating a picture with a filter or kernel obviates the necessity for discrete feature extraction.\n\n    Feature extraction entails determining the most relevant features from a dataset for the specific predictive modeling task at hand. It is a methodical approach to improving classification accuracy by identifying the optimal features from a restricted set of unique data. It eliminates the superfluous functions that are irrelevant to the model.\n\\end{itemize}\n","recorded":"2024-10-28 21:29:20.848626938","filePath":"null","pinned":false},{"value":"The pre-processing of photos involves modifying them to meet the model's specifications. It entails the enhancement and resolution modification of photographs, which can be vital for model development.\n‚Ä¢ Data loading and cleaning: The initial phase involves importing the requisite libraries for the program. Initially, numerous images of oral cancer and non-cancerous conditions were pre-processed utilizing the OpenCV tools. Data cleaning, subsequent to data loading, entails rectifying or removing erroneous, corrupted, misconfigured, redundant, and deficient information from a dataset. No singular strategy is universally applicable to all datasets for delineating the particular steps of the data cleaning process, as varying datasets necessitate unique approaches. Creating a template is essential to ensure the data cleansing process is executed precisely on each occasion.\n‚Ä¢ Exploratory Data Analysis: After preprocessing the images from both datasets, the data was aggregated and visually represented using a histogram. In RGB histograms, R denotes red, G signifies green, and B represents blue. The histogram view illustrates the intensity distribution of photographs and quantifies the number of pixels needed to represent each intensity value. Likewise, histogram equalization enhances the intensity range for comparing different intensity levels, resulting in a uniform distribution of intensity values. In other words, it distributes the most prevalent intensity levels. The image's contrast is augmented.\n‚Ä¢ Feature extraction: - Feature extraction is utilized to mitigate multicollinearity and reduce high dimensionality to low dimensionality while removing correlated features. Each layer in deep learning acquires specific details and integrates them with existing knowledge to extract relevant information.In deep learning, correlating a picture with a filter or kernel obviates the necessity for discrete feature extraction.\nFeature extraction entails determining the most relevant features from a dataset for the specific predictive modeling task at hand. It is a methodical approach to improving classification accuracy by identifying the optimal features from a restricted set of unique data. It eliminates the superfluous functions that are irrelevant to the model.","recorded":"2024-10-28 21:28:48.112760989","filePath":"null","pinned":false},{"value":"\n","recorded":"2024-10-28 21:27:15.150588447","filePath":"null","pinned":false},{"value":"windowrulev2 = float, title:^(YouTube Music ‚Äî Mozilla Firefox)$\n","recorded":"2024-10-28 21:27:14.663248283","filePath":"null","pinned":false},{"value":"chagantivenkataramireddy1@gmail.com","recorded":"2024-10-28 21:26:08.492031998","filePath":"null","pinned":false},{"value":"YouTube Music ‚Äî Mozilla Firefox","recorded":"2024-10-28 21:17:18.613649454","filePath":"null","pinned":false},{"value":"windowrulev2 = float, title:^(Picture in picture)$\n","recorded":"2024-10-28 21:16:59.559055153","filePath":"null","pinned":false},{"value":"‚Ä¢","recorded":"2024-10-28 21:14:43.157389852","filePath":"null","pinned":false},{"value":"Pre-processing of the images include steps to change the image according to the model\nrequirement. It involves cleaning and changing the resolution of photograph, it enhances\nthe picture property which can prove crucial in model development.\n‚Ä¢ Data loading and cleaning: -Importing the program‚Äôs necessary libraries is the ini-\ntial step. Initially, several oral cancer and non-cancer photos were pre-processed\nusing the Opencv packages. Following data loading, data cleaning involves fixing or eliminating inaccurate, damaged, improperly arranged, duplicate, and lacking\ninformation from a set of data. There is no one method that works for all datasets\nto define the specific phases of the data cleaning process because different datasets\nwill require distinct approaches. It is crucial to make a template in order to guaran-\ntee that the data cleansing procedure gets done accurately each time.\n‚Ä¢ Exploratory Data Analysis :- Following the pre-processing of the photos from both\ndatasets, the data was tallied and graphically shown using the histogram. In RGB\nhistograms, R is referred to as red, G is referred to as green, and B is referred\nto as blue. The histogram view displays the intensity distribution of images and\nquantifies how many pixels were required to represent each intensity value. Similar\nto this, histogram equalisation increases the level of intensity range for comparing\none range of intensity to another, producing an equal distribution among intensity\nvalues. In other words, it spreads out the most typical intensity levels. As a result,\nthe contrast of the image is enhanced.\n‚Ä¢ Feature extraction: - Feature extraction is applied to minimises multicollinearity\nand turns high dimensionality into low dimensionality while eliminating corelated\nfeatures. Every layer in deep learning picks up certain details and combines them\nwith prior knowledge to gather information that is valuable.In deep learning, match-\ning an image to a filter or kernel replaces the need for individual feature extraction.\nThe process of feature extraction involves identifying the features from a set of data\nthat are most pertinent to the current predictive modelling task. It is a calculated\nstrategy for enhancing classification accuracy by determining the best features to\nuse from a limited pool of distinctive data. It removes the redundant functions that\nare of no use of the model.","recorded":"2024-10-28 21:14:31.822075563","filePath":"null","pinned":false},{"value":"or eliminating inaccurate, damaged, improperly arranged, duplicate, and lacking\ninformation from a set of data. There is no one method that works for all datasets\nto define the specific phases of the data cleaning process because different datasets\nwill require distinct approaches. It is crucial to make a template in order to guaran-\ntee that the data cleansing procedure gets done accurately each time.\n‚Ä¢ Exploratory Data Analysis :- Following the pre-processing of the photos from both\ndatasets, the data was tallied and graphically shown using the histogram. In RGB\nhistograms, R is referred to as red, G is referred to as green, and B is referred\nto as blue. The histogram view displays the intensity distribution of images and\nquantifies how many pixels were required to represent each intensity value. Similar\nto this, histogram equalisation increases the level of intensity range for comparing\none range of intensity to another, producing an equal distribution among intensity\nvalues. In other words, it spreads out the most typical intensity levels. As a result,\nthe contrast of the image is enhanced.\n‚Ä¢ Feature extraction: - Feature extraction is applied to minimises multicollinearity\nand turns high dimensionality into low dimensionality while eliminating corelated\nfeatures. Every layer in deep learning picks up certain details and combines them\nwith prior knowledge to gather information that is valuable.In deep learning, match-\ning an image to a filter or kernel replaces the need for individual feature extraction.\nThe process of feature extraction involves identifying the features from a set of data\nthat are most pertinent to the current predictive modelling task. It is a calculated\nstrategy for enhancing classification accuracy by determining the best features to\nuse from a limited pool of distinctive data. It removes the redundant functions that\nare of no use of the model.","recorded":"2024-10-28 21:14:19.225501427","filePath":"null","pinned":false},{"value":"Pre-processing of the images include steps to change the image according to the model\nrequirement. It involves cleaning and changing the resolution of photograph, it enhances\nthe picture property which can prove crucial in model development.\n‚Ä¢ Data loading and cleaning: -Importing the program‚Äôs necessary libraries is the ini-\ntial step. Initially, several oral cancer and non-cancer photos were pre-processed\nusing the Opencv packages. Following data loading, data cleaning involves fixing","recorded":"2024-10-28 21:14:08.131485696","filePath":"null","pinned":false},{"value":"The ‚Äúoral and tongue Cancer‚Äù Images were taken from Kaggle, it contained 87 oral cancer\nimages and 44 normal images. These images were correctly label by reputed doctors.\nThese were the clinical images and taken at real time.","recorded":"2024-10-28 21:13:28.531203580","filePath":"null","pinned":false},{"value":"Global age standardized prevalence of tobacco smoking source World Health\nOrganization","recorded":"2024-10-28 21:04:01.247085821","filePath":"null","pinned":false},{"value":"\\begin{figure}[H]\n    \\begin{center}\n    %\\small\n    \\scriptsize\n    \\includegraphics[width=0.8\\textwidth, height=0.4\\textwidth]{images/Global WHO.png}\n    \\caption{Oral Cancer}\n    \\end{center}\n\\end{figure}","recorded":"2024-10-28 21:01:10.154337301","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/proposed method.png","recorded":"2024-10-28 21:00:43.565391870","filePath":"null","pinned":false},{"value":"\\begin{figure}[H]\n    \\centering\n    \\begin{tikzpicture}[\n        font=\\small,\n        node distance=2cm,\n        every node/.style={draw, align=center, minimum width=3cm, minimum height=1cm}\n    ]\n        % Nodes\n        \\node[cylinder, shape border rotate=90, draw, minimum height=1.5cm, cylinder uses custom fill, cylinder body fill=white, cylinder end fill=gray!20] (images) {Oral Cancer\\\\ Images};\n        \\node (processing) [right=1.5cm of images] {Processing of\\\\ Images};\n        \\node (analysis) [right=1.5cm of processing] {Data Analysis};\n        \\node (extraction) [right=1.5cm of analysis] {Feature\\\\ Extraction};\n        \n        \\node (model) [below=2cm of extraction] {Deep Learning\\\\ Model};\n        \\node (optimizer) [left=1.5cm of model] {Metaheuristic\\\\ Optimizer};\n        \\node (classification) [left=1.5cm of optimizer] {Classification};\n        \n        \\node (cancer) [below left=1cm and 0.5cm of classification] {Cancer};\n        \\node (noncancer) [below right=1cm and 0.5cm of classification] {Non Cancer};\n        \n        % Arrows\n        \\draw[-\u003e] (images) -- (processing);\n        \\draw[-\u003e] (processing) -- (analysis);\n        \\draw[-\u003e] (analysis) -- (extraction);\n        \\draw[-\u003e] (extraction) -- (model);\n        \\draw[-\u003e] (model) -- (optimizer);\n        \\draw[-\u003e] (optimizer) -- (classification);\n        \\draw[-\u003e, bend right=20] (classification) to (cancer);\n        \\draw[-\u003e, bend left=20] (classification) to (noncancer);\n        \n    \\end{tikzpicture}\n    \\caption{Proposed Methodology}\n    \\label{fig:proposed_methodology}\n\\end{figure}\n","recorded":"2024-10-28 20:55:22.229557882","filePath":"null","pinned":false},{"value":"\\begin{figure}[H]\n    \\centering\n    \\begin{tikzpicture}[\n        font=\\small,\n        node distance=2cm,\n        every node/.style={draw, align=center, minimum width=3cm, minimum height=1cm}\n    ]\n        % Nodes\n        \\node (images) {Oral Cancer\\\\ Images};\n        \\node (processing) [right=1.5cm of images] {Processing of\\\\ Images};\n        \\node (analysis) [right=1.5cm of processing] {Data Analysis};\n        \\node (extraction) [right=1.5cm of analysis] {Feature\\\\ Extraction};\n        \n        \\node (model) [below=2cm of extraction] {Deep Learning\\\\ Model};\n        \\node (optimizer) [left=1.5cm of model] {Metaheuristic\\\\ Optimizer};\n        \\node (classification) [left=1.5cm of optimizer] {Classification};\n        \n        \\node (cancer) [below left=1cm and 0.5cm of classification] {Cancer};\n        \\node (noncancer) [below right=1cm and 0.5cm of classification] {Non Cancer};\n        \n        % Arrows\n        \\draw[-\u003e] (images) -- (processing);\n        \\draw[-\u003e] (processing) -- (analysis);\n        \\draw[-\u003e] (analysis) -- (extraction);\n        \\draw[-\u003e] (extraction) -- (model);\n        \\draw[-\u003e] (model) -- (optimizer);\n        \\draw[-\u003e] (optimizer) -- (classification);\n        \\draw[-\u003e] (classification) -- (cancer);\n        \\draw[-\u003e] (classification) -- (noncancer);\n        \n    \\end{tikzpicture}\n    \\caption{Proposed Methodology}\n    \\label{fig:proposed_methodology}\n\\end{figure}\n","recorded":"2024-10-28 20:53:39.486241144","filePath":"null","pinned":false},{"value":"The main objective of our research is to evaluate the precision, recall, and accuracy of medical image analysis. We utilize images of oral cancer for this purpose. A MobileNet model has been utilized for classification, and the Manta Ray Foraging Algorithm has been selected as our metaheuristic optimizer due to its superior performance relative to other state-of-the-art genetic optimizers. Initially, we analyzed the Oral Lip and Tongue cancer dataset and determined that it is imbalanced, with a significant skew towards cancer images, as the quantity of cancer photos exceeds twice that of non-cancerous images.\nWe have rectified the imbalance in the photos and processed the data to meet our specifications, subsequently supplying this data to the deep learning model, which derives its weights from a metaheuristic optimizer. The framework is depicted in Figure 3.1.","recorded":"2024-10-28 20:52:25.168766057","filePath":"null","pinned":false},{"value":"The primary goal of our work is to assess the precision, recall, and accuracy of medical\npicture analysis. To do this, we use photos of oral cancer. A mobilenet model has been\ntaken into account for the purpose of classification and we chose Manta ray foraging\nalgorithm as our meta heuristic optimizer due to its better performance as compared to\nanother state of art genetic optimizer. First, we taken the dataset that is Oral Lip and\nTongue cancer and found that the dataset is imbalanced and skewed toward cancer images\nas the number of cancer images were more than twice the number non-cancerous images.\nWe have removed the unbalance of the images and processed the data according to our\nrequirements then we provided this data to the deep learning model which is taking its\nweight from a metahuristic optimizer. The framework is show in the figure 3.1","recorded":"2024-10-28 20:51:59.423570266","filePath":"null","pinned":false},{"value":"The literature evaluation encompassed several methodologies and comparisons with alternative techniques.This literature thoroughly examines the methodologies and procedures utilized in conducting the study. Furthermore, many pre-trained model types are accompanied by relevant visuals. The final section elucidates the proposed model and its modifications.","recorded":"2024-10-28 20:45:28.734879427","filePath":"null","pinned":false},{"value":"We covered many types of strategies in the literature review, as well as comparisons with\nother techniques.The approaches and techniques that were employed for carrying out the\nstudy are covered in great depth in this literature. Additionally, numerous pre-trained\nmodel types are supplemented with pertinent illustrations. The proposed model and its\nvariations are explained in the final section.","recorded":"2024-10-28 20:45:00.611371311","filePath":"null","pinned":false},{"value":"windowrulev2 = float, title:^(Picture in picture)$\n# windowrulev2 = pin, title:^(Picture in picture)$\n\n","recorded":"2024-10-28 20:33:25.421360301","filePath":"null","pinned":false},{"value":"windowrulev2 = float, title:^(Picture in picture)$","recorded":"2024-10-28 20:32:40.582441913","filePath":"null","pinned":false},{"value":"windowrulev2 = pin, title:^(Picture in picture)$","recorded":"2024-10-28 20:32:08.607801597","filePath":"null","pinned":false},{"value":"https://meet.google.com/zds-hsrb-zxj","recorded":"2024-10-28 20:19:06.629292157","filePath":"null","pinned":false},{"value":"/home/karna/dotfiles/rofi/.config/rofi","recorded":"2024-10-28 17:54:43.734084371","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/clipse\n/home/karna/Downloads/hypr\n/home/karna/Downloads/kitty\n/home/karna/Downloads/waybar\n/home/karna/Downloads/wlogout","recorded":"2024-10-28 17:43:26.279520544","filePath":"null","pinned":false},{"value":"/home/karna/dotfiles/Hyprland/.config/clipse\n/home/karna/dotfiles/Hyprland/.config/hypr\n/home/karna/dotfiles/Hyprland/.config/kitty\n/home/karna/dotfiles/Hyprland/.config/waybar\n/home/karna/dotfiles/Hyprland/.config/wlogout","recorded":"2024-10-28 17:43:21.269372785","filePath":"null","pinned":false},{"value":"Savita Shetty et al. \u0026 2022 \u0026 Resnet34 with modified unet for multicalss segmentation was applied \u0026 The developed method provide accuracy of up to 80\\% and F! score of 0.96 \u0026 A better segmentation is required \\\\ \\hline","recorded":"2024-10-28 16:26:27.615481963","filePath":"null","pinned":false},{"value":"Author\nMesfer Al\nDuhayyimal.,\net\nQirui Huang et\nal.,\nHuan Ding et\nal.\nSavitaet al.\nShetty\nYear\n2023\n2023\n2023\n2022\nTechniques\n Findings\n Limitations\nSO with\n Hybrid model\n The dataset\nFusion based\n provided F1\n used was\nClassification\n score of 94.5\n imbalance\n(CADOC-\nSFOFC)\nmodel\nDL algorithm\n Proposed\n complexity of\nbased on\n method\n algorithm was\nmetaheuristic\n provided an\n increased\napproach\n Precision of\n92.66%\nModified\n Model\n SVM could be\nLocust Swarm\n provided\n replaced by a\noptimization\n 92.37%\n better deep\nalgorithm\n specificity and\n learning model\n96.94%\naccuracy rate\nResnet34 with\n The developed\n A better\nModified Unet\n method\n segmentation\nfor Multiclass\n provide\n is required\nsegmentation\n accuracy of up\nwas applied\n to 80%. and\nF1 score of\n0.96","recorded":"2024-10-28 16:16:21.109461820","filePath":"null","pinned":false},{"value":"Mesfer Al\nDuhayyimal.,\net","recorded":"2024-10-28 16:09:45.460351262","filePath":"null","pinned":false},{"value":"https://www.amazon.in/Ant-Esports-GM320-Programmable-Comfortable/dp/B08D64C9FN/ref=sr_1_3?crid=2YOXVOFMYM32A\u0026dib=eyJ2IjoiMSJ9.P9tM3t-kfvm2ttoxInCwWw5tFmryGmXYs_iHbCJSK651F_hjVVvM2AaUvN89mujyvX0Wbfv4SFNd1nr7kEjJ56uWxXL---UZdcTUt4757Vdfz0vulcD2SE3vXppB1V89XopMGjxZx5otZJYidzeYKBvjy58cRoD_Zxo-wefN6Tff9iVS1SW6a5kzs_h6ZZXyqDivrOPC5-SOLpHRxipTTMtfffQM_u6mKC1Pa6qfhis.jYvwtSPHJx-m2MpuW3cGSoNLDPLedXf0eP_MwXi1Sy8\u0026dib_tag=se\u0026keywords=gm320%2Bmouse%2Bant%2Besports\u0026nsdOptOutParam=true\u0026qid=1730111425\u0026sprefix=GM320%2B%2Caps%2C242\u0026sr=8-3\u0026th=1","recorded":"2024-10-28 16:01:19.860492105","filePath":"null","pinned":false},{"value":"https://www.amazon.in/Redgear-MP35-Control-Type-Gaming-Mousepad/dp/B01J1CFO30/ref=sr_1_3?crid=1HVRWVPHT8SWY\u0026dib=eyJ2IjoiMSJ9.PbgGtSoE-fyIRhkldtZn9pB2BSB9adwVMkrJQ5g2joDjX4kz1XeLUyr5HrfWb1EX-KnYAGQjxJFyV-vHXt56YE_aI2yFo5iIxEmVRK1ogz_zVLLynlmDZ7W_yr69CGqkGha2RCWszRQkgsxcUi8tr_37nBtj3FkmU9yk3nOCdIN1UqXjA5NdPyre57M3idcKwed0zC-NmJObF--5YYz1Ho2tyDTr3YfWffL9rjUSNy8.B8AtF1CS6PU1xt9GWS5SOK_DVJc7lyB76ATl8B85RQ8\u0026dib_tag=se\u0026keywords=mousepad+redgear\u0026nsdOptOutParam=true\u0026qid=1730111157\u0026sprefix=mousepad+redgear%2Caps%2C228\u0026sr=8-3","recorded":"2024-10-28 15:56:32.599575444","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Summary of studies on Ensemble Techniques for oral cancer detection} \\label{Table_03}\n    \\addtolength{\\tabcolsep}{1.5pt}\n    \\renewcommand{\\arraystretch}{1.5}\n    \\begin{tabular}{|p{2.5cm}|p{0.8cm}|p{3.8cm}|p{3.5cm}|p{3.5cm}|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Heba M. Afify et al. \u0026 2023 \u0026 Deep learning models with the use of GRAD-CAM \u0026 EfficientNet-B0 achieved an accuracy of 95\\% \u0026 Class imbalance and time complexity were not taken into consideration \\\\ \\hline\n    Mathis Ersted Rasmussen et al. \u0026 2023 \u0026 A single-cycle interactive segmentation model \u0026 The medians of the dice rose with single-cycle segmentation in the range of 0.004 to 0.009 \u0026 The CT-only model failed to predict 36 organs-at-risk \\\\ \\hline\n    Natheer Al Rawi et al. \u0026 2022 \u0026 AI-based model with a modified cross-entropy loss function \u0026 The accuracy ranged from 43.5\\% to 100\\%, sensitivity from 94\\% to 100\\%, and specificity from 96\\% to 100\\% for the proposed model \u0026 The presence of modal noise reduced the effectiveness of this technique to detect oral cancer \\\\ \\hline\n    Nanditha B R et al. \u0026 2021 \u0026 An ensemble model with benefits of ResNet50 and VGG-16 \u0026 Ensemble model provided 96.2\\% accuracy, 98.14\\% sensitivity, and 94.23\\% specificity \u0026 The computational time of the model was very high \\\\ \\hline\n    \\end{tabular}\n\\end{table}","recorded":"2024-10-28 15:36:23.011849199","filePath":"null","pinned":false},{"value":"Lin et al. (2021) introduced a robust picture diagnostic method for cellphones via a deep learning system. A centered rule image-capturing method was proposed for acquiring images of the mouth cavity. A medium-sized oral dataset comprising five categories of illnesses was created to mitigate the effects of picture variability from hand-held smartphone cameras. A newly created deep learning network (HRNet) was utilized to evaluate the efficacy of our oral cancer detection technology. The proposed technique achieved an F1 score of 83.6%, a sensitivity of 83.0%, a specificity of 96.6%, and an accuracy of 84.3% on 455 test images.The newly introduced HRNet exhibited somewhat superior performance compared to VGG, ResNet, and DenseNet for sensitivity, specificity, accuracy, and F1 score.","recorded":"2024-10-28 15:35:01.639073703","filePath":"null","pinned":false},{"value":"(Lin et al. 2021) presented a powerful deep learning algorithm-based image diagnosis\napproach for smartphones. For gathering images of the oral cavity, a centred rule image-\ncapturing method was suggested. In order to lessen the impact of image variability from\nhand-held smartphone cameras, a medium-sized oral dataset with five types of disorders\nwas constructed. A recently developed deep learning network (HRNet) was used to assess\nhow well our technique for detecting oral cancer performed. On 455 test photos, the\nproposed technique performed with an F1 of 83.6%, a sensitivity of 83.0%, a specificity\nof 96.6%, and an accuracy of 84.3%.In terms of the parameters of sensitivity, specificity,\naccuracy, and F1, the newly presented HRNet performed marginally better than VGG,\nResNet, and DenseNet.","recorded":"2024-10-28 15:34:51.515777400","filePath":"null","pinned":false},{"value":"Song et al. (2021) suggested a deep learning model utilizing a severely unbalanced dataset. Three thousand eight hundred fifty-one images of the buccal mucosa in polarized white light were obtained using a custom-designed oral cancer screening device. To improve the neural network's efficacy in classifying oral cancer images from imbalanced multi-class datasets obtained from high-risk populations during screenings in low-resource environments, the researchers employed weight balancing, data augmentation, undersampling, focal loss, and ensemble techniques. The detection success rate of the minority classes, which were initially challenging to distinguish, improved with the application of data-level and algorithm-level techniques in deep learning training. The experiment's results demonstrated that rectifying class imbalance enhanced the efficacy of the proposed technique, yielding an AUC of up to 0.93.","recorded":"2024-10-28 15:34:46.070323350","filePath":"null","pinned":false},{"value":"were initially difficult to differentiate, was enhanced by using data-level and algorithm-\nlevel methods to deep learning training. The experiment‚Äôs findings proved that eliminating\nclass imbalance helped make the proposed strategy more effective, providing an AUC of\nup to 0.93.","recorded":"2024-10-28 15:34:35.326158474","filePath":"null","pinned":false},{"value":"(Song et al. 2021) proposed a deep learning model on a dataset which was highly\nimbalanced. 3851 pictures of the cheek mucosa in polarised white light were captured\nby a specially made oral cancer screening tool. In order to enhance the neural network‚Äôs\nperformance of oral cancer image classification with the imbalanced multi-class datasets\ncollected from high-risk populations during oral cancer screening in low-resource set-\ntings, the researchers used weight balancing, data augmentation, under sampling, focal\nloss, and ensemble methods. The success rate of detection of the minority classes, which","recorded":"2024-10-28 15:34:29.284852159","filePath":"null","pinned":false},{"value":"Al Duhayyim et al. (2023) present the Sailfish Optimisation with Fusion Based Classification (CADOC-SFOFC) model, a distinctive Computer Aided Diagnosis system for the identification of oral cancer. The detection of oral cancer in clinical photographs is facilitated by the proposed CADOC-SFOFC model. This is achieved by utilizing the VGGNet16 and Residual Network (ResNet) models in a fusion-based feature extraction methodology. The integrated feature vectors were subsequently sent to an extreme learning machine for categorization. The SFO approach was selected for parameter optimization of the ELM machine. The proposed strategy, when applied to a publically available dataset, yielded an average F1 score of 94.51 for the categorization of oral cancer images.","recorded":"2024-10-28 15:34:20.484945182","filePath":"null","pinned":false},{"value":"(Al Duhayyim et al. 2023) proposes the Sailfish Optimisation with Fusion Based\nClassification (CADOC-SFOFC) model, it is a unique Computer Aided Diagnosis for\ndetection of oral cancer. The presence of oral cancer on clinical images is determined by\nthe suggested CADOC-SFOFC model. This is accomplished through the employment of\nthe VGGNet16 and Residual Network (ResNet) model in a fusion-based feature extraction\napproach. The fused feature vectors were then provided to an extreme learning machine\nfor classification. SFO method was chosen for parameter selection of ELM machine. The\nproposed method when applied to a publicly available dataset provided an average F1\nscore of 94.51 for classification of oral cancer images.","recorded":"2024-10-28 15:34:12.669688966","filePath":"null","pinned":false},{"value":"Shetty and Patil (2023) suggested a model for detecting oral cancer in a distributed cloud environment with an optimized ensemble. The proposed model employed Improved Linear Discriminant Analysis (ILDA) for feature extraction, which mitigated overfitting and enhanced accuracy, while also decreasing training time. The proposed model integrated a Multi-layer Perceptron (MLP) with a Support Vector Machine (SVM). The model was trained using 1,224 histological images of oral cancer captured at various magnifications. The dataset was subsequently divided into uniformly sized segments, and the model was employed. The developed approach yields an accuracy of up to 80%.","recorded":"2024-10-28 15:33:55.064836058","filePath":"null","pinned":false},{"value":"(Shetty and Patil 2023) proposed a Model for detecting oral cancer in a distributed\ncloud setting using an optimised ensemble. The suggested model was using Improved\nLinear Discriminant Analysis (ILDA) for feature extraction which helped in reducing the\nover fitting and increased the accuracy, also the suggested feature extraction reduced the\ntraining time. The proposed model was a combination of Multi-layer Perceptron (MLP)\nwith Support Vector Machine (SVM). The model was trained on 1224 histological photos\nof oral cancer taken at different magnifications. The dataset was then split into equally\nsized pieces, and the model was used. The developed method provide accuracy of up to\n80%.","recorded":"2024-10-28 15:33:46.216702961","filePath":"null","pinned":false},{"value":"Ding, Huang, and Rodriguez (2023) suggested a system for oral cancer diagnosis that incorporates Reinforcement Learning for image segmentation, the Gabor wavelet technique for feature extraction, and an RBF-kernel-based SVM for classification. Employing an enhanced metaheuristic called the Modified Locus Swarm Optimization (MLSO) method, the optimal attributes were selected. The classification phase similarly employs this methodology to furnish the SVM with the optimal configuration contingent upon the kernel. The \"Oral Cancer images\" dataset is utilized to assess the efficacy of the proposed method. The simulation results indicate that the proposed method achieved a 96.94% accuracy rate, exhibiting the lowest error ratio in comparison to other analogous methods. The results indicate that the proposed method exhibits 92.37% specificity and 93.89% sensitivity.","recorded":"2024-10-28 15:33:30.141395707","filePath":"null","pinned":false},{"value":"Swarm Optimization (MLSO) algorithm, the best characteristics were chosen. The clas-\nsification stage also uses this approach to provide the SVM with the best configuration\npossible based on the kernel. The \"Oral Cancer images\" dataset is used to validate the ef-\nfectiveness of the suggested technique. According to the simulation findings, the proposed\nmethod, which had a 96.94% accuracy rate, had the lowest error ratio when compared to\nother comparable methods. Additionally, the findings show that the suggested approach\nhas 92.37% specificity and 93.89% sensitivity.","recorded":"2024-10-28 15:33:00.827337320","filePath":"null","pinned":false},{"value":"( Ding, Huang, and Rodriguez 2023) proposed a methodology for oral cancer detec-\ntion which include the Reinforcement Learning technique for picture segmentation, Using\nthe Gabor wavelet technique to extract picture features and an RBF-kernel-based SVM to\nclassify the results. Utilising an improved metaheuristic known as the Modified Locus","recorded":"2024-10-28 15:32:54.969348460","filePath":"null","pinned":false},{"value":"Huang, Ding, and Razmjooy (2023) devised an innovative way utilizing a metaheuristic approach and deep learning to create a dependable cancer diagnostic tool. Three preprocessing procedures were employed to enhance the quality and quantity of the raw images, providing sufficient data for the training of convolutional neural networks. A public dataset concerning oral cancer was utilized for model training. The mouth Cancer pictures dataset comprises 131 images of mouth cancer, captured at several ENT institutions and meticulously classified. ISSA was employed for weight optimization. The optimization is derived from the foraging techniques of flying squirrels. The ISSA algorithm, when integrated with CNN, elevates the algorithm's complexity. The proposed strategy achieved a precision of 92.66% in classifying oral cancer images.","recorded":"2024-10-28 15:31:10.298980753","filePath":"null","pinned":false},{"value":"( Huang, Ding, and Razmjooy 2023) developed a novel strategy based on a metaheuristic\napproach and deep learning to make a reliable cancer diagnosis tool. three preprocessing\nstrategies were implemented to improve the quality and quantity of the raw images to\ngive enough data for convolutional neural network training. A public dataset on oral\ncancer was taken for training the model. The Oral Cancer photographs dataset consists of\n131 oral cancer photographs that were photographed at various ENT hospitals and were\nexpertly categorised. ISSA was used for the optimization of the weights. The optimization\nis based on flying squirrels and there foraging techniques. ISSA algorithm combined with\nCNN increases the complexity of algorithm. Proposed method provided an Precision of\n92.66% while classifying oral cancer images.","recorded":"2024-10-28 15:30:55.843717235","filePath":"null","pinned":false},{"value":"Oral Cancer detection using meta heuristic optimizer","recorded":"2024-10-28 15:30:25.897362993","filePath":"null","pinned":false},{"value":"Summary of studies on deep learning techniques for oral cancer detection","recorded":"2024-10-28 15:14:55.033781638","filePath":"null","pinned":false},{"value":"\\begin{tabular}{|p{2.5cm}|p{0.8cm}|p{3.8cm}|p{3.5cm}|p{3.5cm}|}","recorded":"2024-10-28 15:14:21.281636709","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Recent Studies on Deep Learning Techniques for Oral Cancer Detection (Continuation)} \\label{Table_03}\n    \\addtolength{\\tabcolsep}{1.5pt}\n    \\renewcommand{\\arraystretch}{1.5}\n    \\begin{tabular}{|p{4cm}|p{1cm}|p{4cm}|p{4cm}|p{4cm}|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Heba M. Afify et al. \u0026 2023 \u0026 Deep learning models with the use of GRAD-CAM \u0026 EfficientNet-B0 achieved an accuracy of 95\\% \u0026 Class imbalance and time complexity were not taken into consideration \\\\ \\hline\n    Mathis Ersted Rasmussen et al. \u0026 2023 \u0026 A single-cycle interactive segmentation model \u0026 The medians of the dice rose with single-cycle segmentation in the range of 0.004 to 0.009 \u0026 The CT-only model failed to predict 36 organs-at-risk \\\\ \\hline\n    Natheer Al Rawi et al. \u0026 2022 \u0026 AI-based model with a modified cross-entropy loss function \u0026 The accuracy ranged from 43.5\\% to 100\\%, sensitivity from 94\\% to 100\\%, and specificity from 96\\% to 100\\% for the proposed model \u0026 The presence of modal noise reduced the effectiveness of this technique to detect oral cancer \\\\ \\hline\n    Nanditha B R et al. \u0026 2021 \u0026 An ensemble model with benefits of ResNet50 and VGG-16 \u0026 Ensemble model provided 96.2\\% accuracy, 98.14\\% sensitivity, and 94.23\\% specificity \u0026 The computational time of the model was very high \\\\ \\hline\n    \\end{tabular}\n\\end{table}\n","recorded":"2024-10-28 15:13:44.708515385","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102848.png","recorded":"2024-10-28 15:11:55.381553844","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102806.png","recorded":"2024-10-28 15:11:20.548414152","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102835.png","recorded":"2024-10-28 15:10:43.748238779","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102856.png","recorded":"2024-10-28 15:10:29.479990085","filePath":"null","pinned":false},{"value":"Welikala et al. (2020) introduced an ensemble deep learning model that integrates ResNet 101 and R-CNN for the categorization of oral cancer photos. This model was designed for mobile oral screening, capable of capturing real photos and classifying oral lesions accordingly. We employed various metrics, such as accuracy, precision, and F1-score, to assess the established approach. The results indicated that the proposed method achieved an F1 score of 87% for item identification, whereas the score for object detection diminished to just 41.8%.","recorded":"2024-10-28 15:08:54.305597216","filePath":"null","pinned":false},{"value":"(Welikala et al. 2020) proposed an ensemble deep learning model combining resnet\n101 and R cnn for classification of oral cnacer images. This model was developed to\nbe used for mobile mouth screening anywhere, this model take real images and classify\noral lesion from it. We used a variety of metrics, including accuracy, precision, and F1-\nscore, to evaluate the established method. The outcomes showed that the proposed method\nproduced F1 score of 87%, for object identification furthermore for object detection the\nscore was reduced to a mere 41.8%","recorded":"2024-10-28 15:08:45.788498572","filePath":"null","pinned":false},{"value":"Nanditha et al. (2021) established a computerized system for the automatic detection of oral cancer, proposing an ensemble model that combines ResNet50 and VGG-Skip. By integrating these networks, one may distinguish between images utilizing extracted features and a classifier. Images of oral lesions were collected from several colleges and hospitals in Karnataka. A total of 332 photographs of oral lesions were collected, comprising 269 precancerous lesions and 63 benign lesions. The proposed model outperforms other commonly used deep learning models in classifying oral pictures. The ensemble deep learning model achieved an accuracy of 96.2%, sensitivity of 98.14%, and specificity of 94.23%.","recorded":"2024-10-28 15:08:31.227643335","filePath":"null","pinned":false},{"value":"performs better than other widely utilised deep learning models when it comes to clas-\nsifying oral images. Using the ensemble deep learning model, 96.2% accuracy, 98.14%\nsensitivity, and 94.23% specificity were attained.","recorded":"2024-10-28 15:08:21.627449587","filePath":"null","pinned":false},{"value":"(Nanditha et al. 2021) developed a computerised system for automatic detecting oral\ncancer, an ensemble model was suggested with combination of resnet50 and vgg-skip, By\ncombining these networks, it is possible to differentiate between images using extracted\ncharacteristics and a classifier. Images of oral lesions were gathered from several col-\nleges and hospitals in Karnataka. A total of 332 photos of oral lesions wete gathered, of\nwhich 269 were precancerous lesions and 63 were benign lesions. The suggested model","recorded":"2024-10-28 15:08:13.668868723","filePath":"null","pinned":false},{"value":"Al-Rawi et al. (2022) suggested an AI-based model for diagnosing oral cancer, utilizing the ReLU activation function alongside a modified cross-entropy loss function. The model's dataset was sourced from four repositories: PubMed, Scopus, EBSCO, and OVID.\nThe prediction framework was assessed for its applicability and potential bias risk. The collection has a total of 7,245 patients and 69,425 images. The research assessed the efficacy of AI employing ten statistical methods. The accuracy varied between 43.5% and 100%, sensitivity ranged from 94% to 100%, specificity ranged from 96% to 100%, and an AUC of 93% was noted based on the findings of supervised machine learning.","recorded":"2024-10-28 14:54:00.489743217","filePath":"null","pinned":false},{"value":"S. Panigrahi et al. (2023) developed an innovative approach employing transfer learning and a suggested CNN model to concentrate on the binary categorization of oral histopathology images. Improving the pretrained VGG, ResNet, Inception, and MobileNet involves training half of the layers while keeping the remaining layers frozen. The experimental results indicate that ResNet50, with an accuracy of 96.6%, outperforms several fine-tuned DCNN models and the proposed baseline model significantly.","recorded":"2024-10-28 14:53:48.559240104","filePath":"null","pinned":false},{"value":"(Al-Rawi et al. 2022) proposed an AI based model for the diagnosis of oral can-\ncer which used the Relu activation function with a modified cross entropy loss function,\ndataset for the model was taken from 4 datasets(PubMed, Scopus, EBSCO, and OVID).\nUsing the prediction framework as a potential bias assessment tool, the applicability and\nrisk of bias were evaluated. The dataset consist a total of 7245 patients and 69,425 im-\nages. In the included research, the effectiveness of AI was evaluated using ten statistical\ntechniques. The accuracy ranged from 43.5% to 100%, the sensitivity ranged from 94%\nto 100%, the specificity ranged from 96% to 100%, and the AUC of 93% according to the\nresults of supervised machine learning was observed.","recorded":"2024-10-28 14:53:44.909675004","filePath":"null","pinned":false},{"value":"(S. Panigrahi et al. 2023) proposed a novel strategy that utilises transfer learning\nand a recommended CNN model to focus on binary classification of oral histopathology\npictures. By training half of the layers and leaving the other layers frozen, the pretrained\nVGG, ResNet, Inception, and MobileNet are improved. The experimental results show\nthat ResNet50, which has an accuracy of 96.6%, performs significantly better than a few\nfine-tuned DCNN models and the baseline model proposed.","recorded":"2024-10-28 14:53:38.337017325","filePath":"null","pinned":false},{"value":"Rasmussen et al. (2023) suggested a singular cycle dynamic segmentation model utilizing CT and the thickest cranial and caudal slices for each of the 16 organs most susceptible to head and neck cancer. This research included data from 730 planning CTs and clinical characteristics of patients with head and neck cancer. Ninety percent of the dataset was allocated for training, whereas ten percent was designated for an independent test set. Employing nnUNet single folds with default parameters. The models were evaluated utilizing the Dice similarity coefficient, the 95th percentile of the Hausdorff distance, and the average symmetric surface distance.\nThe medians of the dice in the proposed framework increased with single-cycle interactive segmentation, ranging from 0.004 to 0.009.","recorded":"2024-10-28 14:53:31.642686083","filePath":"null","pinned":false},{"value":"(Rasmussen et al. 2023) proposed a single cycle dynamic segmentation model using CT\nand the thickest cranial and caudal slices for each of the 16 organs most at risk for head\nand neck cancer. 730 planning CTs and clinical contours from patients who had head\nand neck cancer treated made up the data for this research. 90% of the data set was used\nfor training, and 10% was used for a separate test set. Using nnUNet single folds and\ndefault parameters. The models were compared using the Dice similarity coefficient, the\n95th percentile of the Hausdorff distance, and the average symmetric surface distance.\nThe medians of the dice in the proposed framework rose with single-cycle interactive\nsegmentation in the range of 0.004 to 0.009.","recorded":"2024-10-28 14:53:14.250694910","filePath":"null","pinned":false},{"value":"Afify, Mohammed, and Hassanien (2023) introduced an innovative deep learning model that use gradient class activation mapping to predict OSCC pictures. The proposed model utilizes a recent public resource containing 1224 normal oral histopathology images at both 100x and 400x magnifications, together with OSCC cells. Once the models' performances have been evaluated, the findings are compared, and the model exhibiting the optimal performance is selected. Diverse deep learning models were utilized to identify the optimal solution, and the Grad-CAM technique was employed to show the localization of cancerous regions. Among the various selected deep learning models for classification, EfficientNet-b0 was a lightweight model that attained an accuracy of 95%.","recorded":"2024-10-28 14:53:08.119998693","filePath":"null","pinned":false},{"value":"lignant area. Amongst the several chosen deep learning models for the classification\npurpose, EfficientNet-b0 was a light weight model that achieved the accuracy of 95%","recorded":"2024-10-28 14:52:37.996944764","filePath":"null","pinned":false},{"value":"(Afify, Mohammed, and Hassanien 2023) proposed a novel model of deep learning\nwhere in order to forecast the OSCC images it uses gradient class activation mapping. The\nsuggested model makes use of a recent public resource with 1224 normal oral histopathol-\nogy pictures both at 100x and 400x magnifications, and OSCC cells. The results are\ncompared once the models‚Äô performances have been estimated, and the model with the\nbest performance is chosen. Various deep learning models were applied to find the best\nsolution and Grad-cam algorithm was applied to present a visualized localization of ma","recorded":"2024-10-28 14:52:30.416484365","filePath":"null","pinned":false},{"value":"Oral cancer Detection using ensemble techniques","recorded":"2024-10-28 14:52:21.099353888","filePath":"null","pinned":false},{"value":"\\subsection{Oral cancer detection using Deep Learning} \\label{sec-02.01}","recorded":"2024-10-28 14:52:04.270256377","filePath":"null","pinned":false},{"value":"cancer detection using Deep learning","recorded":"2024-10-28 14:51:00.102115137","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Summary of studies on deep learning techniques for oral cancer detection} \\label{Table_01}\n    \\begin{tabular}{|p{3cm}|p{1.5cm}|p{4cm}|p{4cm}|p{4cm}|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Leandro Muniz de Lima et al., \u0026 2023 \u0026 A deep learning model pre-trained on ImageNet dataset \u0026 The model has balanced accuracy of 83.24\\% \u0026 Lower accuracy due to unbalanced dataset \\\\ \\hline\n    Jubair, F et al., \u0026 2022 \u0026 A lightweight EfficientNet-B0 was proposed \u0026 The model is lightweight and fast with 85\\% accuracy \u0026 The introduced system was insufficient for a larger dataset \\\\ \\hline\n    Sreerama Prasad et al., \u0026 2022 \u0026 Computer-aided tongue diagnosis system (CATSDNet) was used \u0026 The introduced model provided an accuracy of 92.3\\% \u0026 The dataset quality was poor \\\\ \\hline\n    Sumsum P Sunny et al., \u0026 2022 \u0026 Modified Unet on MSMF dataset \u0026 Model provided IOU of 0.73-0.76 \u0026 Unet not very effective and can be modified further \\\\ \\hline\n    Aritri Ghosh et al., \u0026 2022 \u0026 Modifications in the Raman and FTIR spectra using DNN \u0026 Testing accuracy of 83.33\\% and ROC of 0.88 observed \u0026 Lower accuracy observed due to unbalanced dataset \\\\ \\hline\n    Qiuyun Fu et al., \u0026 2020 \u0026 Two-phase learning with ensemble model \u0026 Algorithm offered an AUC of 0.983 and accuracy of 91.5\\% \u0026 Accuracy achieved at the cost of time complexity \\\\ \\hline\n    \\end{tabular}\n\\end{table}\n","recorded":"2024-10-28 14:43:44.912993789","filePath":"null","pinned":false},{"value":"\\begin{table}[H]\n    \\centering\n    \\caption{Summary of studies on deep learning techniques for oral cancer detection} \\label{Table_01}\n    \\resizebox{\\textwidth}{!}{ % This command scales the table to fit within the text width\n    \\begin{tabular}{|l|c|l|l|l|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Leandro Muniz de Lima et al., \u0026 2023 \u0026 A deep learning model pre-trained on ImageNet dataset \u0026 The model has balanced accuracy of 83.24\\% \u0026 Lower accuracy due to unbalanced dataset \\\\ \\hline\n    Jubair, F et al., \u0026 2022 \u0026 A lightweight EfficientNet-B0 was proposed \u0026 The model is lightweight and fast with 85\\% accuracy \u0026 The introduced system was insufficient for a larger dataset \\\\ \\hline\n    Sreerama Prasad et al., \u0026 2022 \u0026 Computer-aided tongue diagnosis system (CATSDNet) was used \u0026 The introduced model provided an accuracy of 92.3\\% \u0026 The dataset quality was poor \\\\ \\hline\n    Sumsum P Sunny et al., \u0026 2022 \u0026 Modified Unet on MSMF dataset \u0026 Model provided IOU of 0.73-0.76 \u0026 Unet not very effective and can be modified further \\\\ \\hline\n    Aritri Ghosh et al., \u0026 2022 \u0026 Modifications in the Raman and FTIR spectra using DNN \u0026 Testing accuracy of 83.33\\% and ROC of 0.88 observed \u0026 Lower accuracy observed due to unbalanced dataset \\\\ \\hline\n    Qiuyun Fu et al., \u0026 2020 \u0026 Two-phase learning with ensemble model \u0026 Algorithm offered an AUC of 0.983 and accuracy of 91.5\\% \u0026 Accuracy achieved at the cost of time complexity \\\\ \\hline\n    \\end{tabular}\n    }\n\\end{table}","recorded":"2024-10-28 14:41:54.476505012","filePath":"null","pinned":false},{"value":"Table \\ref{Table_01}.\n\\begin{table}[H]\n    \\centering\n    %\\small\t\n    \\caption{Summary of studies on deep learning techniques for oral cancer detection} \\label{Table_01}\n    \\addtolength{\\tabcolsep}{2.0pt}\n    \\begin{tabular}{|l|c|l|l|l|}\n    \\hline\n    \\textbf{Author} \u0026 \\textbf{Year} \u0026 \\textbf{Techniques} \u0026 \\textbf{Findings} \u0026 \\textbf{Limitations} \\\\ \\hline\n    Leandro Muniz de Lima et al., \u0026 2023 \u0026 A deep learning model pre-trained on imagenet dataset \u0026 The model has balanced accuracy of 83.24\\% \u0026 Lower accuracy due to unbalanced dataset \\\\ \\hline\n    Jubair, F et al., \u0026 2022 \u0026 A lightweight EfficientNet-B0 was proposed \u0026 The model is lightweight and fast with 85\\% accuracy \u0026 The introduced system was insufficient for a larger dataset \\\\ \\hline\n    Sreerama Prasad et al., \u0026 2022 \u0026 Computer-aided tongue diagnosis system (CATSDNet) was used \u0026 The introduced model provided an accuracy of 92.3\\% \u0026 The dataset quality was poor \\\\ \\hline\n    Sumsum P Sunny et al., \u0026 2022 \u0026 Modified Unet on MSMF dataset \u0026 Model provided IOU of 0.73-0.76 \u0026 Unet not very effective and can be modified further \\\\ \\hline\n    Aritri Ghosh et al., \u0026 2022 \u0026 Modifications in the Raman and FTIR spectra using DNN \u0026 Testing accuracy of 83.33\\% and ROC of 0.88 observed \u0026 Lower accuracy observed due to unbalanced dataset \\\\ \\hline\n    Qiuyun Fu et al., \u0026 2020 \u0026 Two-phase learning with ensemble model \u0026 Algorithm offered an AUC of 0.983 and accuracy of 91.5\\% \u0026 Accuracy achieved at the cost of time complexity \\\\ \\hline\n    \\end{tabular}\n\\end{table}\n","recorded":"2024-10-28 14:40:15.201471483","filePath":"null","pinned":false},{"value":"Table \\ref{Table_01}.\n\\begin{table}[H]\n    \\centering\n    %\\small\t\n    \\caption{Distribution of cases within the dataset} \\label{Table_01}\n    \\addtolength{\\tabcolsep}{2.0pt}\n    \\begin{tabular}{l l l l}\n    \\hline\n    %\\rowcolor[HTML]{EFEFEF}\n        Dataset     \u0026 Number of Cases \u0026 Percentage \u0026 Purpose   \\\\ \\hline\n        Training    \u0026 $34,655$        \u0026 $64\\%$     \u0026 Model Training   \\\\\n        Testing     \u0026 $10,830$        \u0026 $20\\%$     \u0026 Model Evaluation \\\\\n        Validation  \u0026 $8,664$         \u0026 $16\\%$     \u0026 Hyperparameter Tuning \\\\ \\hline\n    \\end{tabular}\n\\end{table}","recorded":"2024-10-28 14:39:43.209359154","filePath":"null","pinned":false},{"value":"Rubin et al. (2019) A unique deep learning technique for medical imaging has been created, targeting the challenge of a restricted training dataset, which is a fundamental limitation of deep learning, and applying it to the categorization of cancerous and healthy cell lines acquired using quantitative phase imaging. The proposed method, known as transferring of pre-trained generative adversarial network (TOP-GAN), integrates transfer learning with generative adversarial networks (GANs). Unstained cancer cell photos were sourced from many origins and amalgamated into a single collection. The suggested model attained a sensitivity rate of 98% in classifying cancer cells. The authors neglected to address class imbalance and time complexity.","recorded":"2024-10-28 14:30:52.769023849","filePath":"null","pinned":false},{"value":"(Rubin et al. 2019) A novel deep learning method for medical imaging has been\ndeveloped, which addresses the issue of a limited training set deep learning‚Äôs primary\nbottleneck and applies it to the classification of cancer and healthy cell lines obtained\nby quantitative phase imaging. The suggested technique, referred to as transferring of\npre-trained generative adversarial network (TOP-GAN), combines transfer learning with\ngenerative adversarial networks (GANs). Stain free cancer cell images were taken form\nmultiple sources and combined into one. The proposed model achieved an sensitivity rate\nof 98% while classifying the cancer cells. The authors did not take class imbalance and\ntime complexity into consideration","recorded":"2024-10-28 14:30:40.460170394","filePath":"null","pinned":false},{"value":"Ghosh et al. (2022) developed an approach. To categorize the epigenetic modifications identified in the Raman and FTIR spectra with a Deep Reinforcement Neural Network (DRNN). Utilizing data from many domains, RS and FTIR offer substantial benefits compared to conventional molecular biology methodologies. The threshold detection layer, together with the reinforced learning layer, is utilized in the feature extraction layer of the deep learning model to identify significant epigenetic features. The classification layer consists of N layers of back-propagated Artificial Neural Networks (ANN). A substantial spectral dataset was obtained and utilized to train the model. The testing accuracy of the suggested DRNN model is 83.33%. The ROC for the suggested DRNN model is 0.88.","recorded":"2024-10-28 14:26:49.503465879","filePath":"null","pinned":false},{"value":"(Ghosh et al. 2022) proposed a methodology To categories the epigenetic modifica-\ntions discovered in the Raman and FTIR spectra using a Deep neural network which is\nreinforced(DRNN). By using data from different areas, RS and FTIR provide significant\nadvantages over traditional molecular biology techniques. The threshold detection layer\nwith the reinforced learning layer are used in the feature extraction layer of the DL model\nto find important epigenetic features. N numbers of back-propagated Artificial Neural\nNetwork (ANN) layers make up the classification layer. A large spectral dataset was ac-\nquired and used to train the model. The proposed DRNN model‚Äôs testing accuracy is\n83.33%. ROC for the proposed DRNN model is 0.88.","recorded":"2024-10-28 14:24:08.249675617","filePath":"null","pinned":false},{"value":"S. P. Sunny et al. (2022) developed a semantic segmentation model to categorize segmented images following the isolation of Single Epithelial Cells (SEC) from fluorescent, multi-channel, microscopic oral cytology images. A total of 2730 differently stained, multi-channel, fluorescent microscopic images of the cytoplasm and nucleus were utilized to train the model. A novel bespoke Convolutional Neural Network (CNN) model, designated Artefact-Net, alongside the InceptionV3 model, was developed for data classification. The highest overall IoU (0.73-0.76) and for SEC segmentation (0.79) were achieved by the U-Net and modified U-Net models. The Artefact-Net surpassed InceptionV3 in cluster identification, achieving superior precision and F1 score (Precision: 0.91 compared to 0.80; F1: 0.91 compared to 0.86).","recorded":"2024-10-28 14:23:48.131508160","filePath":"null","pinned":false},{"value":"(S. P. Sunny et al. 2022) devised a semantic segmentation model to classify the seg-\nmented images after separating Single Epithelial Cells (SEC) from fluorescent, multi-\nchannel, microscopic oral cytology images. 2730 differentially stained multi-channel,\nfluorescent, microscopic pictures of the cytoplasm and nucleus were used to train the\nmodel. A new bespoke Convolutional-Neural-Network (CNN) model (Artefact-Net) and\nthe InceptionV3 model were trained to classify data. The best IoU overall (0.73-0.76)\nand for SEC segmentation (079) were provided by the U-Net and modified U-Net mod-\nels. When identifying clusters, the Artefact-Net outperformed InceptionV3 in terms of\nprecision and F1 score (Precision: 0.91 vs 0.80; F1: 0.91 vs 0.86).","recorded":"2024-10-28 14:23:39.656846221","filePath":"null","pinned":false},{"value":"Fu et al. (2020) Cascaded convolutional neural networks (CCNN) were emphasized for the detection of oral cavity squamous cell carcinoma (OCSCC) in clinical images of oral cancer. Forty-four thousand clinical photos were acquired from various hospitals in China over a period of fourteen years. The dataset was validated using six prestigious journals in the domain of oral surgery and dentistry. The area under receiver operating characteristic curves (AUCs), together with accuracy, sensitivity, and specificity, accompanied by two-sided 95% confidence intervals, were utilized to evaluate the algorithm's performance on the internal, external, and clinical validation datasets. An ensemble model was utilized for two-phase learning. The evaluation indicated that the suggested algorithm achieved an area under the curve (AUC) of 0.983 and an accuracy of 91.5% in the categorization of OCSCC lesions. The proposed model attained a high AUC, although its complexity escalated.","recorded":"2024-10-28 14:23:22.099705120","filePath":"null","pinned":false},{"value":"under curve (AUC) of 0.983 and accuracy of 91.5% during the classification of OCSCC\nlegions. The proposed model achieved a great AUC but the complexity increased","recorded":"2024-10-28 14:23:15.548872431","filePath":"null","pinned":false},{"value":"(Fu et al. 2020) Cascaded convolutional neural networks (CCNN) were highlighted for\ndetecting OCSCC on clinical images of oral cancer. 44000 clinical images were taken\nform multiple hospitals in China during a span of 14 years. The dataset was validated\nfrom six exemplary journals in the field of oral surgery and dentistry. Area under re-\nceiver operating characteristic curves (AUCs), accuracy, sensitivity, and specificity with\ntwo-sided 95% confidence intervals were used to assess the algorithm performance on\nthe internal, external, and clinical validation datasets. Two Phase learning with ensemble\nmodel was applied. The assessment showed that the proposed algorithm provided the area","recorded":"2024-10-28 14:23:10.665477699","filePath":"null","pinned":false},{"value":"Jubair et al. (2022) suggested a lightweight deep convolutional neural network utilizing EfficientNet-B0 for the diagnosis of oral cancer through clinical photos. The dataset comprised 716 oral cancer photos categorized into two classifications, and transfer learning techniques were employed to create the CNN. The dataset had over double the amount of malignant photos, resulting in a class imbalance. To address this imbalance, bootstrapping was employed with over 120 iterations. The suggested CNN's performance parameters for categorizing clinical images of the tongue as benign or worrisome. The suggested model achieved an accuracy of 85% in classifying oral cancer photos into two categories, which is much superior to prior methods.","recorded":"2024-10-28 14:23:01.350355432","filePath":"null","pinned":false},{"value":"(Jubair et al. 2022) proposed a lightweight deep convolutional neural network based\non EfficientNet-B0 for detection of oral cancer using clinical images. The dataset con-\nsisted of 716 oral cancer images belonging to 2 classes, transfer learning models was used\nto develop the CNN. The dataset contained more than twice numbers of cancerous images\nwhich caused a class imbalance, to solve class imbalance bootstrapping was applied it had\nmore than 120 repetitions. The proposed CNN‚Äôs performance metrics for classifying clin-\nical images of the tongue as either benign or suspicious. The proposed model provided\nan accuracy of 85% while classification of oral cancer images into 2 classes which was\ncomparably higher than other algorithms.","recorded":"2024-10-28 14:22:51.762611872","filePath":"null","pinned":false},{"value":"Lima et al. (2023) created an approach that evaluates the significance of supplementary information for computer-aided design in the analysis of histological pictures of oral leukoplakia and cancer. Between 2011 and 2021, a novel dataset (NDB-UFES) comprising 237 histopathological image samples and associated data was compiled.\nThe leading models, based on testing results, exhibit a balanced accuracy of 83.24% when utilizing images, demographic information, and clinical data with MetaBlock fusion and ResNetV2 as the foundation.","recorded":"2024-10-28 14:22:33.085434791","filePath":"null","pinned":false},{"value":"(Lima et al. 2023) developed a strategy that assesses the value of additional informa-\ntion for computer-aided design in the interpretation of histological images of leukoplakia\nof the mouth and carcinoma. From 2011 to 2021, a brand-new dataset (NDB-UFES) of\nhistopathological pictures and data was gathered which contained 237 image samples.\nThe top models, according to experimental findings,the model have balanced accuracy of\n83.24% when using pictures, demographic data, and clinical data using MetaBlock fusion\nand ResNetV2 as the foundation.","recorded":"2024-10-28 14:22:13.801307372","filePath":"null","pinned":false},{"value":"Chelluboina and Rao (2023) developed an innovative methodology centered on the CATD-SNet, a computer-aided tongue diagnosis system that utilizes tongue image analysis for disease prediction. The test image undergoes fast nonlocal mean (FNLM) filtering to preprocess the supplied tongue dataset. The pre-processed tongue images are subsequently utilized to extract color features through color moments. Furthermore, the texture features are derived via the grey level co-occurrence matrix (GLCM). The proposed CATDSNet is trained with a hybrid extreme learning machine (HELM) classifier to identify various diseases utilizing the extracted color and texture information.\nThe simulation results of the tongue image dataset indicate that the proposed CATDSNet model outperforms leading approaches such as random forest and support vector machine (SVM), with a classification accuracy of 92.3%.","recorded":"2024-10-28 14:21:59.940137681","filePath":"null","pinned":false},{"value":"order to preprocess the provided tongue dataset. The pre-processed tongue photos are then\nused to extract colour features using colour moments. In addition, the information about\nthe texture features is extracted using the grey level cooccurrence matrix (GLCM). The\nproposed CATDSNet is then trained using a hybrid extreme learning machine (HELM)\nclassifier to diagnose various diseases using the extracted colour and texture information.\nThe tongue image dataset simulation results show that the proposed CATDSNet model\nperforms better than state-of-the-art methods like random forest, support vector machine\n(SVM)with a classification accuracy of 92.3%.","recorded":"2024-10-28 14:21:50.991314504","filePath":"null","pinned":false},{"value":"(Chelluboina and Rao 2023) formulated a novel methodology that focus on the CATD-\nSNet, or computer-aided tongue diagnosis system, which employs tongue image analysis\nto predict disease. the test image is subjected to fast nonlocal mean (FNLM) filtering in","recorded":"2024-10-28 14:21:45.260719982","filePath":"null","pinned":false},{"value":"under curve (AUC) of 0.983 and accuracy of 91.5% during the classification of OCSCC\nlegions. The proposed model achieved a great AUC but the complexity increased\n(S. P. Sunny et al. 2022) devised a semantic segmentation model to classify the seg-\nmented images after separating Single Epithelial Cells (SEC) from fluorescent, multi-\nchannel, microscopic oral cytology images. 2730 differentially stained multi-channel,\nfluorescent, microscopic pictures of the cytoplasm and nucleus were used to train the\nmodel. A new bespoke Convolutional-Neural-Network (CNN) model (Artefact-Net) and\nthe InceptionV3 model were trained to classify data. The best IoU overall (0.73-0.76)\nand for SEC segmentation (079) were provided by the U-Net and modified U-Net mod-\nels. When identifying clusters, the Artefact-Net outperformed InceptionV3 in terms of\nprecision and F1 score (Precision: 0.91 vs 0.80; F1: 0.91 vs 0.86).\n(Ghosh et al. 2022) proposed a methodology To categories the epigenetic modifica-\ntions discovered in the Raman and FTIR spectra using a Deep neural network which is\nreinforced(DRNN). By using data from different areas, RS and FTIR provide significant\nadvantages over traditional molecular biology techniques. The threshold detection layer\nwith the reinforced learning layer are used in the feature extraction layer of the DL model\nto find important epigenetic features. N numbers of back-propagated Artificial Neural\nNetwork (ANN) layers make up the classification layer. A large spectral dataset was ac-\nquired and used to train the model. The proposed DRNN model‚Äôs testing accuracy is\n83.33%. ROC for the proposed DRNN model is 0.88.\n(Rubin et al. 2019) A novel deep learning method for medical imaging has been\ndeveloped, which addresses the issue of a limited training set deep learning‚Äôs primary\nbottleneck and applies it to the classification of cancer and healthy cell lines obtained\nby quantitative phase imaging. The suggested technique, referred to as transferring of\npre-trained generative adversarial network (TOP-GAN), combines transfer learning with\ngenerative adversarial networks (GANs). Stain free cancer cell images were taken form\nmultiple sources and combined into one. The proposed model achieved an sensitivity rate\nof 98% while classifying the cancer cells. The authors did not take class imbalance and\ntime complexity into consideration","recorded":"2024-10-28 14:21:18.121470436","filePath":"null","pinned":false},{"value":"order to preprocess the provided tongue dataset. The pre-processed tongue photos are then\nused to extract colour features using colour moments. In addition, the information about\nthe texture features is extracted using the grey level cooccurrence matrix (GLCM). The\nproposed CATDSNet is then trained using a hybrid extreme learning machine (HELM)\nclassifier to diagnose various diseases using the extracted colour and texture information.\nThe tongue image dataset simulation results show that the proposed CATDSNet model\nperforms better than state-of-the-art methods like random forest, support vector machine\n(SVM)with a classification accuracy of 92.3%.\n(Lima et al. 2023) developed a strategy that assesses the value of additional informa-\ntion for computer-aided design in the interpretation of histological images of leukoplakia\nof the mouth and carcinoma. From 2011 to 2021, a brand-new dataset (NDB-UFES) of\nhistopathological pictures and data was gathered which contained 237 image samples.\nThe top models, according to experimental findings,the model have balanced accuracy of\n83.24% when using pictures, demographic data, and clinical data using MetaBlock fusion\nand ResNetV2 as the foundation.\n(Jubair et al. 2022) proposed a lightweight deep convolutional neural network based\non EfficientNet-B0 for detection of oral cancer using clinical images. The dataset con-\nsisted of 716 oral cancer images belonging to 2 classes, transfer learning models was used\nto develop the CNN. The dataset contained more than twice numbers of cancerous images\nwhich caused a class imbalance, to solve class imbalance bootstrapping was applied it had\nmore than 120 repetitions. The proposed CNN‚Äôs performance metrics for classifying clin-\nical images of the tongue as either benign or suspicious. The proposed model provided\nan accuracy of 85% while classification of oral cancer images into 2 classes which was\ncomparably higher than other algorithms.\n(Fu et al. 2020) Cascaded convolutional neural networks (CCNN) were highlighted for\ndetecting OCSCC on clinical images of oral cancer. 44000 clinical images were taken\nform multiple hospitals in China during a span of 14 years. The dataset was validated\nfrom six exemplary journals in the field of oral surgery and dentistry. Area under re-\nceiver operating characteristic curves (AUCs), accuracy, sensitivity, and specificity with\ntwo-sided 95% confidence intervals were used to assess the algorithm performance on\nthe internal, external, and clinical validation datasets. Two Phase learning with ensemble\nmodel was applied. The assessment showed that the proposed algorithm provided the area","recorded":"2024-10-28 14:21:09.761687377","filePath":"null","pinned":false},{"value":"Oral cancer detection using Deep Learning","recorded":"2024-10-28 14:20:36.772371884","filePath":"null","pinned":false},{"value":"https://www.amazon.in/Heavy-Strips-Adhesive-Sticky-Fastener/dp/B081RMGKV6/ref=pd_vtp_h_pd_vtp_h_d_sccl_3/258-7013446-4850045?pd_rd_w=IWJAP\u0026content-id=amzn1.sym.6c9a4279-ad42-4fd6-b9a9-3cd14ede34c9\u0026pf_rd_p=6c9a4279-ad42-4fd6-b9a9-3cd14ede34c9\u0026pf_rd_r=3BH4GE86E9B4NVCT7NYR\u0026pd_rd_wg=F7WkD\u0026pd_rd_r=8e495ce0-1416-49a9-8e18-9c2f176171d6\u0026pd_rd_i=B081RMGKV6\u0026psc=1","recorded":"2024-10-28 14:03:16.200977543","filePath":"null","pinned":false},{"value":"A large number of deaths were recorded from oral cancer as a result of lack of its identification and late treatment. Oral cavity cancer has a significant mortality rate that is rising. It is crucial to develop and put into practise a method for detecting this malignancy early on. By identifying cancer early and adopting preventative measures, it is simple to limit the number of deaths brought on by the disease. Although many researchers have already conducted their research in the field of oral cancer disorders, there is still a great deal of research that may be done in this area owing to performance improvements. Machine learning has advanced to the point where getting more use out of it is all but impossible during the last several years. The performance of deep learning models has increased, but there is still a concern of model size, low accuracy, and high computation time. \n\n\\begin{itemize}\n    \\item To propose and implement an optimized deep learning algorithm for detection of oral cancer in its early stages.\n    \\item To implement metaheuristic optimization for better weight selection of clinical images.\n    \\item To conduct an analysis and compare the proposed approach with state-of-the-art models based on evaluation metrics like accuracy, precision, sensitivity, and specificity.\n\\end{itemize}\n","recorded":"2024-10-28 13:33:22.572209837","filePath":"null","pinned":false},{"value":"A large number of deaths were recorded from oral cancer as a result of lack of its identification and late treatment. Oral cavity cancer has a significant mortality rate that is rising. It is crucial to develop and put into practise a method for detecting this malignancy early on. By identifying cancer early and adopting preventative measures, it is simple to limit the number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of research that may be done in this area owing to performance improvements. Machine learning has advanced to the point where getting more use out of it is all but impossible during the last several years. The performance of Deep learning models have increased but there is still a concern of model size, low accuracy and high computation time. \n\n‚Ä¢ To propose and implement optimized Deep learning algorithm for detection of oral cancer in it‚Äôs early stages.\n‚Ä¢ To implement Metaheuristic optimization for better weight selection of clinical images.\n‚Ä¢ To conduct an analysis and compare the proposed approach with state of art models on basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.","recorded":"2024-10-28 13:32:22.877725580","filePath":"null","pinned":false},{"value":"A large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n‚Ä¢ To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in it‚Äôs early stages.\n‚Ä¢ To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n‚Ä¢ To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.","recorded":"2024-10-28 13:31:41.122259700","filePath":"null","pinned":false},{"value":"The recent rise in mouth cancer is negatively impacting human health. Oral cancer is treatable if identified early; given the rising incidence of oral cancer cases, there is a pressing need for a precise and rapid method to detect cancer cells. The risk of oral cancer exists across all age demographics; however, older individuals are more susceptible due to poor lifestyles. Many individuals have had financial difficulties. The early detection of sickness is essential for enabling patients to initiate preventative actions promptly. Artificial intelligence, encompassing machine learning and deep learning, is fundamentally dependent on categorization, grading, segmentation, and computer vision. The primary motivation for conducting research in this topic is to enhance model optimization.\n\n\\begin{itemize}\n    \\item The concept of deep learning captivates my desire to acquire further knowledge in this field. A deep learning-based approach can identify early indications of mouth cancer detectable by contemporary cameras.\n    \\item Clinical images can provide more accurate and rapid results compared to conventional methods employed by physicians.\n\\end{itemize}\n","recorded":"2024-10-28 13:30:45.320221387","filePath":"null","pinned":false},{"value":"The recent rise in mouth cancer is negatively impacting human health. Oral cancer is treatable if identified early; given the rising incidence of oral cancer cases, there is a pressing need for a precise and rapid method to detect cancer cells. The risk of oral cancer exists across all age demographics; however, older individuals are more susceptible due to poor lifestyles. Many individuals have had financial difficulties. The early detection of sickness is essential for enabling patients to initiate preventative actions promptly. Artificial intelligence, encompassing machine learning and deep learning, is fundamentally dependent on categorization, grading, segmentation, and computer vision. The primary motivation for conducting research in this topic is to enhance model optimization.\n‚Ä¢ The concept of deep learning captivates my desire to acquire further knowledge in this field. A deep learning-based approach can identify early indications of mouth cancer detectable by contemporary cameras.\n‚Ä¢ Clinical images can provide more accurate and rapid results compared to conventional methods employed by physicians.","recorded":"2024-10-28 13:30:27.489185718","filePath":"null","pinned":false},{"value":"The latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n‚Ä¢ Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n‚Ä¢ Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors","recorded":"2024-10-28 13:29:38.132635645","filePath":"null","pinned":false},{"value":"Deep learning has emerged as a potent method in artificial intelligence, allowing models to independently learn from extensive, unstructured data sets via deep neural network topologies (Dubuc et al. 2022). In contrast to conventional machine learning, deep learning utilizes unsupervised and sophisticated neural networks to discern complicated patterns in data, mirroring certain decision-making processes of the human brain. The significance is highlighted by substantial enhancements in computational speed and memory, enabling models to learn from vast data sets, even when the input is noisy or confusing. The capacity to manage intricate data kinds and subtle characteristics renders deep learning particularly appropriate for applications in medical diagnostics, such as cancer diagnosis.\n\nDeveloping deep learning models from inception necessitates an extensive labeled dataset and substantial time, frequently spanning weeks or months, to fine-tune learning rates and attain precision. Transfer learning has emerged as a widely used technique, facilitating the adaption of pre-trained models, such as AlexNet or GoogleNet, to novel categories with diminished data requirements and lower computational costs. Deep learning facilitates feature extraction, wherein distinct characteristics are derived from several network levels, assisting in applications such as picture classification.\n\nIn medical imaging for cancer diagnosis, deep learning improves diagnostic accuracy, aids clinical decision-making, and minimizes significant data preprocessing by effectively detecting pertinent features. Methods include pre-processing, feature extraction, and feature selection are essential for enhancing model performance and minimizing computation time. This facilitates models that attain significant diagnostic accuracy while enhancing workflow efficiency and patient-clinician interactions, ultimately leading to more tailored treatment options and improved results in cancer care.\n\nMetaheuristic optimization, in conjunction with deep learning, is essential for effectively addressing difficult, nonlinear optimization challenges. Nature-inspired metaheuristic algorithms, such as swarm-based approaches, provide flexible and adaptable solutions to problems characterized by ambiguous search spaces or many local optima. Their simplicity, ease of implementation, and capacity to address complex real-world optimization challenges render them indispensable tools in domains necessitating computing efficiency, such as medical diagnosis and image processing. Deep learning and metaheuristic optimization collaboratively create a synergistic framework that improves the accuracy, efficiency, and application of AI-driven diagnostic tools.","recorded":"2024-10-28 13:27:35.331638169","filePath":"null","pinned":false},{"value":"Deep learning has emerged as a powerful approach within artificial intelligence, enabling models to autonomously learn from vast, unstructured data sets through deep neural network architectures (Dubuc et al. 2022). Unlike traditional machine learning, deep learning leverages unsupervised and complex neural networks to identify intricate patterns in data, emulating some decision-making aspects of the human brain. Its importance is underscored by significant improvements in computational speed and memory, which have empowered models to learn from extensive data pools even when the information is noisy or ambiguous. This ability to handle complex data types and nuanced features makes deep learning especially suited to applications in medical diagnostics, including cancer detection.\n\nTraining deep learning models from scratch requires a large labeled dataset and considerable time, often weeks or months, to optimize learning rates and achieve accuracy. However, transfer learning has become a prevalent method, enabling the adaptation of pre-trained models, such as AlexNet or GoogleNet, to new categories with less data and reduced computational demand. Deep learning also supports feature extraction, whereby specific features are drawn from multiple network layers, aiding in applications like image categorization.\n\nIn the context of medical imaging for cancer detection, deep learning‚Äôs capabilities enhance diagnostic accuracy, facilitate clinical decision-making, and reduce the need for extensive data preprocessing by efficiently identifying relevant features. Techniques such as pre-processing, feature extraction, and feature selection are crucial in optimizing model performance and reducing computation time. This allows for models that not only achieve a high level of diagnostic precision but also improve workflow efficiency and patient-clinician interactions, ultimately contributing to more personalized treatment options and better outcomes in cancer care.\n\nComplementing deep learning, metaheuristic optimization plays a crucial role in efficiently solving complex, nonlinear optimization problems. Metaheuristic algorithms, inspired by nature, such as swarm-based techniques, offer flexible, adaptable solutions to challenges involving ambiguous search spaces or multiple local optima. Their simplicity, ease of implementation, and capability to handle real-world, intricate optimization problems make them invaluable tools in fields that require computational efficiency, including medical diagnosis and image analysis. Together, deep learning and metaheuristic optimization form a synergistic framework, enhancing the accuracy, efficiency, and applicability of AI-driven diagnostic tools.","recorded":"2024-10-28 13:27:17.404476765","filePath":"null","pinned":false},{"value":"Deep Learning and Metaheuristic Optimization","recorded":"2024-10-28 13:26:55.455734125","filePath":"null","pinned":false},{"value":"C","recorded":"2024-10-28 13:25:56.737733786","filePath":"null","pinned":false},{"value":"1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, tglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.","recorded":"2024-10-28 13:21:08.837484650","filePath":"null","pinned":false},{"value":"1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a lefrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians‚Äô work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n‚Ä¢ Pre ‚ÄìProcessing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n‚Ä¢ Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n‚Ä¢ Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity","recorded":"2024-10-28 13:21:01.547658142","filePath":"null","pinned":false},{"value":"1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.","recorded":"2024-10-28 13:20:41.182409114","filePath":"null","pinned":false},{"value":"The primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.","recorded":"2024-10-28 12:43:16.800950210","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/102852.png","recorded":"2024-10-28 12:41:39.984905696","filePath":"null","pinned":false},{"value":"Oral cancer is one of the most common cancers globally. The predominant instances of this subtype of head and neck carcinoma originate in the squamous epithelial cells that line the oral cavity, tongue, and facial regions. If this is not discovered and managed promptly, it could be fatal. Approximately 53,000 cases of oral cancer, including three percent of all cancers recognized annually in the United States, are associated with mouth cancer. Oral cancer occurs more frequently in males than in females, with males affected more than twice as often, and individuals over the age of 40 are at the highest risk.\nSmoking, using alcoholic beverages, and having a Human Papillomavirus (HPV) infection are the primary contributors of oral cancer. In 2020, global fatalities from lip and oral cavity cancer are projected to exceed 177,000. Despite advancements, mortality rates from oral cancer have persisted at elevated levels in recent decades.\nThe majority of oral cancer patients, especially in rural areas, are unable to access prompt and appropriate diagnosis and treatment, hence diminishing their survival prospects.\nPatients with cancer exhibit a five-year survival rate of approximately 50%, contingent upon race and geographic area. Reports indicate that the survival rate in wealthy countries may attain 65%.\nConversely, depending on the region impacted by oral cancer, a survival rate of fifteen percent is observed in certain rural locations. Cancer therapy can be prohibitively costly, particularly in advanced stages. Both health experts and the general population possess a limited understanding of mouth cancer. The 2020 Cancer Statistics Report for India indicates that 66.6 percent of patients with head and neck cancer had already experienced local progression at the time of diagnosis. Persistent inflammation or non-healing ulcers, accompanied by irritation and hemorrhage, are indicative of oral cancer.\nOral cancer may be attributed to several activities, with smoking and alcohol consumption being the two most prominent factors. The consumption of maggots is prevalent in India, resulting in internal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) projected that in 2018, there would be 177,384 cancer-related fatalities and 354,864 new cancer cases, representing two percent and one point nine percent of all cancer occurrences and deaths, respectively. Mouth cancer, including approximately one-third of all cancer cases, is a significant cause of mortality in Bangladesh, Pakistan, Taiwan, and India.","recorded":"2024-10-28 12:38:33.906111598","filePath":"null","pinned":false},{"value":"which corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.","recorded":"2024-10-28 12:38:09.999269887","filePath":"null","pinned":false},{"value":"2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,","recorded":"2024-10-28 12:38:04.041294314","filePath":"null","pinned":false},{"value":"deaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncan‚Äôt obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. It‚Äôs because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in","recorded":"2024-10-28 12:37:48.647539622","filePath":"null","pinned":false},{"value":"Among the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000","recorded":"2024-10-28 12:36:18.690354254","filePath":"null","pinned":false},{"value":"/home/karna/.config/waybar/Backup/scripts\n/home/karna/.config/waybar/Backup/config.jsonc\n/home/karna/.config/waybar/Backup/style.css\n/home/karna/.config/waybar/Backup/theme.css","recorded":"2024-10-28 11:48:06.685404465","filePath":"null","pinned":false},{"value":"/home/karna/.config/waybar/scripts\n/home/karna/.config/waybar/backup.css\n/home/karna/.config/waybar/config\n/home/karna/.config/waybar/config-background\n/home/karna/.config/waybar/machiatto.css\n/home/karna/.config/waybar/style.css\n/home/karna/.config/waybar/style-background.css","recorded":"2024-10-28 11:47:54.855972950","filePath":"null","pinned":false},{"value":"hyprland","recorded":"2024-10-28 11:44:06.173653879","filePath":"null","pinned":false},{"value":"JetBrainsMono Nerd Font","recorded":"2024-10-28 11:39:59.278901117","filePath":"null","pinned":false},{"value":"\"tooltip\": true,\n","recorded":"2024-10-28 11:34:06.913000292","filePath":"null","pinned":false},{"value":"        \"tooltip-format\": \"{ipaddr}\",\n","recorded":"2024-10-28 11:33:28.687576143","filePath":"null","pinned":false},{"value":"#custom-left6 {\n    color: @pulseaudio;\n    background: @background;\n    padding-left: 3px;\n}\n","recorded":"2024-10-28 11:31:15.519552766","filePath":"null","pinned":false},{"value":"        \"custom/left6\",\n","recorded":"2024-10-28 11:29:44.377074591","filePath":"null","pinned":false},{"value":"        \"idle_inhibitor\",\n","recorded":"2024-10-28 11:27:09.943600402","filePath":"null","pinned":false},{"value":"    foreground: #DA9320;\n","recorded":"2024-10-28 11:26:01.715829434","filePath":"null","pinned":false},{"value":"#DA9320","recorded":"2024-10-28 11:21:08.906725758","filePath":"null","pinned":false},{"value":"#4C2105","recorded":"2024-10-28 11:20:15.628575784","filePath":"null","pinned":false},{"value":"@define-color background        #11111b;\n@define-color foreground        #cdd6f4;\n\n/* Workspace Button Colors */\n@define-color active-bg         #9399b2;\n@define-color active-fg         #11111b;\n@define-color hover-bg          #45475a;\n@define-color hover-fg          #cdd6f4;\n\n/* Wlogout Colors */\n@define-color bar-bg            #11111b;\n@define-color main-bg           #11111b;\n@define-color main-fg           #cdd6f4;\n@define-color wb-act-bg         #9399b2;\n@define-color wb-act-fg         #11111b;\n@define-color wb-hvr-bg         #6c7086;\n@define-color wb-hvr-fg         #11111b;","recorded":"2024-10-28 11:18:14.540912438","filePath":"null","pinned":false},{"value":"/home/karna/dotfiles/XFCEPic/Pictures/WallPapers/naruto-uzumaki.png","recorded":"2024-10-26 15:36:31.286498910","filePath":"null","pinned":false},{"value":"src=\"https://megacloud.tube/embed-1/e-1/74yheM7k6e8I?autoPlay=0\"","recorded":"2024-10-24 22:37:04.349279158","filePath":"null","pinned":false},{"value":"https://archive.org/download/moonfall-60fps-dualgb/moonfall-60fps-dualgb_archive.torrent","recorded":"2024-10-24 22:24:16.007383104","filePath":"null","pinned":false},{"value":"https://archive.org/download/moonfall-60fps-dualgb/Moonfall60fpsDUALGB.mpv","recorded":"2024-10-24 22:23:51.659750878","filePath":"null","pinned":false},{"value":"There are different techniques that are used for the diagnosis of oral cancer, a few of the clinical techniques used by doctors are discussed below:\n\n\\textbf{Barium Swallow:} The voice box, throat, and surroundings may display abnormalities during a barium swallow test, which is also frequently used to find small, early oral tumors.\n\n\\textbf{Biopsy:} The initial step in identifying mouth cancer is an oral tissue biopsy. A small amount of abnormal tissue from the area where oral cancer is suspected is removed by the surgeon during the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following types of biopsies are frequently used to identify oral carcinoma:\n\n\\begin{itemize}\n    \\item \\textbf{Incisive biopsies:} The region has a small amount of tissue taken from it that appears to be abnormal. If the abnormal location is easily accessible, the specimen could be obtained at the office of a doctor. If the cancer is more deeply embedded in the mouth or throat, biopsy procedures might have to be carried out in a surgical theatre while receiving anesthesia in order to lessen pain.\n    \n    \\item \\textbf{Exfoliative cytology:} Cell samples are gently scraped from a questionable location. To make the cells visible under a microscope, they are placed on a transparent slide and subsequently colored. A deeper biopsy will be done if any cells seem suspicious.\n\\end{itemize}\n\n\\textbf{Image-based tests:}\n\n\\begin{itemize}\n    \\item \\textbf{Computerized Tomography (CT) Scan:} Information on the size, shape, and location of any tumors can be obtained via a CT scan, which can help detect lymph nodes that are bulging and may contain cancer cells.\n    \n    \\item \\textbf{Magnetic Resonance Imaging (MRI):} Oral cancer may be examined with an MRI scan, although this is less usual. MRIs give a very detailed picture and may be very helpful in figuring out whether other areas of the body, such as the neck, have been affected by the disease's spread.\n    \n    \\item \\textbf{Positron Emission Tomography (PET):} Patients with cancer of the oral cavity might get a scan using PET technology to determine if the disease has migrated to the lymph nodes or has recently progressed to that location.\n    \n    \\item \\textbf{Genomic Testing for Advanced Oral Cancer:} Genomic testing, sometimes known as molecular profiling or cancer sequencing, involves examining the collected cells from a biopsy to check for any genetic mutations (changes in DNA) that might be connected to the person‚Äôs specific type of cancer.\n\\end{itemize}\n","recorded":"2024-10-24 16:39:18.468177891","filePath":"null","pinned":false},{"value":"There are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n‚Ä¢ Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n‚Ä¢ Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n‚Ä¢ Computerized Tomography, or CT, Scanning ‚Äì Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n‚Ä¢ Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the disease‚Äôs spread.\n‚Ä¢ Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n‚Ä¢ Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the person‚Äôs particular type of cancer.","recorded":"2024-10-24 16:36:34.237729235","filePath":"null","pinned":false},{"value":"/home/karna/Pictures/Screenshots/mouth structure.png","recorded":"2024-10-24 14:44:02.307887243","filePath":"null","pinned":false},{"value":"The Mandible (Lower Jawbone)","recorded":"2024-10-24 14:42:09.927807496","filePath":"null","pinned":false},{"value":"Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.","recorded":"2024-10-24 14:41:53.330676270","filePath":"null","pinned":false},{"value":"The roof of the mouth","recorded":"2024-10-24 14:41:48.149986970","filePath":"null","pinned":false},{"value":"The buccal mucosa, which coats the cheekbones‚Äô interior","recorded":"2024-10-24 14:41:37.876343214","filePath":"null","pinned":false},{"value":"The Uvula and the Tongue","recorded":"2024-10-24 14:41:31.764317912","filePath":"null","pinned":false},{"value":"The Tonsils and The Soft Palate","recorded":"2024-10-24 14:30:07.501523361","filePath":"null","pinned":false},{"value":"The Lips","recorded":"2024-10-24 14:29:37.758953009","filePath":"null","pinned":false},{"value":"The human mouth commences at the junction of the lips and skin. Figure 1.1 illustrates the anatomy of the human oral cavity. The palette consists of both hard and soft components. The soft palate separates the mouth from the nasopharynx, the upper segment of the pharynx, which is linked to the mouth through the oropharynx, the middle region of the pharynx. The lateral aspects of the mouth are constituted by the inner surface of the cheeks (De Angeli et al. 2022). The tongue occupies the majority of the floor of the mouth.\nThe mouth can be categorized into several components, including:","recorded":"2024-10-24 14:29:14.604650366","filePath":"null","pinned":false},{"value":"The start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks‚Äô inner surface (De Angeli et\nal. 2022). The majority of the mouth‚Äôs floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:","recorded":"2024-10-24 14:29:03.166788082","filePath":"null","pinned":false},{"value":"/mnt/Media/Magician Launcher.app\n/mnt/Media/Magician Launcher.exe\n/mnt/Media/RootCA.crt","recorded":"2024-10-24 14:10:33.155916880","filePath":"null","pinned":false},{"value":"/run/media/karna/T7/Magician Launcher.app\n/run/media/karna/T7/Magician Launcher.exe\n/run/media/karna/T7/RootCA.crt","recorded":"2024-10-24 14:08:36.993377931","filePath":"null","pinned":false},{"value":"Mutations in the cells of the mouth or lips lead to the development of oral carcinomas. DNA contains the directives for cellular functions. When typically functioning cells would perish, modifications induce the persistent proliferation and division of the cells. The anomalous oral carcinoma cells might aggregate into a neoplasm. Eventually, they may disseminate from the oral cavity throughout the entire body, encompassing the neck and other regions of the head.\nMouth cancers typically originate in the flat, thin squamous cells that comprise the surface of the lips and the interior of the mouth. Oral cancer is predominantly induced by squamous cell carcinomas.","recorded":"2024-10-24 13:56:56.620897761","filePath":"null","pinned":false},{"value":"When DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.","recorded":"2024-10-24 13:56:40.926593375","filePath":"null","pinned":false},{"value":"Cancer can develop in the lips, tongue, inner lining of the cheek, gums, mouth, and the hard and soft palate.","recorded":"2024-10-24 13:55:22.507309735","filePath":"null","pinned":false},{"value":"The following organs can develop cancer:\n\\begin{enumerate}\n    \\item Lips\n    \\item Tongue\n    \\item Inner lining of the cheek\n    \\item Gums\n    \\item Mouth Cancer\n    \\item Hard and Soft Palate\n\\end{enumerate}","recorded":"2024-10-24 13:55:00.043487526","filePath":"null","pinned":false},{"value":"Painful or arduous deglutition","recorded":"2024-10-24 13:54:40.201201929","filePath":"null","pinned":false},{"value":"A growth or protrusion within the oral cavity; mobility of teeth; ‚Ä¢ Painful or arduous deglutition","recorded":"2024-10-24 13:54:33.615566190","filePath":"null","pinned":false},{"value":"An oral or labial ulcer that remains unhealed","recorded":"2024-10-24 13:54:26.320951419","filePath":"null","pinned":false},{"value":"An intraoral lesion that is either white or red","recorded":"2024-10-24 13:54:20.249709675","filePath":"null","pinned":false},{"value":"An internal mouth patch that is either white or red","recorded":"2024-10-24 13:54:04.870851335","filePath":"null","pinned":false},{"value":"A mouth or lip sore that does not heal","recorded":"2024-10-24 13:53:57.931914779","filePath":"null","pinned":false},{"value":"Mouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:","recorded":"2024-10-24 13:53:22.116184847","filePath":"null","pinned":false},{"value":"Mouth cancer, or oral cancer, occurs when a tumor develops within the lining of the mouth. It may be situated on the tongue's surface, the inner cheeks, the palate, the lips, or the gums. Furthermore, the glands responsible for saliva production, specifically the tonsils located at the posterior region of the oral cavity, might develop malignancies. However, these events transpire with regularity.\nManifestations of oral cancer:","recorded":"2024-10-24 13:53:13.703937480","filePath":"null","pinned":false},{"value":"Mouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n‚Ä¢ A mouth or lip sore that does not heal\n‚Ä¢ An internal mouth patch that is either white or red\n‚Ä¢ A growth or bulge inside your mouth; loose teeth;\n‚Ä¢ Painful or difficult swallowing","recorded":"2024-10-24 13:52:47.918901975","filePath":"null","pinned":false},{"value":"\\subsection{Oral Cancer} \\label{sec-01.01}","recorded":"2024-10-24 13:52:24.180689948","filePath":"null","pinned":false},{"value":"The following organs can develop cancer:\n‚Ä¢ Lips\n‚Ä¢ Tongue\n‚Ä¢ Inner lining of the cheek\n‚Ä¢ Gums\n‚Ä¢ Mouth Cancer\n‚Ä¢ Hard and Soft Palate","recorded":"2024-10-24 13:34:22.098787690","filePath":"null","pinned":false},{"value":"Type of Oral Cancer","recorded":"2024-10-24 13:33:12.677862193","filePath":"null","pinned":false},{"value":"The cells of the mouth are the initial sites of oral cancer development. A malignant nodule is a cluster of cancer cells capable of infiltrating adjacent tissue and causing significant harm. It can also metastasis to various regions of the body. The lymph nodes in the neck are the primary sites for the metastasis of oral cancer. Oral cancer is also known as mouth cancer. Occasionally, oral cells undergo alterations that impede their growth or correct functioning. These modifications may lead to benign malignancies such as warts and fibromas. Precancerous conditions may also be induced by alterations in the cells of the oral cavity. This suggests that while the aberrant cells are currently not cancerous, there exists a possibility that they may progress to cancer if left untreated. Leukoplakia and erythroplakia are two of the most common precancerous conditions of the oral cavity.\nOral cancer may, however, occasionally arise from modifications to the cellular architecture of the mouth. The oral mucosa is a mucosal membrane that lines the oral cavity. The squamous epithelium, constituting the oral mucosa, consists of squamous cells. Mouth cancer often originates in these thin, flat squamous cells. The designation for this type of malignancy is oral squamous cell carcinoma.","recorded":"2024-10-24 13:32:44.422801143","filePath":"null","pinned":false},{"value":"The mouth‚Äôs cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouth‚Äôs cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if lefuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouth‚Äôs cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.","recorded":"2024-10-24 13:31:23.499089763","filePath":"null","pinned":false},{"value":"Oral Cancer detection using Deep Learning","recorded":"2024-10-24 13:30:48.931760804","filePath":"null","pinned":false},{"value":"Cancer is a disease caused by abnormal cells proliferating uncontrollably within the body. A segment of the body's cells in all cancers initiates fast division and disseminates to adjacent tissues. Cancer can manifest in practically any location within the multitude of cells in the body. Typically, human cells proliferate and divide to generate new cells as needed by the body. When a cell sustains injury or reaches senescence, it ceases to function and is supplanted by a new cell. However, when cancer progresses, this systemic mechanism deteriorates.\nOld or damaged cells that ought to have perished persist, as cells increasingly become erroneous, while new cells are produced even when they are undesirable. These cells possess the capability to proliferate, potentially resulting in tumor-like formations. Solid tumors, or tissue lumps, are a prevalent kind of cancer. Leukemias and other hematologic malignancies generally do not become solid tumors.","recorded":"2024-10-24 13:29:36.710421208","filePath":"null","pinned":false},{"value":"Cancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the body‚Äôs cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors","recorded":"2024-10-24 13:27:07.852367038","filePath":"null","pinned":false},{"value":"1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.\n7\n1.7.2\n CNN\nConvolutional neural network (CNN) is a subtype of ANN. In at least one of their layers,\nCNN‚Äôs replace conventional matrix multiplication methods with the convolution mathe-\nmatical technique. Since they were developed specifically to handle pixel data, they are\nused in image recognition and processing. The design with which CNN is built is compa-\nrable with the model of neural connection like a person‚Äôs brain (Jeyaraj, B. K. Panigrahi,\nand Samuel Nadar 2022). Because of the way CNN is built, there are some strong prefer-\nences ingrained in them, which makes it easier to comprehend why they are so effective.\nCNN can be seen as a feed-forward network but having connection with each image can\nFigure 1.3: A CNN Architecture\n(Sun et al. 2019)\nbe inefficient. Therefore, we can prune the useless connection between the hidden layers\nto increase the performance of the layer. A CNN is a special artificial neural network with\nlimited connections between the layers of artificial neural network.\n‚Ä¢ Max-Pooling: Each feature map produced by processing the input through many\nlayers of convolution is subsequently combined in a pooling layer. Little grids are\nused for input for pooling procedures, which generate only one value for every re-\ngion. The pooling layers provide CNN significant translational consistency since a\n8\ntiny change in the input image causes a slight modification in the activation maps.\nApplying convolutions with longer strides is another method for obtaining the pool-\ning‚Äôs down sampling effect. The network design is made simpler by eliminating the\npooling levels without compromising performance. Max-pooling is the most widely\nemployed of all these pooling techniques.\n‚Ä¢ Fully-Connected Layers: Matrix multiplications have traditionally been the build-\ning blocks of neural networks, which are scattered with sigmoid nonlinearities. The\nlayers of the multiplication matrices are referred to as connected layers due to the\nconnection between each unit in the layer before and each unit in the layer af-\nter. There is just small-scale spatial connectivity when using convolutional layers.\nSignificant amounts of completely linked layers are typically avoided in modern\nnetworks since they require massive parameters.\n‚Ä¢ Learning algorithm: Lacking an algorithm to quickly and effectively learn the pa-\nrameters of the model, there is little value for an expensive model. Lacking a tech-\nnique for efficiently acquiring the model‚Äôs parameters, a strong, expressive model\nis of little use. In the pre-AlexNet era, greedy layer-wise pre-training techniques\nattempted to create such an efficient approach. A more straightforward supervised\ntraining approach is sufficient to learn a reliable model for tasks relating to com-\nputer vision.\n‚Ä¢ Optimization Based on Gradient: - Typically, the backpropagation technique is used\nto train networks, which accelerates mathematical calculation to calculate the gra-\ndient used in the Gradient Descent (GD) algorithm. However, employing GD is\nimpracticable for datasets with many hundreds or even more data points. In these\ncircumstances, Stochastic Gradient Descent (SGD), an approximation where gradi-\nents are computed for data points individually rather than the complete data set, is\nfrequently used. Training using SGD generalizes more successfully than with GD,\nit has been discovered.\n‚Ä¢ Batch Normalization:- A helpful regularizes that enhances generalisation and sharply\naccelerates convergence is batch normalisation (BN). The order of presentation of\nthe inputs to each layer varies continuously during the training phase, which is a\nproblem caused by inner covariate variation. This effect typically causes training\n9\nto take longer and requires careful initialization. This problem is addressed by BN,\nwhich normalises a layer‚Äôs production stimulation to ensure that its spectrum is\nconstrained to a restricted range. In particular, BN normalises each mini-batch‚Äôs\nmean-variance statistics using its running average. Recently, BN has been recog-\nnised as a crucial element of very deep networks.\n‚Ä¢ Activation layer :- Deep networks typically have convolutions after each layer,\nwhich then follows a nonlinear process. This is required because convolutions are\nan example of a cascading linear system. Layer-to-layer nonlinearities make the\nmodel more evocative than a model with linear dynamics. Theoretically, as long\nas nonlinearities are ongoing bounded, and gradually rising, no nonlinearity has a\ngreater capacity for expressiveness than any other. The sigmoid or the tanh were\nnonlinearities employed in classical neural networks that feed forward. However,\nthe Rectified Linear Unit (ReLU) is used in contemporary convolutional networks.\nIt has been discovered that CNNs with this nonlinearity train more quickly. The\nleaky- ReLU is a brand-new category of nonlinearity that has lately been intro-\nduced. Leaky-ReLU(x) = max(0, x) + min(0, x) is its formula, where is a preset\nparameter. It is better since it implies that the characteristic can also be taught,\ncreating a model that is considerably deeper. Leaky ReLUs or adjustable ReLUs\nare examples of variations on ReLU(z)=max (0; z). The feature maps, which are\nfrequently also referred to as feature maps, are fed through a process of activation\nto create new tensors.\n1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a less\n10\nfrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians‚Äô work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n‚Ä¢ Pre ‚ÄìProcessing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n‚Ä¢ Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n‚Ä¢ Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity\nFigure 1.4: Flow chart showing different Phases in detection of oral cancer\n1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, the\n12\nglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.\n1.9\n Motivation\nThe latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n‚Ä¢ Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n‚Ä¢ Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors\n1.10\n Problem Statement and Research Objective\nA large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n‚Ä¢ To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in it‚Äôs early stages.\n‚Ä¢ To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n‚Ä¢ To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.\n1.11\n Thesis outline\nThe chapters of the thesis are organised consistently into an overview, key facts and fig-\nures, significant content, pertinent data, and a final chapter summary. All references are\nincluded at the end and each Figure, table, and piece of text is correctly referenced. The\nfive chapters that make up this thesis are arranged as follows:\n14\n‚Ä¢ Chapter 1: It provides a succinct overview of oral cancer, including its kinds, symp-\ntoms, and methods of diagnosis. It describes how the process of making medical\ndiagnoses has been transformed by machine learning, neural networks, and deep\nneural networks. Why has CNN surpassed conventional neural networks? what\nmotivated and inspired you to work in medicine. Additionally, it provides informa-\ntion about the goals and motivation.\n‚Ä¢ Chapter 2: It gives a brief overview of the literature for a number of researchers who\nworked on various methods for automatic oral cancer diagnosis, image processing,\nand texture-based categorization. Artificial neural networks, deep learning. A re-\nview of all pertinent theories and techniques for diagnosing oral cancer that are\navailable in the literature.\n‚Ä¢ Chapter 3: The chapter sheds insight on a crucial experiment study and the approach\nused to carry out our investigation. The models and various detection architectures\nemployed by CNN have been described. The proposed model is covered in this\nchapter; it has fewer parameters and a shallower learning curve than the pretrained\nmodel, but it is more accurate.\n‚Ä¢ Chapter 4: With the use of a graph, bar chart, and other presentation approaches,\nall model and performance metric results are shown. The model‚Äôs shortcomings are\nthen displayed and contrasted with the suggested model.\n‚Ä¢ Chapter 5: The entire work is concluded in the last chapter. This chapter also\ndiscusses how we might enhance our efforts in the future.","recorded":"2024-10-24 13:23:17.540417744","filePath":"null","pinned":false},{"value":"Cancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the body‚Äôs cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors\n1.1\n Oral Cancer\nThe mouth‚Äôs cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouth‚Äôs cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if lefuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouth‚Äôs cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.\n1.1.1\n Type of Oral Cancer\nThe following organs can develop cancer:\n‚Ä¢ Lips\n‚Ä¢ Tongue\n‚Ä¢ Inner lining of the cheek\n‚Ä¢ Gums\n‚Ä¢ Mouth Cancer\n‚Ä¢ Hard and Soft Palate\n1.2\n Mouth Cancer\nMouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n‚Ä¢ A mouth or lip sore that does not heal\n‚Ä¢ An internal mouth patch that is either white or red\n‚Ä¢ A growth or bulge inside your mouth; loose teeth;\n‚Ä¢ Painful or difficult swallowing\n2\n1.2.1\n Cause of Mouth Cancer\nWhen DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.\n1.3\n Human Mouth Structure\nThe start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks‚Äô inner surface (De Angeli et\nal. 2022). The majority of the mouth‚Äôs floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:\n‚Ä¢ The Lips\n‚Ä¢ The Tonsils and The Soft Palate\n‚Ä¢ The Uvula and the Tongue\n‚Ä¢ The buccal mucosa, which coats the cheekbones‚Äô interior\n‚Ä¢ The roof of the mouth\n‚Ä¢ Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.\n‚Ä¢ The Mandible (Lower Jawbone)\n3\nFigure 1.1: Structure of Human mouth\n(German and Palmer 2006)\n1.4\n Diagnosing Techniques for Oral Cancer\nThere are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n‚Ä¢ Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n‚Ä¢ Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n‚Ä¢ Computerized Tomography, or CT, Scanning ‚Äì Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n‚Ä¢ Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the disease‚Äôs spread.\n‚Ä¢ Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n‚Ä¢ Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the person‚Äôs particular type of cancer.\n1.5\n Oral cancer: Globally\nAmong the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000\n5\ndeaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncan‚Äôt obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. It‚Äôs because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in\nFigure 1.2: Global age standardized prevalence of tobacco smoking source World Health\nOrganization\n(Dai, Gakidou, and Lopez 2022)\n2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,\n6\nwhich corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.\n1.6\n Issues with Oral Cancer Manual Diagnoses\nThe primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.","recorded":"2024-10-24 13:23:05.035134500","filePath":"null","pinned":false},{"value":"Cancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the body‚Äôs cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors\n1.1\n Oral Cancer\nThe mouth‚Äôs cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouth‚Äôs cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if lefuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouth‚Äôs cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.\n1.1.1\n Type of Oral Cancer\nThe following organs can develop cancer:\n‚Ä¢ Lips\n‚Ä¢ Tongue\n‚Ä¢ Inner lining of the cheek\n‚Ä¢ Gums\n‚Ä¢ Mouth Cancer\n‚Ä¢ Hard and Soft Palate\n1.2\n Mouth Cancer\nMouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n‚Ä¢ A mouth or lip sore that does not heal\n‚Ä¢ An internal mouth patch that is either white or red\n‚Ä¢ A growth or bulge inside your mouth; loose teeth;\n‚Ä¢ Painful or difficult swallowing\n2\n1.2.1\n Cause of Mouth Cancer\nWhen DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.\n1.3\n Human Mouth Structure\nThe start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks‚Äô inner surface (De Angeli et\nal. 2022). The majority of the mouth‚Äôs floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:\n‚Ä¢ The Lips\n‚Ä¢ The Tonsils and The Soft Palate\n‚Ä¢ The Uvula and the Tongue\n‚Ä¢ The buccal mucosa, which coats the cheekbones‚Äô interior\n‚Ä¢ The roof of the mouth\n‚Ä¢ Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.\n‚Ä¢ The Mandible (Lower Jawbone)\n3\nFigure 1.1: Structure of Human mouth\n(German and Palmer 2006)\n1.4\n Diagnosing Techniques for Oral Cancer\nThere are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n‚Ä¢ Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n‚Ä¢ Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n‚Ä¢ Computerized Tomography, or CT, Scanning ‚Äì Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n‚Ä¢ Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the disease‚Äôs spread.\n‚Ä¢ Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n‚Ä¢ Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the person‚Äôs particular type of cancer.\n1.5\n Oral cancer: Globally\nAmong the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000\n5\ndeaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncan‚Äôt obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. It‚Äôs because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in\nFigure 1.2: Global age standardized prevalence of tobacco smoking source World Health\nOrganization\n(Dai, Gakidou, and Lopez 2022)\n2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,\n6\nwhich corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.\n1.6\n Issues with Oral Cancer Manual Diagnoses\nThe primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.\n1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.\n7\n1.7.2\n CNN\nConvolutional neural network (CNN) is a subtype of ANN. In at least one of their layers,\nCNN‚Äôs replace conventional matrix multiplication methods with the convolution mathe-\nmatical technique. Since they were developed specifically to handle pixel data, they are\nused in image recognition and processing. The design with which CNN is built is compa-\nrable with the model of neural connection like a person‚Äôs brain (Jeyaraj, B. K. Panigrahi,\nand Samuel Nadar 2022). Because of the way CNN is built, there are some strong prefer-\nences ingrained in them, which makes it easier to comprehend why they are so effective.\nCNN can be seen as a feed-forward network but having connection with each image can\nFigure 1.3: A CNN Architecture\n(Sun et al. 2019)\nbe inefficient. Therefore, we can prune the useless connection between the hidden layers\nto increase the performance of the layer. A CNN is a special artificial neural network with\nlimited connections between the layers of artificial neural network.\n‚Ä¢ Max-Pooling: Each feature map produced by processing the input through many\nlayers of convolution is subsequently combined in a pooling layer. Little grids are\nused for input for pooling procedures, which generate only one value for every re-\ngion. The pooling layers provide CNN significant translational consistency since a\n8\ntiny change in the input image causes a slight modification in the activation maps.\nApplying convolutions with longer strides is another method for obtaining the pool-\ning‚Äôs down sampling effect. The network design is made simpler by eliminating the\npooling levels without compromising performance. Max-pooling is the most widely\nemployed of all these pooling techniques.\n‚Ä¢ Fully-Connected Layers: Matrix multiplications have traditionally been the build-\ning blocks of neural networks, which are scattered with sigmoid nonlinearities. The\nlayers of the multiplication matrices are referred to as connected layers due to the\nconnection between each unit in the layer before and each unit in the layer af-\nter. There is just small-scale spatial connectivity when using convolutional layers.\nSignificant amounts of completely linked layers are typically avoided in modern\nnetworks since they require massive parameters.\n‚Ä¢ Learning algorithm: Lacking an algorithm to quickly and effectively learn the pa-\nrameters of the model, there is little value for an expensive model. Lacking a tech-\nnique for efficiently acquiring the model‚Äôs parameters, a strong, expressive model\nis of little use. In the pre-AlexNet era, greedy layer-wise pre-training techniques\nattempted to create such an efficient approach. A more straightforward supervised\ntraining approach is sufficient to learn a reliable model for tasks relating to com-\nputer vision.\n‚Ä¢ Optimization Based on Gradient: - Typically, the backpropagation technique is used\nto train networks, which accelerates mathematical calculation to calculate the gra-\ndient used in the Gradient Descent (GD) algorithm. However, employing GD is\nimpracticable for datasets with many hundreds or even more data points. In these\ncircumstances, Stochastic Gradient Descent (SGD), an approximation where gradi-\nents are computed for data points individually rather than the complete data set, is\nfrequently used. Training using SGD generalizes more successfully than with GD,\nit has been discovered.\n‚Ä¢ Batch Normalization:- A helpful regularizes that enhances generalisation and sharply\naccelerates convergence is batch normalisation (BN). The order of presentation of\nthe inputs to each layer varies continuously during the training phase, which is a\nproblem caused by inner covariate variation. This effect typically causes training\n9\nto take longer and requires careful initialization. This problem is addressed by BN,\nwhich normalises a layer‚Äôs production stimulation to ensure that its spectrum is\nconstrained to a restricted range. In particular, BN normalises each mini-batch‚Äôs\nmean-variance statistics using its running average. Recently, BN has been recog-\nnised as a crucial element of very deep networks.\n‚Ä¢ Activation layer :- Deep networks typically have convolutions after each layer,\nwhich then follows a nonlinear process. This is required because convolutions are\nan example of a cascading linear system. Layer-to-layer nonlinearities make the\nmodel more evocative than a model with linear dynamics. Theoretically, as long\nas nonlinearities are ongoing bounded, and gradually rising, no nonlinearity has a\ngreater capacity for expressiveness than any other. The sigmoid or the tanh were\nnonlinearities employed in classical neural networks that feed forward. However,\nthe Rectified Linear Unit (ReLU) is used in contemporary convolutional networks.\nIt has been discovered that CNNs with this nonlinearity train more quickly. The\nleaky- ReLU is a brand-new category of nonlinearity that has lately been intro-\nduced. Leaky-ReLU(x) = max(0, x) + min(0, x) is its formula, where is a preset\nparameter. It is better since it implies that the characteristic can also be taught,\ncreating a model that is considerably deeper. Leaky ReLUs or adjustable ReLUs\nare examples of variations on ReLU(z)=max (0; z). The feature maps, which are\nfrequently also referred to as feature maps, are fed through a process of activation\nto create new tensors.\n1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a less\n10\nfrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians‚Äô work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n‚Ä¢ Pre ‚ÄìProcessing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n‚Ä¢ Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n‚Ä¢ Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity\nFigure 1.4: Flow chart showing different Phases in detection of oral cancer\n1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, the\n12\nglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.\n1.9\n Motivation\nThe latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n‚Ä¢ Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n‚Ä¢ Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors\n1.10\n Problem Statement and Research Objective\nA large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n‚Ä¢ To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in it‚Äôs early stages.\n‚Ä¢ To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n‚Ä¢ To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.\n1.11\n Thesis outline\nThe chapters of the thesis are organised consistently into an overview, key facts and fig-\nures, significant content, pertinent data, and a final chapter summary. All references are\nincluded at the end and each Figure, table, and piece of text is correctly referenced. The\nfive chapters that make up this thesis are arranged as follows:\n14\n‚Ä¢ Chapter 1: It provides a succinct overview of oral cancer, including its kinds, symp-\ntoms, and methods of diagnosis. It describes how the process of making medical\ndiagnoses has been transformed by machine learning, neural networks, and deep\nneural networks. Why has CNN surpassed conventional neural networks? what\nmotivated and inspired you to work in medicine. Additionally, it provides informa-\ntion about the goals and motivation.\n‚Ä¢ Chapter 2: It gives a brief overview of the literature for a number of researchers who\nworked on various methods for automatic oral cancer diagnosis, image processing,\nand texture-based categorization. Artificial neural networks, deep learning. A re-\nview of all pertinent theories and techniques for diagnosing oral cancer that are\navailable in the literature.\n‚Ä¢ Chapter 3: The chapter sheds insight on a crucial experiment study and the approach\nused to carry out our investigation. The models and various detection architectures\nemployed by CNN have been described. The proposed model is covered in this\nchapter; it has fewer parameters and a shallower learning curve than the pretrained\nmodel, but it is more accurate.\n‚Ä¢ Chapter 4: With the use of a graph, bar chart, and other presentation approaches,\nall model and performance metric results are shown. The model‚Äôs shortcomings are\nthen displayed and contrasted with the suggested model.\n‚Ä¢ Chapter 5: The entire work is concluded in the last chapter. This chapter also\ndiscusses how we might enhance our efforts in the future.","recorded":"2024-10-24 13:19:00.175188261","filePath":"null","pinned":false},{"value":"chat.tinygrad.org","recorded":"2024-10-24 11:39:19.851389071","filePath":"null","pinned":false},{"value":"\"idle_inhibitor\": {\n        \"format\": \"{icon}\",\n        \"format-icons\": {\n            \"activated\": \"ÔÅÆ\",\n            \"deactivated\": \"ÔÅ∞\"\n        }\n    },","recorded":"2024-10-23 22:21:53.227543478","filePath":"null","pinned":false},{"value":"# Load the model\nsaved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_model.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\n\n# Since training history is saved to CSV, load it from there\nimport pandas as pd\ntraining_history = pd.read_csv(log_path)\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['Generator Loss'].tolist()\ndiscriminator_loss = training_history['Discriminator Loss'].tolist()","recorded":"2024-10-23 15:18:08.864613066","filePath":"null","pinned":false},{"value":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport matplotlib.pyplot as plt\nimport csv\nimport os\n\n# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save the entire GAN model\n    gan.save(save_path)\n\nepochs = 10000\nbatch_size = 64\n\ndef save_images(images, path='/workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\n# Define the log path\nlog_path = '/workspace/dataset/chest_xray/training_logs.csv'\n\n# Train the GAN with the log path\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, \n           '/workspace/dataset/chest_xray/lungs_generator_model.h5', log_path)\n","recorded":"2024-10-23 15:18:02.274697649","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray/lungs_generator_weights2.h5","recorded":"2024-10-23 15:17:41.112533238","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 2\n      1 saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\n----\u003e 2 model = tf.keras.models.load_model(saved_model_path)\n      4 training_history = model.history.history\n      6 # Get generator and discriminator losses\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)\n    254     return saving_lib.load_model(\n    255         filepath,\n    256         custom_objects=custom_objects,\n    257         compile=compile,\n    258         safe_mode=safe_mode,\n    259     )\n    261 # Legacy case.\n--\u003e 262 return legacy_sm_saving_lib.load_model(\n    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs\n    264 )\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.\u003clocals\u003e.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---\u003e 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/legacy/hdf5_format.py:197, in load_model_from_hdf5(filepath, custom_objects, compile)\n    195 model_config = f.attrs.get(\"model_config\")\n    196 if model_config is None:\n--\u003e 197     raise ValueError(\n    198         f\"No model config found in the file at {filepath}.\"\n    199     )\n    200 if hasattr(model_config, \"decode\"):\n    201     model_config = model_config.decode(\"utf-8\")\n\nValueError: No model config found in the file at \u003ctensorflow.python.platform.gfile.GFile object at 0x798a1043efd0\u003e.","recorded":"2024-10-23 15:16:50.971438848","filePath":"null","pinned":false},{"value":"saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\n\ntraining_history = model.history.history\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 15:16:45.900218321","filePath":"null","pinned":false},{"value":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport matplotlib.pyplot as plt\nimport csv\nimport os\n\n# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)\n\nepochs = 10000\nbatch_size = 64\n\ndef save_images(images, path='/workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\n# Define the log path\nlog_path = '/workspace/dataset/chest_xray/training_logs.csv'\n\n# Train the GAN with the log path\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, \n           '/workspace/dataset/chest_xray/lungs_generator_weights2.h5', log_path)\n","recorded":"2024-10-23 15:16:40.267587080","filePath":"null","pinned":false},{"value":"tf.keras.models.load_model(save_path)","recorded":"2024-10-23 15:15:38.242544530","filePath":"null","pinned":false},{"value":"# Example of getting losses after training\ntraining_history = {'generator_loss': vgan_losses['generator_loss'], 'discriminator_loss': vgan_losses['discriminator_loss']}\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 15:15:12.522462191","filePath":"null","pinned":false},{"value":"import tensorflow as tf\n\nsaved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_model.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\n\n# Accessing training history requires saving it separately, as it is not part of the model\n# Example: loading from a CSV file where you saved it during training\nimport pandas as pd\n\nlog_path = '/workspace/dataset/chest_xray/training_logs.csv'\ntraining_history = pd.read_csv(log_path)\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['Generator Loss'].tolist()\ndiscriminator_loss = training_history['Discriminator Loss'].tolist()\n","recorded":"2024-10-23 15:14:30.450980053","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[7], line 2\n      1 saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\n----\u003e 2 model = tf.keras.models.load_model(saved_model_path)\n      3 training_history = model.history.history\n      5 # Get generator and discriminator losses\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)\n    254     return saving_lib.load_model(\n    255         filepath,\n    256         custom_objects=custom_objects,\n    257         compile=compile,\n    258         safe_mode=safe_mode,\n    259     )\n    261 # Legacy case.\n--\u003e 262 return legacy_sm_saving_lib.load_model(\n    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs\n    264 )\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.\u003clocals\u003e.error_handler(*args, **kwargs)\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\n     68     # To get the full stack trace, call:\n     69     # `tf.debugging.disable_traceback_filtering()`\n---\u003e 70     raise e.with_traceback(filtered_tb) from None\n     71 finally:\n     72     del filtered_tb\n\nFile /usr/local/lib/python3.11/dist-packages/keras/src/saving/legacy/hdf5_format.py:197, in load_model_from_hdf5(filepath, custom_objects, compile)\n    195 model_config = f.attrs.get(\"model_config\")\n    196 if model_config is None:\n--\u003e 197     raise ValueError(\n    198         f\"No model config found in the file at {filepath}.\"\n    199     )\n    200 if hasattr(model_config, \"decode\"):\n    201     model_config = model_config.decode(\"utf-8\")\n\nValueError: No model config found in the file at \u003ctensorflow.python.platform.gfile.GFile object at 0x798a3057a410\u003e.","recorded":"2024-10-23 15:14:19.150224938","filePath":"null","pinned":false},{"value":"saved_model_path = \"/workspace/dataset/chest_xray/lungs_generator_weights2.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\ntraining_history = model.history.history\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 15:14:11.185804051","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/lalitha-j-a9672011b/","recorded":"2024-10-23 15:11:15.447686927","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/priyanka-sharma-p12/","recorded":"2024-10-23 15:10:33.373665656","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/prashar-snigdha/","recorded":"2024-10-23 15:09:46.341044396","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/shreya-padmanabhan/","recorded":"2024-10-23 15:09:02.085985621","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/varshar25/","recorded":"2024-10-23 15:08:16.094438114","filePath":"null","pinned":false},{"value":"https://www.linkedin.com/in/ishudayma/","recorded":"2024-10-23 15:06:52.278131146","filePath":"null","pinned":false},{"value":"ResMed","recorded":"2024-10-23 14:52:54.099645650","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray/generated_imagesImproved","recorded":"2024-10-23 13:29:48.152296524","filePath":"null","pinned":false},{"value":"2024-10-23 07:36:57.832679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43598 MB memory:  -\u003e device: 0, name: NVIDIA A40, pci bus id: 0000:ce:00.0, compute capability: 8.6\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[6], line 101\n     98         plt.savefig(f\"{path}generated_image_{i}.png\")\n     99         plt.close()\n--\u003e 101 train_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n\nTypeError: train_gan() missing 1 required positional argument: 'log_path'","recorded":"2024-10-23 13:09:47.746726859","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='/workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 13:09:37.435511593","filePath":"null","pinned":false},{"value":"def train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path, log_path):\n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n\n        for epoch in range(epochs):\n            for _ in range(batch_size):\n                idx = np.random.randint(0, real_data.shape[0], batch_size)\n                real_images = real_data[idx]\n\n                # Train the discriminator\n                noise = np.random.normal(0, 1, (batch_size, latent_dim))\n                fake_images = generator.predict(noise)\n                epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n                interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n                interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n                \n                with tf.GradientTape() as tape:\n                    tape.watch(interpolated_samples)\n                    pred_real = discriminator(real_images)\n                    pred_fake = discriminator(fake_images)\n                    gradient_penalty = gradient_penalty_loss(real_images, fake_images, discriminator)\n                    d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n                \n                grads = tape.gradient(d_loss, discriminator.trainable_weights)\n                discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n            # Train the generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {d_loss.numpy()}, Generator Loss: {g_loss}\")\n            log_writer.writerow([epoch, d_loss.numpy(), g_loss])\n            wgan_losses['discriminator_loss'].append(d_loss.numpy())\n            wgan_losses['generator_loss'].append(g_loss)\n\n            if epoch % 20 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:03:49.429649034","filePath":"null","pinned":false},{"value":"def train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images,discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss)\n        wgan_losses['generator_loss'].append(g_loss)\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"/workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:03:28.181245694","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray","recorded":"2024-10-23 13:02:21.098520401","filePath":"null","pinned":false},{"value":"#WGAN model\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Define the generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n\n# Define the gradient penalty\ndef gradient_penalty_loss(real_images, fake_images, discriminator):\n    batch_size = tf.shape(real_images)[0]\n\n    # Randomly interpolate between real and fake samples\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n\n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        pred_interpolated = discriminator(interpolated)\n\n    gradients = tape.gradient(pred_interpolated, interpolated)\n    gradients_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1.0))\n\n    return gradient_penalty\n\n# Define the WGAN-GP model\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)\n\n# Define parameters\nlatent_dim = 100\n\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss='mse')\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the WGAN-GP\nwgan_gp = build_wgan_gp(generator, discriminator, latent_dim)\n\n# Compile the WGAN-GP\nwgan_gp.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss=lambda y_true, y_pred: -y_pred)\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the WGAN-GP\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images,discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss)\n        wgan_losses['generator_loss'].append(g_loss)\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"/content/drive/My Drive/Lung dataset/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n\n# Train the WGAN-GP\nepochs = 1000\nbatch_size =64\n\ndef save_images(images, path='/content/drive/My Drive/Lung dataset/generated_imagesWGAN1'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\ntrain_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, '/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights_wgan_gp1.h5')\n","recorded":"2024-10-23 13:02:12.594784467","filePath":"null","pinned":false},{"value":"def train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path, log_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Open the log file\n    with open(log_path, mode='w', newline='') as log_file:\n        log_writer = csv.writer(log_file)\n        # Write the header\n        log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n        \n        for epoch in range(epochs):\n            # Train Discriminator\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n            \n            # Train Generator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            generator_loss = gan.train_on_batch(noise, real_labels)\n            \n            # Log losses\n            print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n            log_writer.writerow([epoch, discriminator_loss[0], generator_loss])\n            vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n            vgan_losses['generator_loss'].append(generator_loss)\n\n            if epoch % 1000 == 0:\n                save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), \n                             path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n\n    # Save generator weights\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:01:17.831338061","filePath":"null","pinned":false},{"value":"learning_rate","recorded":"2024-10-23 13:00:54.075018222","filePath":"null","pinned":false},{"value":"def train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"/workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)","recorded":"2024-10-23 13:00:27.519605253","filePath":"null","pinned":false},{"value":"# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n\n        # Discriminator training\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        \n        # If the outputs are not tuples, just assign them directly\n        if isinstance(discriminator_loss_real, tuple):\n            discriminator_loss_real_value = discriminator_loss_real[0]\n            discriminator_accuracy_real = discriminator_loss_real[1]\n        else:\n            discriminator_loss_real_value = discriminator_loss_real\n            discriminator_accuracy_real = 0  # or handle it as needed\n\n        if isinstance(discriminator_loss_fake, tuple):\n            discriminator_loss_fake_value = discriminator_loss_fake[0]\n            discriminator_accuracy_fake = discriminator_loss_fake[1]\n        else:\n            discriminator_loss_fake_value = discriminator_loss_fake\n            discriminator_accuracy_fake = 0  # or handle it as needed\n\n        # Combine the losses and accuracies\n        discriminator_loss = 0.5 * (discriminator_loss_real_value + discriminator_loss_fake_value)\n        discriminator_accuracy = 0.5 * (discriminator_accuracy_real + discriminator_accuracy_fake)\n\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n\n        # Log the results\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss}, Discriminator Accuracy: {discriminator_accuracy}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss)\n        vgan_losses['generator_loss'].append(generator_loss)\n        vgan_losses['discriminator_accuracy'].append(discriminator_accuracy)\n\n        # Log to CSV\n        with open(log_file_path, mode='a', newline='') as log_file:\n            log_writer = csv.writer(log_file)\n            log_writer.writerow([epoch, discriminator_loss, discriminator_accuracy, generator_loss, 'N/A'])  # Generator accuracy can be complex to compute.\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    \n    generator.save_weights(save_path)\n","recorded":"2024-10-23 12:56:15.792818230","filePath":"null","pinned":false},{"value":"2024-10-23 07:14:14.261599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43598 MB memory:  -\u003e device: 0, name: NVIDIA A40, pci bus id: 0000:ce:00.0, compute capability: 8.6\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n2024-10-23 07:14:15.319684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n2024-10-23 07:14:15.411235: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n2/2 [==============================] - 1s 4ms/step\n\n2024-10-23 07:14:15.714699: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n2024-10-23 07:14:16.327901: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n2024-10-23 07:14:17.508732: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f808c1af080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2024-10-23 07:14:17.508758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n2024-10-23 07:14:17.514387: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-10-23 07:14:17.626066: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[6], line 118\n    115 batch_size = 64\n    117 # Start training the GAN\n--\u003e 118 train_gan(generator, discriminator, gan, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n\nCell In[6], line 89, in train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path)\n     86 generator_loss = gan.train_on_batch(noise, real_labels)\n     88 # Log the results\n---\u003e 89 print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Discriminator Accuracy: {discriminator_accuracy}, Generator Loss: {generator_loss}\")\n     90 vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n     91 vgan_losses['generator_loss'].append(generator_loss)\n\nIndexError: invalid index to scalar variable.","recorded":"2024-10-23 12:55:56.909570122","filePath":"null","pinned":false},{"value":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, optimizers\n\n# Build the Generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\n# Hyperparameters\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': [], 'discriminator_accuracy': [], 'generator_accuracy': []}\n\n# Save logs to a CSV file\nimport csv\n\nlog_file_path = 'gan_training_logs.csv'\nwith open(log_file_path, mode='w', newline='') as log_file:\n    log_writer = csv.writer(log_file)\n    log_writer.writerow(['Epoch', 'Discriminator Loss', 'Discriminator Accuracy', 'Generator Loss', 'Generator Accuracy'])\n\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n\n        # Discriminator training\n        discriminator_loss_real, discriminator_accuracy_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake, discriminator_accuracy_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        \n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        discriminator_accuracy = 0.5 * np.add(discriminator_accuracy_real, discriminator_accuracy_fake)\n\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n\n        # Log the results\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Discriminator Accuracy: {discriminator_accuracy}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        vgan_losses['discriminator_accuracy'].append(discriminator_accuracy)\n\n        # Log to CSV\n        with open(log_file_path, mode='a', newline='') as log_file:\n            log_writer = csv.writer(log_file)\n            log_writer.writerow([epoch, discriminator_loss[0], discriminator_accuracy, generator_loss, 'N/A'])  # Generator accuracy can be complex to compute.\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    \n    generator.save_weights(save_path)\n\n# Image saving function\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\n# Training parameters\nepochs = 10000\nbatch_size = 64\n\n# Start training the GAN\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 12:55:47.991131234","filePath":"null","pinned":false},{"value":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport csv\n\n# Define the generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n\n# Define the gradient penalty\ndef gradient_penalty_loss(real_images, fake_images, discriminator):\n    batch_size = tf.shape(real_images)[0]\n\n    # Randomly interpolate between real and fake samples\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n\n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        pred_interpolated = discriminator(interpolated)\n\n    gradients = tape.gradient(pred_interpolated, interpolated)\n    gradients_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1.0))\n\n    return gradient_penalty\n\n# Define the WGAN-GP model\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)\n\n# Define parameters\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss='mse')\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the WGAN-GP\nwgan_gp = build_wgan_gp(generator, discriminator, latent_dim)\n\n# Compile the WGAN-GP\nwgan_gp.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss=lambda y_true, y_pred: -y_pred)\n\n# Initialize logs\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\n# Prepare log file\nlog_file_path = 'wgan_training_logs.csv'\nwith open(log_file_path, mode='w', newline='') as log_file:\n    log_writer = csv.writer(log_file)\n    log_writer.writerow(['Epoch', 'Discriminator Loss', 'Generator Loss'])\n\n# Train the WGAN-GP\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images, discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            \n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        # Log the results\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss.numpy()}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss.numpy())\n        wgan_losses['generator_loss'].append(g_loss)\n\n        # Log to CSV\n        with open(log_file_path, mode='a', newline='') as log_file:\n            log_writer = csv.writer(log_file)\n            log_writer.writerow([epoch, d_loss.numpy(), g_loss])\n\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n\n# Image saving function\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesWGAN1'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}/generated_image_{i}.png\")\n        plt.close()\n\n# Train the WGAN-GP\nepochs = 1000\nbatch_size = 64\ntrain_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights_wgan_gp1.h5')\n","recorded":"2024-10-23 12:41:45.350052454","filePath":"null","pinned":false},{"value":"#WGAN model\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Define the generator\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n\n# Define the gradient penalty\ndef gradient_penalty_loss(real_images, fake_images, discriminator):\n    batch_size = tf.shape(real_images)[0]\n\n    # Randomly interpolate between real and fake samples\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n\n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        pred_interpolated = discriminator(interpolated)\n\n    gradients = tape.gradient(pred_interpolated, interpolated)\n    gradients_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n    gradient_penalty = tf.reduce_mean(tf.square(gradients_l2_norm - 1.0))\n\n    return gradient_penalty\n\n# Define the WGAN-GP model\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)\n\n# Define parameters\nlatent_dim = 100\n\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss='mse')\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the WGAN-GP\nwgan_gp = build_wgan_gp(generator, discriminator, latent_dim)\n\n# Compile the WGAN-GP\nwgan_gp.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss=lambda y_true, y_pred: -y_pred)\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the WGAN-GP\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            # Train the discriminator\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n            interpolated_samples = tf.Variable(interpolated_samples, dtype=tf.float32)\n            with tf.GradientTape() as tape:\n                tape.watch(interpolated_samples)\n                pred_real = discriminator(real_images)\n                pred_fake = discriminator(fake_images)\n                gradient_penalty = gradient_penalty_loss(real_images, fake_images,discriminator)\n                d_loss = tf.reduce_mean(pred_fake) - tf.reduce_mean(pred_real) + 10 * gradient_penalty\n            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n\n        # Train the generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = wgan_gp.train_on_batch(noise, -np.ones((batch_size, 1)))\n\n        print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n        wgan_losses['discriminator_loss'].append(d_loss)\n        wgan_losses['generator_loss'].append(g_loss)\n        if epoch % 20 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesWGAN1/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n\n# Train the WGAN-GP\nepochs = 1000\nbatch_size =64\n\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesWGAN1'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\ntrain_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, 'workspace/dataset/chest_xray/lungs_generator_weights_wgan_gp1.h5')\n","recorded":"2024-10-23 12:41:13.559475795","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"workspace/dataset/chest_xray/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='workspace/dataset/chest_xray/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'workspace/dataset/chest_xray/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 12:38:30.013030425","filePath":"null","pinned":false},{"value":"# Function to load and preprocess images with labels\ndef load_images_with_labels(folder, img_width, img_height, label):\n    images = []\n    labels = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height))\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n        labels.append(label)\n    return np.array(images), np.array(labels)\n\n# Load and preprocess images with labels\nnormal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=\"Normal\")\npneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=\"Pneumonia\")\n\n\nreal_data = np.concatenate((normal_images, pneumonia_images), axis=0)\nreal_labels = np.concatenate((normal_labels, pneumonia_labels), axis=0)\n\n\nshuffled_indices = np.random.permutation(len(real_data))\nreal_data = real_data[shuffled_indices]\nreal_labels = real_labels[shuffled_indices]\n\n\ndef display_images_with_labels(images, labels, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][:, :, 0], cmap='gray')\n        axes[i].set_title(labels[i])\n        axes[i].axis('off')\n    plt.show()\n\n\nnum_samples_to_display = 5\ndisplay_images_with_labels(real_data, real_labels, num_samples_to_display)\n","recorded":"2024-10-23 12:33:19.936614744","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray/chest_xray/train/NORMAL/.ipynb_checkpoints","recorded":"2024-10-23 12:31:40.528911593","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to load and preprocess images with labels\ndef load_images_with_labels(folder, img_width, img_height, label):\n    images = []\n    labels = []\n    for filename in os.listdir(folder):\n        img_path = os.path.join(folder, filename)\n        img = cv2.imread(img_path)\n        \n        # Check if the image was loaded correctly\n        if img is None:\n            print(f\"Warning: Failed to load image: {img_path}\")\n            continue\n        \n        # Convert to grayscale\n        try:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        except cv2.error as e:\n            print(f\"Error converting image {filename} to grayscale: {e}\")\n            continue\n        \n        # Resize the image\n        img = cv2.resize(img, (img_width, img_height))\n        \n        # Normalize and expand dimensions for compatibility with models\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        \n        images.append(img)\n        labels.append(label)\n    \n    return np.array(images), np.array(labels)\n\n# Load and preprocess images with labels\nnormal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=\"Normal\")\npneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=\"Pneumonia\")\n\n# Concatenate data and labels\nreal_data = np.concatenate((normal_images, pneumonia_images), axis=0)\nreal_labels = np.concatenate((normal_labels, pneumonia_labels), axis=0)\n\n# Shuffle data\nshuffled_indices = np.random.permutation(len(real_data))\nreal_data = real_data[shuffled_indices]\nreal_labels = real_labels[shuffled_indices]\n\n# Function to display images with labels\ndef display_images_with_labels(images, labels, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][:, :, 0], cmap='gray')\n        axes[i].set_title(labels[i])\n        axes[i].axis('off')\n    plt.show()\n\n# Display a sample of images\nnum_samples_to_display = 5\ndisplay_images_with_labels(real_data, real_labels, num_samples_to_display)\n","recorded":"2024-10-23 12:31:25.987231233","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nerror                                     Traceback (most recent call last)\nCell In[9], line 16\n     13     return np.array(images), np.array(labels)\n     15 # Load and preprocess images with labels\n---\u003e 16 normal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=\"Normal\")\n     17 pneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=\"Pneumonia\")\n     20 real_data = np.concatenate((normal_images, pneumonia_images), axis=0)\n\nCell In[9], line 7, in load_images_with_labels(folder, img_width, img_height, label)\n      5 for filename in os.listdir(folder):\n      6     img = cv2.imread(os.path.join(folder, filename))\n----\u003e 7     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n      8     img = cv2.resize(img, (img_width, img_height))\n      9     img = img.astype('float32') / 255.0\n\nerror: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'","recorded":"2024-10-23 12:31:12.702278326","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray/chest_xray/train/PNEUMONIA/.DS_Store","recorded":"2024-10-23 12:30:24.781001714","filePath":"null","pinned":false},{"value":"/workspace/dataset/chest_xray/chest_xray/train/NORMAL/.DS_Store","recorded":"2024-10-23 12:30:13.285060041","filePath":"null","pinned":false},{"value":"def load_images(folder, img_width, img_height):\n    images = []\n    for filename in os.listdir(folder):\n        img_path = os.path.join(folder, filename)\n        img = cv2.imread(img_path)\n        \n        # Check if the image was loaded correctly\n        if img is None:\n            print(f\"Warning: Failed to load image: {img_path}\")\n            continue\n        \n        # Convert to grayscale\n        try:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        except cv2.error as e:\n            print(f\"Error converting image {filename} to grayscale: {e}\")\n            continue\n        \n        # Resize the image\n        img = cv2.resize(img, (img_width, img_height))\n        \n        # Normalize and expand dimensions for compatibility with models\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        \n        images.append(img)\n    \n    return np.array(images)\n\n","recorded":"2024-10-23 12:29:00.844706758","filePath":"null","pinned":false},{"value":"def load_images(folder, img_width, img_height):\n    images = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height)) \n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n    return np.array(images)","recorded":"2024-10-23 12:28:34.471436032","filePath":"null","pinned":false},{"value":"for filename in os.listdir(folder):\n    img_path = os.path.join(folder, filename)\n    img = cv2.imread(img_path)\n    if img is None:\n        print(f\"Failed to load image: {img_path}\")\n        continue\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.resize(img, (img_width, img_height)) \n    img = img.astype('float32') / 255.0","recorded":"2024-10-23 12:28:24.829231202","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nerror                                     Traceback (most recent call last)\nCell In[4], line 7\n      5 get_ipython().system('ls \"/workspace/dataset/chest_xray\"')\n      6 # Load and preprocess images from the dataset\n----\u003e 7 normal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\n      8 pneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)\n\nCell In[2], line 5, in load_images(folder, img_width, img_height)\n      3 for filename in os.listdir(folder):\n      4     img = cv2.imread(os.path.join(folder, filename))\n----\u003e 5     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n      6     img = cv2.resize(img, (img_width, img_height)) \n      7     img = img.astype('float32') / 255.0\n\nerror: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'","recorded":"2024-10-23 12:27:30.245014925","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray/chest_xray/test","recorded":"2024-10-23 12:24:43.323418224","filePath":"null","pinned":false},{"value":"workspace/dataset/chest_xray","recorded":"2024-10-23 12:18:33.385037471","filePath":"null","pinned":false},{"value":"import os\n\n# List files in the directory to check if path is correct\nprint(os.listdir(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\")))","recorded":"2024-10-23 12:17:01.129420987","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\nerror                                     Traceback (most recent call last)\nCell In[17], line 2\n      1 # Load the dataset\n----\u003e 2 normal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=0)\n      3 pneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=1)\n      5 real_data = np.concatenate((normal_images, pneumonia_images), axis=0)\n\nCell In[15], line 18, in load_images_with_labels(folder, img_width, img_height, label)\n     16 for filename in os.listdir(folder):\n     17     img = cv2.imread(os.path.join(folder, filename))\n---\u003e 18     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n     19     img = cv2.resize(img, (img_width, img_height))\n     20     img = img.astype('float32') / 255.0\n\nerror: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'","recorded":"2024-10-23 12:16:40.518388042","filePath":"null","pinned":false},{"value":"chest_xray","recorded":"2024-10-23 12:15:43.877473448","filePath":"null","pinned":false},{"value":"workspace/kaggle.json","recorded":"2024-10-23 12:09:53.294822331","filePath":"null","pinned":false},{"value":"/workspace","recorded":"2024-10-23 12:09:17.107460049","filePath":"null","pinned":false},{"value":"/home/karna/dotfiles/kaggle.json.cpt","recorded":"2024-10-23 12:08:01.502693641","filePath":"null","pinned":false},{"value":"!pip install kaggle","recorded":"2024-10-23 12:06:24.354750017","filePath":"null","pinned":false},{"value":"dataset","recorded":"2024-10-23 12:05:27.011671097","filePath":"null","pinned":false},{"value":"dataset_dir = \"/content/drive/MyDrive/dataset/chest_xray\"","recorded":"2024-10-23 12:04:49.681158398","filePath":"null","pinned":false},{"value":"!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia -p /content/drive/MyDrive/dataset --unzip\n","recorded":"2024-10-23 12:04:07.764340705","filePath":"null","pinned":false},{"value":"!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n","recorded":"2024-10-23 12:03:44.255108814","filePath":"null","pinned":false},{"value":"!pip install kaggle\n","recorded":"2024-10-23 12:03:31.625319405","filePath":"null","pinned":false},{"value":"# Plot and save WGAN loss\nplot_loss(wgan_losses)","recorded":"2024-10-23 12:00:03.239619535","filePath":"null","pinned":false},{"value":"train_wgan_gp(generator_wgan, discriminator_wgan, wgan_gp, real_data, epochs, batch_size, save_path_wgan)","recorded":"2024-10-23 11:59:58.732493675","filePath":"null","pinned":false},{"value":"# Train WGAN-GP\ngenerator_wgan = build_generator(latent_dim)\ndiscriminator_wgan = build_discriminator((img_width, img_height, channels))\n\ndiscriminator_wgan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\nwgan_gp = build_wgan_gp(generator_wgan, discriminator_wgan, latent_dim)\n\nwgan_gp.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0001, 0.5))","recorded":"2024-10-23 11:59:53.988241875","filePath":"null","pinned":false},{"value":"# WGAN Training Function\nwgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\ndef train_wgan_gp(generator, discriminator, wgan_gp, real_data, epochs, batch_size, save_path):\n    for epoch in range(epochs):\n        for _ in range(batch_size):\n            idx = np.random.randint(0, real_data.shape[0], batch_size)\n            real_images = real_data[idx]\n\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            fake_images = generator.predict(noise)\n\n            epsilon = np.random.uniform(0, 1, (batch_size, 1, 1, 1))\n            interpolated_samples = epsilon * real_images + (1 - epsilon) * fake_images\n\n            d_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n            d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))\n\n            # Generator Training\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            g_loss = wgan_gp.train_on_batch(noise, np.ones((batch_size, 1)))\n\n            wgan_losses['discriminator_loss'].append(d_loss_real + d_loss_fake)\n            wgan_losses['generator_loss'].append(g_loss)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss_real}, G Loss: {g_loss}\")\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), f\"/content/drive/My Drive/Lung dataset/generated_imagesWGAN/epoch_{epoch}\")\n\n    generator.save_weights(save_path)\n","recorded":"2024-10-23 11:59:45.288796012","filePath":"null","pinned":false},{"value":"# WGAN-GP Model Definition\ndef build_wgan_gp(generator, discriminator, latent_dim):\n    z = layers.Input(shape=(latent_dim,))\n    img = generator(z)\n    valid = discriminator(img)\n    return models.Model(z, valid)","recorded":"2024-10-23 11:59:39.779711983","filePath":"null","pinned":false},{"value":"# Save and plot loss\ndef plot_loss(vgan_losses, save_dir='/content/drive/My Drive/Lung dataset/loss_plots'):\n    os.makedirs(save_dir, exist_ok=True)\n    plt.figure(figsize=(10, 5))\n    plt.plot(vgan_losses['discriminator_loss'], label='Discriminator Loss', color='blue')\n    plt.plot(vgan_losses['generator_loss'], label='Generator Loss', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Vanilla GAN Training Losses')\n    plt.legend()\n    plt.savefig(os.path.join(save_dir, 'gan_loss.png'))\n    plt.show()\n\nplot_loss(vgan_losses)","recorded":"2024-10-23 11:59:33.580519643","filePath":"null","pinned":false},{"value":"train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path_gan)","recorded":"2024-10-23 11:59:27.850346119","filePath":"null","pinned":false},{"value":"# Train Vanilla GAN\ngenerator = build_generator(latent_dim)\ndiscriminator = build_discriminator((img_width, img_height, channels))\n\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.0001, 0.5))\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path_gan)","recorded":"2024-10-23 11:59:22.650139943","filePath":"null","pinned":false},{"value":"# GAN Training Function\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size, save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n\n        # Discriminator Training\n        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Generator Training\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        g_loss = gan.train_on_batch(noise, real_labels)\n\n        vgan_losses['discriminator_loss'].append(d_loss[0])\n        vgan_losses['generator_loss'].append(g_loss)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n\n        if epoch % 1000 == 0:\n            save_images(generator.predict(np.random.normal(0, 1, (25, latent_dim))), f\"/content/drive/My Drive/Lung dataset/generated_imagesImproved/epoch_{epoch}\")\n\n    generator.save_weights(save_path)","recorded":"2024-10-23 11:59:12.614530150","filePath":"null","pinned":false},{"value":"def save_metrics(generator, real_labels, predictions, metrics_dir):\n    os.makedirs(metrics_dir, exist_ok=True)\n    \n    accuracy = accuracy_score(real_labels, predictions)\n    precision = precision_score(real_labels, predictions, average='binary')\n    recall = recall_score(real_labels, predictions, average='binary')\n    f1 = f1_score(real_labels, predictions, average='binary')\n    \n    with open(os.path.join(metrics_dir, 'metrics.txt'), 'w') as f:\n        f.write(f\"Accuracy: {accuracy}\\n\")\n        f.write(f\"Precision: {precision}\\n\")\n        f.write(f\"Recall: {recall}\\n\")\n        f.write(f\"F1-Score: {f1}\\n\")\n\n# Vanilla GAN Model\ndef build_generator(latent_dim):\n    model = models.Sequential([\n        layers.Dense(128 * 16 * 16, input_dim=latent_dim),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Reshape((16, 16, 128)),\n        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same')\n    ])\n    return model\n\ndef build_discriminator(input_shape):\n    model = models.Sequential([\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.4),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.4),\n        layers.Flatten(),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    return model","recorded":"2024-10-23 11:58:59.771958073","filePath":"null","pinned":false},{"value":"# Display sample images\nnum_samples_to_display = 5\ndisplay_images_with_labels(real_data, real_labels, num_samples_to_display)","recorded":"2024-10-23 11:58:54.043670951","filePath":"null","pinned":false},{"value":"# Load the dataset\nnormal_images, normal_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height, label=0)\npneumonia_images, pneumonia_labels = load_images_with_labels(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height, label=1)\n\nreal_data = np.concatenate((normal_images, pneumonia_images), axis=0)\nreal_labels = np.concatenate((normal_labels, pneumonia_labels), axis=0)\n\n# Shuffle dataset\nshuffled_indices = np.random.permutation(len(real_data))\nreal_data, real_labels = real_data[shuffled_indices], real_labels[shuffled_indices]\n\n# Display images and labels\ndef display_images_with_labels(images, labels, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][:, :, 0], cmap='gray')\n        axes[i].set_title(f\"Label: {labels[i]}\")\n        axes[i].axis('off')\n    plt.show()\n","recorded":"2024-10-23 11:58:47.203338314","filePath":"null","pinned":false},{"value":"# Parameters\nimg_width, img_height, channels = 64, 64, 1\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\nlatent_dim = 100\nepochs = 10000\nbatch_size = 64\nsave_path_gan = \"/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.h5\"\nsave_path_wgan = \"/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights_wgan_gp.h5\"\n","recorded":"2024-10-23 11:58:41.217742266","filePath":"null","pinned":false},{"value":"# Load Images\ndef load_images(folder, img_width, img_height):\n    images = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height)) \n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n    return np.array(images)\n\n# Load and preprocess images with labels\ndef load_images_with_labels(folder, img_width, img_height, label):\n    images, labels = [], []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (img_width, img_height))\n        img = img.astype('float32') / 255.0\n        img = np.expand_dims(img, axis=-1)\n        images.append(img)\n        labels.append(label)\n    return np.array(images), np.array(labels)","recorded":"2024-10-23 11:58:25.115630389","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models, optimizers\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","recorded":"2024-10-23 11:57:47.445430535","filePath":"null","pinned":false},{"value":"latent_dim=100\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\ndef display_generated_images(generator, z_dim, num_images, title):\n    random_latent_vectors = np.random.normal(0, 1, (num_images, z_dim))\n    generated_images = generator.predict(random_latent_vectors)\n\n    # Rescale images to the [0,1] range\n    generated_images = 0.5 * generated_images + 0.5\n\n    plt.figure(figsize=(10, 10))\n    for i in range(num_images):\n        plt.subplot(10, 10, i+1)\n        plt.imshow(generated_images[i, :, :, :])\n        plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()\n\n\ngenerator_lungs = build_generator(latent_dim)\ngenerator_lungs.load_weights('/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights_wgan_gp.h5')\nprint(generator_lungs.summary())\n# Generate and display cat images\ndisplay_generated_images(generator_lungs, latent_dim, 100, 'Generated Lung Images')","recorded":"2024-10-23 11:53:05.730696317","filePath":"null","pinned":false},{"value":"# for epoch in range(epochs):\n\nplt.figure(figsize=(10, 5))\n\n    # Plot Vanilla GAN losses\nplt.plot(vgan_losses['discriminator_loss'], label='Vanilla GAN - Discriminator Loss', color='blue')\nplt.plot(vgan_losses['generator_loss'], label='Vanilla GAN - Generator Loss', color='orange')\n\n\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('GAN Training Losses')\nplt.legend()\nplt.show()","recorded":"2024-10-23 11:52:57.707153821","filePath":"null","pinned":false},{"value":"#GAN model\ndef build_generatorSimple(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\ndef display_generated_images(generator, z_dim, num_images, title):\n    random_latent_vectors = np.random.normal(0, 1, (num_images, z_dim))\n    generated_images = generator.predict(random_latent_vectors)\n\n    # Rescale images to the [0,1] range\n    generated_images = 0.5 * generated_images + 0.5\n\n    plt.figure(figsize=(10, 10))\n    for i in range(num_images):\n        plt.subplot(10, 10, i+1)\n        plt.imshow(generated_images[i, :, :, :])\n        plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()\n\n\ngenerator_lungs = build_generatorSimple(latent_dim)\ngenerator_lungs.load_weights('/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.h5')\nprint(generator_lungs.summary())\n# Generate and display cat images\ndisplay_generated_images(generator_lungs, latent_dim, 100, 'Generated Lung Images')","recorded":"2024-10-23 11:52:52.648158548","filePath":"null","pinned":false},{"value":"saved_model_path = \"/content/drive/MyDrive/Lung dataset/archive/lungs_generator_weights2.h5\"\nmodel = tf.keras.models.load_model(saved_model_path)\ntraining_history = model.history.history\n\n# Get generator and discriminator losses\ngenerator_loss = training_history['generator_loss']\ndiscriminator_loss = training_history['discriminator_loss']","recorded":"2024-10-23 11:52:48.442264990","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"/content/drive/My Drive/Lung dataset/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='/content/drive/My Drive/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights2.h5')\n","recorded":"2024-10-23 11:52:42.244270266","filePath":"null","pinned":false},{"value":"img_width, img_height = 64, 64\nchannels = 1  # Grayscale\n\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\n!ls \"/connt/drive/My Drive/Lung dataset/archizve\"\n# Load and preprocess images from the dataset\nnormal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\npneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)","recorded":"2024-10-23 11:52:33.807025394","filePath":"null","pinned":false},{"value":"from google.colab import drive\ndrive.mount('/content/drive')","recorded":"2024-10-23 11:52:30.772763401","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models, optimizers","recorded":"2024-10-23 11:52:18.279024124","filePath":"null","pinned":false},{"value":"https://www.ieltsadvantage.com/2015/03/10/ielts-writing-task-1-grammar-guide/","recorded":"2024-10-23 11:29:34.613496751","filePath":"null","pinned":false},{"value":"/mnt/Karna/aco-main\n/mnt/Karna/CP\n/mnt/Karna/Git\n/mnt/Karna/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main\n/mnt/Karna/Pratik Project GAN Generate lung images\n/mnt/Karna/Reinforcement-learning-approach-for-prognosis-in-ICU\n/mnt/Karna/aco-main.zip\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-23 10:36:38.011631544","filePath":"null","pinned":false},{"value":"/10.1016/j.jcp.2018.10.045","recorded":"2024-10-21 16:33:00.580348637","filePath":"null","pinned":false},{"value":"Scopus author profile","recorded":"2024-10-21 16:24:32.801406379","filePath":"null","pinned":false},{"value":"import torch\nimport torch.nn as nn\n\n# Define the generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        # Fully connected layer to project latent vector into a 2D shape\n        self.fc = nn.Linear(latent_dim, 128 * 8 * 8)  # Modify this based on your target shape\n        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n        \n        # Transpose convolutions to upscale the image\n        self.conv_trans1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # Output: (batch_size, 64, 16, 16)\n        self.conv_trans2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)   # Output: (batch_size, 32, 32, 32)\n        self.conv_trans3 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)   # Output: (batch_size, 16, 64, 64)\n        \n        # Final output layer (e.g., grayscale image)\n        self.conv_out = nn.Conv2d(16, 1, kernel_size=7, padding=3)  # Output: (batch_size, 1, 64, 64)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        x = self.fc(z)  # Project the latent vector to a 2D shape\n        x = self.lrelu(x)\n        x = x.view(-1, 128, 8, 8)  # Reshape to (batch_size, 128, 8, 8) for ConvTranspose layers\n        \n        x = self.lrelu(self.conv_trans1(x))  # (batch_size, 64, 16, 16)\n        x = self.lrelu(self.conv_trans2(x))  # (batch_size, 32, 32, 32)\n        x = self.lrelu(self.conv_trans3(x))  # (batch_size, 16, 64, 64)\n        x = self.sigmoid(self.conv_out(x))   # (batch_size, 1, 64, 64)\n        \n        return x","recorded":"2024-10-21 15:36:01.577885091","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\n\nRuntimeError                              Traceback (most recent call last)\n\n\u003cipython-input-18-565860103019\u003e in \u003ccell line: 1\u003e()\n----\u003e 1 train_gan(generator, discriminator, dataloader, epochs, latent_dim, '/content/drive/My Drive/Lung dataset/generated_images')\n\n6 frames\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    115 \n    116     def forward(self, input: Tensor) -\u003e Tensor:\n--\u003e 117         return F.linear(input, self.weight, self.bias)\n    118 \n    119     def extra_repr(self) -\u003e str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x32768)","recorded":"2024-10-21 15:35:21.146513092","filePath":"null","pinned":false},{"value":"import torch\nimport torch.nn as nn\n\n# Define the generator\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        self.fc = nn.Linear(latent_dim, 128 * 16 * 16)\n        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n        self.conv_trans1 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv_trans2 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv_out = nn.Conv2d(128, 1, kernel_size=7, padding=3)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        x = self.fc(z)\n        x = self.lrelu(x)\n        x = x.view(-1, 128, 16, 16)  # Reshape to (batch_size, 128, 16, 16)\n        x = self.lrelu(self.conv_trans1(x))\n        x = self.lrelu(self.conv_trans2(x))\n        x = self.sigmoid(self.conv_out(x))\n        return x\n\nlatent_dim = 100\ngenerator = Generator(latent_dim)","recorded":"2024-10-21 15:34:05.212110566","filePath":"null","pinned":false},{"value":"---------------------------------------------------------------------------\n\nRuntimeError                              Traceback (most recent call last)\n\n\u003cipython-input-13-565860103019\u003e in \u003ccell line: 1\u003e()\n----\u003e 1 train_gan(generator, discriminator, dataloader, epochs, latent_dim, '/content/drive/My Drive/Lung dataset/generated_images')\n\n9 frames\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    115 \n    116     def forward(self, input: Tensor) -\u003e Tensor:\n--\u003e 117         return F.linear(input, self.weight, self.bias)\n    118 \n    119     def extra_repr(self) -\u003e str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (6400x1 and 100x32768)","recorded":"2024-10-21 15:33:29.572857291","filePath":"null","pinned":false},{"value":"# Save model weights\ntorch.save(generator.state_dict(), '/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.pth')\n","recorded":"2024-10-21 15:32:44.118202991","filePath":"null","pinned":false},{"value":"\ntrain_gan(generator, discriminator, dataloader, epochs, latent_dim, '/content/drive/My Drive/Lung dataset/generated_images')\n\n# Save model weights\ntorch.save(generator.state_dict(), '/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights.pth')\n","recorded":"2024-10-21 15:26:36.359391248","filePath":"null","pinned":false},{"value":"epochs = 10000\nbatch_size = 64\n\ndef train_gan(generator, discriminator, dataloader, epochs, latent_dim, save_path):\n    for epoch in range(epochs):\n        for i, real_images in enumerate(dataloader):\n            real_images = real_images.to(device)\n\n            # Train Discriminator\n            optimizer_D.zero_grad()\n            batch_size = real_images.size(0)\n            real_labels = torch.ones(batch_size, 1).to(device)\n            fake_labels = torch.zeros(batch_size, 1).to(device)\n\n            outputs = discriminator(real_images)\n            d_loss_real = adversarial_loss(outputs, real_labels)\n\n            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n            fake_images = generator(z)\n            outputs = discriminator(fake_images.detach())\n            d_loss_fake = adversarial_loss(outputs, fake_labels)\n\n            d_loss = d_loss_real + d_loss_fake\n            d_loss.backward()\n            optimizer_D.step()\n\n            # Train Generator\n            optimizer_G.zero_grad()\n            outputs = discriminator(fake_images)\n            g_loss = adversarial_loss(outputs, real_labels)\n            g_loss.backward()\n            optimizer_G.step()\n\n        print(f\"Epoch [{epoch}/{epochs}] - D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n        if epoch % 100 == 0:\n            save_images(fake_images, path=f\"{save_path}/epoch_{epoch}\", num_images=25)\n","recorded":"2024-10-21 15:26:31.106015733","filePath":"null","pinned":false},{"value":"# Function to save generated images\ndef save_images(images, path, num_images=25):\n    grid = vutils.make_grid(images[:num_images], nrow=5, normalize=True)\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.transpose(grid.cpu().numpy(), (1, 2, 0)))\n    plt.axis('off')\n    os.makedirs(path, exist_ok=True)\n    plt.savefig(f\"{path}/generated_images.png\")\n    plt.close()\n","recorded":"2024-10-21 15:26:23.680078723","filePath":"null","pinned":false},{"value":"\n# Initialize Generator and Discriminator\nlatent_dim = 100\ngenerator = Generator(latent_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Optimizers\nlr = 0.0002\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# Loss function\nadversarial_loss = nn.BCELoss()","recorded":"2024-10-21 15:26:19.255118352","filePath":"null","pinned":false},{"value":"# Define the Discriminator model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.4),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.4),\n            nn.Flatten(),\n            nn.Linear(128 * 16 * 16, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img):\n        return self.model(img)","recorded":"2024-10-21 15:26:11.197326329","filePath":"null","pinned":false},{"value":"# Define the Generator model\nclass Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128 * 16 * 16),\n            nn.LeakyReLU(0.2),\n            nn.Unflatten(1, (128, 16, 16)),\n            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 1, kernel_size=7, padding=3),\n            nn.Tanh()  # Output is in range [-1, 1]\n        )\n\n    def forward(self, z):\n        return self.model(z)","recorded":"2024-10-21 15:26:05.777196512","filePath":"null","pinned":false},{"value":"# Display sample images\nsample_images = next(iter(dataloader))\ndisplay_images_with_labels(sample_images, num_samples=5)","recorded":"2024-10-21 15:25:56.620628129","filePath":"null","pinned":false},{"value":"\n# Function to display images\ndef display_images_with_labels(images, num_samples=5):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        axes[i].imshow(images[i][0], cmap='gray')\n        axes[i].axis('off')\n    plt.show()","recorded":"2024-10-21 15:25:51.227129099","filePath":"null","pinned":false},{"value":"# Combine the datasets\ncombined_dataset = normal_dataset + pneumonia_dataset\ndataloader = DataLoader(combined_dataset, batch_size=64, shuffle=True)","recorded":"2024-10-21 15:25:42.139262685","filePath":"null","pinned":false},{"value":"# Load the datasets\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\nnormal_dataset = LungDataset(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), 64, 64, transform)\npneumonia_dataset = LungDataset(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), 64, 64, transform)\n","recorded":"2024-10-21 15:25:36.818057651","filePath":"null","pinned":false},{"value":"# Define image transformation (resizing and normalization)\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n])","recorded":"2024-10-21 15:25:28.273902682","filePath":"null","pinned":false},{"value":"# Dataset class to load and preprocess images\nclass LungDataset(Dataset):\n    def __init__(self, folder, img_width, img_height, transform=None):\n        self.folder = folder\n        self.img_width = img_width\n        self.img_height = img_height\n        self.transform = transform\n        self.image_paths = [os.path.join(folder, fname) for fname in os.listdir(folder)]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('L')\n        if self.transform:\n            image = self.transform(image)\n        return image","recorded":"2024-10-21 15:25:22.110850466","filePath":"null","pinned":false},{"value":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","recorded":"2024-10-21 15:25:15.206548255","filePath":"null","pinned":false},{"value":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport matplotlib.pyplot as plt\nfrom PIL import Image","recorded":"2024-10-21 15:25:05.905438143","filePath":"null","pinned":false},{"value":"# Vanilla GAN model\ndef build_generator(latent_dim):\n\n    # model = models.Sequential()\n    # model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    # model.add(layers.BatchNormalization())\n    # model.add(layers.LeakyReLU(alpha=0.2))\n    # model.add(layers.Reshape((16, 16, 128)))\n    # model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    # model.add(layers.BatchNormalization())\n    # model.add(layers.LeakyReLU(alpha=0.2))\n    # model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    # model.add(layers.BatchNormalization())\n    # model.add(layers.LeakyReLU(alpha=0.2))\n    # model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    # return model\n    model = models.Sequential()\n    model.add(layers.Dense(128 * 16 * 16, input_dim=latent_dim))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((16, 16, 128)))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Conv2D(channels, (7, 7), activation='sigmoid', padding='same'))\n    return model\n\n# Define the Discriminator\ndef build_discriminator(input_shape):\n    model = models.Sequential()\n    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Dropout(0.4))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\n# Define the GAN\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = models.Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    return model\n\nlatent_dim = 100\ninput_shape = (img_width, img_height, channels)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator(input_shape)\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator(latent_dim)\n\n# Build the GAN\ngan = build_gan(generator, discriminator)\n\ngan.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.0001, beta_1=0.5))\nvgan_losses = {'discriminator_loss': [], 'generator_loss': []}\n# Train the GAN\ndef train_gan(generator, discriminator, gan, real_data, epochs, batch_size,save_path):\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # Train Discriminator\n        idx = np.random.randint(0, real_data.shape[0], batch_size)\n        real_images = real_data[idx]\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        fake_images = generator.predict(noise)\n        discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n        discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n        discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n        # Train Generator\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        generator_loss = gan.train_on_batch(noise, real_labels)\n        print(f\"Epoch {epoch}, Discriminator Loss: {discriminator_loss[0]}, Generator Loss: {generator_loss}\")\n        vgan_losses['discriminator_loss'].append(discriminator_loss[0])\n        vgan_losses['generator_loss'].append(generator_loss)\n        if epoch % 1000 == 0:\n          save_images(generator.predict(np.random.normal(0, 1, (25,latent_dim))), path=f\"/content/drive/My Drive/Lung dataset/generated_imagesImproved/epoch_{epoch}\")\n    generator.save_weights(save_path)\n\nepochs=10000\nbatch_size=64\ndef save_images(images, path='/content/drive/My Drive/generated_imagesImproved/'):\n    os.makedirs(path, exist_ok=True)\n    for i, image in enumerate(images):\n        plt.imshow(image)\n        plt.axis('off')\n        plt.savefig(f\"{path}generated_image_{i}.png\")\n        plt.close()\n\ntrain_gan(generator, discriminator, gan, real_data, epochs, batch_size,'/content/drive/My Drive/Lung dataset/archive/lungs_generator_weights2.h5')\n","recorded":"2024-10-21 15:21:36.672098692","filePath":"null","pinned":false},{"value":"img_width, img_height = 64, 64\nchannels = 1  # Grayscale\n\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\n!ls \"/connt/drive/My Drive/Lung dataset/archive\"\n# Load and preprocess images from the dataset\nnormal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\npneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)","recorded":"2024-10-21 15:21:26.078676251","filePath":"null","pinned":false},{"value":"from tensorflow.keras import layers, models, optimizers","recorded":"2024-10-21 15:21:08.130368589","filePath":"null","pinned":false},{"value":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","recorded":"2024-10-21 15:21:01.221079794","filePath":"null","pinned":false},{"value":"Pratik Gupta","recorded":"2024-10-21 13:16:42.650612739","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Results.zip","recorded":"2024-10-21 13:07:37.042481862","filePath":"null","pinned":false},{"value":"INTRODUCTION\nCancer is a disease brought on by aberrant cells when an internal component is expanding\nout of control. A portion of the body‚Äôs cells in all tumors, begin to divide rapidly and\nspread to parts of the neighboring tissues. Among the millions of cells, cancer can appear\nvirtually at any place in the body. Normally, human cells multiply and divide to produce\nnew cells as the body requires them. When a cell becomes damaged or old, it expires and\nis replaced by a fresh cell. But as cancer grows, this systematic mechanism disintegrates.\nOld or injured cells that should have died survive, as cells become more and more erro-\nneous whereas new cells are generated even when they are unwanted. These cells can\ndivide to form new ones, which may lead to tumor-like growth. Solid tumors, or masses\nof tissue, are a common kind of cancer. Leukemias and other blood cancers typically do\nnot develop solid tumors\n1.1\n Oral Cancer\nThe mouth‚Äôs cells are the first to develop oral cancer. A cancerous (malignant) nodule is\na group of cancer cells tumor that has the ability to invade neighboring tissue and wreck\nmisery on it. It can also metastasize to different parts of the body. Nodes of lymph in\nthe neck are the part where mouth cancer spreads most frequently. Oral cancer may also\nbe referred to as mouth cancer. Sometimes, cells that are present in the mouth undergo\nchanges and will stop growing or behaving properly. These alterations could result in\nbenign (non-cancerous) tumors like warts and fibromas. Precancerous diseases can also\nbe brought on by changes in the mouth‚Äôs cells. This indicates that although the abnormal\ncells are not now cancer, there is a potential that they could develop into cancer if left\n1\nuntreated. Leukoplakia and erythroplakia are two of the most prevalent precancerous\ndisorders of the mouth.\nOral cancer can, however, occasionally result from alterations to the mouth‚Äôs cellular\nstructure. The oral mucosa (mucous membrane) is a lining that lines the mouth. The\nsquamous epithelium, which composes the oral mucosa, is made up of squamous cells.\nThese thin, flat squamous cells are where mouth cancer typically begins. The term for\nthis type of cancer is mouth squamous cell carcinoma.\n1.1.1\n Type of Oral Cancer\nThe following organs can develop cancer:\n‚Ä¢ Lips\n‚Ä¢ Tongue\n‚Ä¢ Inner lining of the cheek\n‚Ä¢ Gums\n‚Ä¢ Mouth Cancer\n‚Ä¢ Hard and Soft Palate\n1.2\n Mouth Cancer\nMouth cancer, commonly referred to as oral cancer, happens whenever a tumor forms\ninside the mouth lining. It could be located on the surface of the tongue, the interior of\nthe cheeks, the palate, the lips themselves, or the gums. Additionally, the glands that\ncreate tumors saliva, the tonsils in the rear within the mouth. But these occur frequently.\nSymptoms of mouth cancer:\n‚Ä¢ A mouth or lip sore that does not heal\n‚Ä¢ An internal mouth patch that is either white or red\n‚Ä¢ A growth or bulge inside your mouth; loose teeth;\n‚Ä¢ Painful or difficult swallowing\n2\n1.2.1\n Cause of Mouth Cancer\nWhen DNA alterations (mutations) occur in the mouth or lip cells, mouth carcinomas de-\nvelop. DNA includes the instructions for what the cell must accomplish. When normally\nfunctioning cells would die, alterations cause the continued growth and division of the\ncells. The aberrant mouth cancer cells can assemble into a tumor. In time, they might\nspread from the inside of the mouth to the whole body, including the neck or various parts\nof the head.\nMouth cancers tend to start in the flat, thin cells (squamous cells) which define the sur-\nface of the lips and the interior of the mouth. Oral cancer is most frequently caused by\nsquamous cell tumors.\n1.3\n Human Mouth Structure\nThe start of the human mouth is where the lips and skin converge Figure 1.1 shows the\nstructure of the human mouth. The roof of the mouth is made up of both hard and soft\npalates. A soft palate divides the mouth from the nasopharynx (the upper part of the\npharynx), which is connected to the mouth via the oropharynx (the middle section of the\npharynx). The sides of the mouth are formed by the cheeks‚Äô inner surface (De Angeli et\nal. 2022). The majority of the mouth‚Äôs floor, or lowest portion, is occupied by the tongue.\nThe mouth can be divided into various sections, including-:\n‚Ä¢ The Lips\n‚Ä¢ The Tonsils and The Soft Palate\n‚Ä¢ The Uvula and the Tongue\n‚Ä¢ The buccal mucosa, which coats the cheekbones‚Äô interior\n‚Ä¢ The roof of the mouth\n‚Ä¢ Teeth, gums, and alveolar ridge, which is the ridge-like border of the jaws that\ncontains the tooth sockets.\n‚Ä¢ The Mandible (Lower Jawbone)\n3\nFigure 1.1: Structure of Human mouth\n(German and Palmer 2006)\n1.4\n Diagnosing Techniques for Oral Cancer\nThere are different techniques that are used for the diagnosis of oral cancer, few of the\nclinical techniques used by doctors are discussed below :-\nBarium Swallow: - The voice box, the throat, referral, and surroundings may display\nabnormalities during a barium swallow test, which is also frequently used to find small,\nearly oral tumors.\nBiopsy: - The initial step in identifying mouth cancer is an oral tissue biopsy. A little bit\nof aberrant tissue from the area where oral cancer is suspected is removed by the surgeon\nduring the biopsy. An oral cancer diagnosis may be confirmed by biopsy. The following\ntypes of biopsies are frequently used to identify oral carcinoma:\n‚Ä¢ Incisive biopsies: The region has a small amount of tissue taken from it that appears\nto be abnormal. If the abnormal location is easily accessible, the specimen could\nbe obtained at the office of a doctor. If the cancer is more deeply embedded in the\nmouth or throat, biopsy procedures might have to be carried out in a surgical theatre\nwhile receiving anesthesia in order to lessen pain.\n‚Ä¢ Exfoliative cytology: Cell samples are gently scraped from a questionable loca-\ntion. To make the cells visible under a microscope, they are put upon a transparent\n4\nslide, and subsequently colored. A deeper biopsy will be done if any cells seem\nsuspicious.\nImage-based tests\n‚Ä¢ Computerized Tomography, or CT, Scanning ‚Äì Information on the size, shape, and\nlocation of any tumors can be obtained via a CT scan, which can help detect lymph\nnodes that are bulging that may contain cancer cells.\n‚Ä¢ Magnetic Resonance Imaging (MRI): Oral cancer may be examined with an MRI\nscan, although this is less usual. MRIs give a very thorough picture and may be\nvery helpful in figuring out whether other areas of the body, such as the neck, have\nbeen affected by the disease‚Äôs spread.\n‚Ä¢ Positron emission computed tomography (PET): Patients with cancer of the oral\ncavity might get a scan using PET technology to find out whether the disease has\nmigrated to the lymph nodes or whether it has only recently progressed to that\nlocation.\n‚Ä¢ Genomic testing for advanced oral cancer: -Genomic testing is sometimes known as\nmolecular profiling or cancer sequencing. Examining the collected cells is required\n8 from a biopsy in order to check for any genetic mutations (changes in your DNA)\nthat might be connected to the person‚Äôs particular type of cancer.\n1.5\n Oral cancer: Globally\nAmong the most prevalent malignancies worldwide is oral cancer. The majority of cases\nof this subtype of head and neck cancer begin in the cells of squamous tissue that cover\nthe surface of our mouth, tongue, and faces. When this fails to be identified and if not\naddressed in a timely manner, it could be deadly. About 53,000 incidences of oral cancer,\nor three percent of all cancers identified during the study in US annually, are related to\noral cancer. Oral cancer strikes males more frequently than females, more than twice as\noften, and persons over an age of 40 are most at risk.\nSmoking, drinking alcoholic beverages, or having HPV, short for People Papilloma virus\ninfection are the main causes of oral cancer. In 2020, there are expected to be over 177,000\n5\ndeaths globally from lip and oral cavity cancer, In spite of improvements mouth cancer\nfatality rates have remained high in recent decades.\nThe majority of mouth cancer patients, particularly those located in countryside regions,\ncan‚Äôt obtain fast, effective diagnosis and treatment, which lowers their chance of survival.\nDepending on race and location, patients with cancer have a five-year living rate among\nthe 50%. According to reports, the survival rate in developed nations can reach 65%.\nIn contrast, leaning upon the area of the mouth cancer affected, a living rate of fifteen\npercent is noted in some countryside areas. It‚Äôs because cancer therapy may be highly\nexpensive, especially in later stages. Health experts and the general public both lack a\nsignificant grasp of oral cancer. The 2020 Cancer Statistics Report for India states 66.6\npercent of patients with head and neck cancer had already progressed locally when they\nreceived their diagnosis. Inflammation or ulcers that do not heal, along with discomfort\nand bleeding, are signs of oral cancer.\nOral cancer can be caused by a number of habits, with smoking and drinking being the\ntwo most significant ones. Consuming maggots is so common in India that it causes in-\nternal gum damage.\nGLOBOCAN (Global Cancer Incidence, Mortality and Prevalence) anticipated that in\nFigure 1.2: Global age standardized prevalence of tobacco smoking source World Health\nOrganization\n(Dai, Gakidou, and Lopez 2022)\n2018, there would be 177,384 cancer-related deaths and 354,864 new instances of cancer,\n6\nwhich corresponds to two percent and one point nine percent of all occurrences and fa-\ntalities from cancer, respectively. In summary, mouth cancer, which accounts for around\none-third of all cancer cases, is a major reason for death in Bangladesh, Pakistan, Taiwan,\nand India.\n1.6\n Issues with Oral Cancer Manual Diagnoses\nThe primary problem with manual cancer diagnosis is the delay in diagnosis. It requires\nextremely competent labor, and the number of needed diagnostic tests is increasing dra-\nmatically. Because of the time requirements for a proper diagnosis, it is less likely that\nan early identification of the tumor grade will be made.Pathologists heavy workload is a\nserious worry, and this also affects how well they can anticipate outcomes. It also Prevent\nthe delivery of an accurate diagnosis report as the findings must be carefully crafted to\navoid any fatalities.\n1.7\n Deep Learning\nOne of the main components of an Artificially Intelligent system is learning. Learning\nmeans when a computer program can learn through its surrounding. Artificially intelli-\ngent systems have the ability to mimic the human brain and have the ability to process\ninformation and develop various patterns used to make decisions (Dubuc et al. 2022). A\nsub type of machine learning called \"deep learning\" in artificial intelligence (AI) allows\nnetworks to learn unsupervised from unlabelled input. Deep learning can also refer to\ndeep neural networks or deep learning.\n1.7.1\n Importance of Deep Learning\nMachine learning techniques can now build and learn from a large pool of training data\nbecause to improvements in computer speed and memory over time.\nDeep learning has been a cutting-edge method for humanity, especially when the Informa-\ntion is noisy. Artificial neural networks can learn any function with just one hidden layer,\nregardless of how ambiguous it is, which is why they are regarded as universal function\napproximations.\n7\n1.7.2\n CNN\nConvolutional neural network (CNN) is a subtype of ANN. In at least one of their layers,\nCNN‚Äôs replace conventional matrix multiplication methods with the convolution mathe-\nmatical technique. Since they were developed specifically to handle pixel data, they are\nused in image recognition and processing. The design with which CNN is built is compa-\nrable with the model of neural connection like a person‚Äôs brain (Jeyaraj, B. K. Panigrahi,\nand Samuel Nadar 2022). Because of the way CNN is built, there are some strong prefer-\nences ingrained in them, which makes it easier to comprehend why they are so effective.\nCNN can be seen as a feed-forward network but having connection with each image can\nFigure 1.3: A CNN Architecture\n(Sun et al. 2019)\nbe inefficient. Therefore, we can prune the useless connection between the hidden layers\nto increase the performance of the layer. A CNN is a special artificial neural network with\nlimited connections between the layers of artificial neural network.\n‚Ä¢ Max-Pooling: Each feature map produced by processing the input through many\nlayers of convolution is subsequently combined in a pooling layer. Little grids are\nused for input for pooling procedures, which generate only one value for every re-\ngion. The pooling layers provide CNN significant translational consistency since a\n8\ntiny change in the input image causes a slight modification in the activation maps.\nApplying convolutions with longer strides is another method for obtaining the pool-\ning‚Äôs down sampling effect. The network design is made simpler by eliminating the\npooling levels without compromising performance. Max-pooling is the most widely\nemployed of all these pooling techniques.\n‚Ä¢ Fully-Connected Layers: Matrix multiplications have traditionally been the build-\ning blocks of neural networks, which are scattered with sigmoid nonlinearities. The\nlayers of the multiplication matrices are referred to as connected layers due to the\nconnection between each unit in the layer before and each unit in the layer af-\nter. There is just small-scale spatial connectivity when using convolutional layers.\nSignificant amounts of completely linked layers are typically avoided in modern\nnetworks since they require massive parameters.\n‚Ä¢ Learning algorithm: Lacking an algorithm to quickly and effectively learn the pa-\nrameters of the model, there is little value for an expensive model. Lacking a tech-\nnique for efficiently acquiring the model‚Äôs parameters, a strong, expressive model\nis of little use. In the pre-AlexNet era, greedy layer-wise pre-training techniques\nattempted to create such an efficient approach. A more straightforward supervised\ntraining approach is sufficient to learn a reliable model for tasks relating to com-\nputer vision.\n‚Ä¢ Optimization Based on Gradient: - Typically, the backpropagation technique is used\nto train networks, which accelerates mathematical calculation to calculate the gra-\ndient used in the Gradient Descent (GD) algorithm. However, employing GD is\nimpracticable for datasets with many hundreds or even more data points. In these\ncircumstances, Stochastic Gradient Descent (SGD), an approximation where gradi-\nents are computed for data points individually rather than the complete data set, is\nfrequently used. Training using SGD generalizes more successfully than with GD,\nit has been discovered.\n‚Ä¢ Batch Normalization:- A helpful regularizes that enhances generalisation and sharply\naccelerates convergence is batch normalisation (BN). The order of presentation of\nthe inputs to each layer varies continuously during the training phase, which is a\nproblem caused by inner covariate variation. This effect typically causes training\n9\nto take longer and requires careful initialization. This problem is addressed by BN,\nwhich normalises a layer‚Äôs production stimulation to ensure that its spectrum is\nconstrained to a restricted range. In particular, BN normalises each mini-batch‚Äôs\nmean-variance statistics using its running average. Recently, BN has been recog-\nnised as a crucial element of very deep networks.\n‚Ä¢ Activation layer :- Deep networks typically have convolutions after each layer,\nwhich then follows a nonlinear process. This is required because convolutions are\nan example of a cascading linear system. Layer-to-layer nonlinearities make the\nmodel more evocative than a model with linear dynamics. Theoretically, as long\nas nonlinearities are ongoing bounded, and gradually rising, no nonlinearity has a\ngreater capacity for expressiveness than any other. The sigmoid or the tanh were\nnonlinearities employed in classical neural networks that feed forward. However,\nthe Rectified Linear Unit (ReLU) is used in contemporary convolutional networks.\nIt has been discovered that CNNs with this nonlinearity train more quickly. The\nleaky- ReLU is a brand-new category of nonlinearity that has lately been intro-\nduced. Leaky-ReLU(x) = max(0, x) + min(0, x) is its formula, where is a preset\nparameter. It is better since it implies that the characteristic can also be taught,\ncreating a model that is considerably deeper. Leaky ReLUs or adjustable ReLUs\nare examples of variations on ReLU(z)=max (0; z). The feature maps, which are\nfrequently also referred to as feature maps, are fed through a process of activation\nto create new tensors.\n1.7.3\n Working on Deep Learning Networks\nSince most deep learning methods rely on neural network topologies, they are referred\ndescribed as \"deep neural networks\".\nNormal neural nets only have a few hidden levels, whereas deeper networks may contain\nup to 150 layers. Very vast quantity of categorised autonomously generated data and neu-\nral network topology extract features.\na) Training from Scratch:- For a deep network to be trained from beginning, a very large\nlabelled data set must be gathered, and a network architecture must be created that will\nallow the network to gain insight into its characteristics and predict. This is advantageous\nfor newly developed apps or applications with numerous output categories. This is a less\n10\nfrequent strategy because these networks often take weeks or even months to train be-\ncause to the volume of data and learning rate.\nb) Transfer Learning:- It is a deep learning technique where a pre-trained model is mod-\nified as part of the transfer learning approach. It begins with a reliable network, like\nAlexNet or GoogleNet, then feeds it new values which are previously undiscovered classes.\nThe task can now be carried, out after making network modifications that are minimal.\nMoreover, processing hundreds of photographs as opposed to millions has the advantage\nof requiring much less data, which cuts down computation time to minutes or hours.\nc) Feature Extraction:- The network can be used as a feature extractor, which is a little less\ntypical and a more specialised method of deep learning. Feature Extraction can remove\nspecific features from the network at any point throughout the training process because\nall the layers are charged with learning specific features from images.\n1.7.4\n Purpose of Deep Learning\nThe models developed using Deep Learning have the potential to provide more precise\nand individualised cancer treatment by better predicting the prognosis of the disease. They\nare superior to or on par with the methods now used in clinical settings. Deep learning\ntechniques are anticipated to help in the proper handling of squamous cell carcinoma of\nthe oral cavity through enhanced diagnostic performance, wise clinical decision-making,\nstreamlining of clinicians‚Äô work, the potential for lowering cancer screening costs, and a\nsuccessful evaluation and detection of the disease. In order to increase the quality of care,\nprofessionals and patients can spend more time talking to one another and deliberating\ntogether. Future research should focus on creating deep learning models that integrate\ndiverse datasets from many modalities.\n‚Ä¢ Pre ‚ÄìProcessing: Due to a variety of factors, the original image will always contain\nsome noise. The accuracy of the diagnosis is compromised by these noises. A cru-\ncial part of the image processing process is pre-processing. Asymmetric filtration is\na filter that is frequently used to enhance grayscale photographs by reducing noise\nand improving image arrangement, particularly edge boundaries.\n‚Ä¢ Feature Extraction: - we can generate new features from the previous feature and\nthen we can delete the original features by doing this we can reduce the features\n11\npresent in the dataset. It helps us to categorize the images into different groups.\n‚Ä¢ Feature Selection: - Providing a vast amount of features to the model can result\nin a overfitted model with a very high computational time, having a better feature\nextraction will help in reducing the time complexity\nFigure 1.4: Flow chart showing different Phases in detection of oral cancer\n1.8\n Metaheuristic Optimization\nReal-world optimisation issues frequently involve a large number of choice variables, in-\ntricate nonlinear constraints, and difficult objective functions, which makes them more\nand more difficult to solve. Using conventional strategies like numerical methods, the\n12\nglobal optimization is less effective, particularly when limitations or objective functions\ninclude many peaks. Strong instruments for tackling difficult optimisation problems,\nmetaheuristic algorithms are gaining popularity.\nThe simplicity of metaheuristic algorithms is by far their most notable feature. The fun-\ndamental theories or mathematical models underlying these metaheuristic techniques are\nderived from nature. The majority of these techniques are straightforward and simple to\nuse. One can utilise metaheuristics to solve real-world problems thanks to their usability.\nAdditionally, it is simple to create their versions using current techniques.\nThese optimisation technologies can be thought of as \"black boxes,\" capable of providing\na set of outputs for a specific problem for a specific set of inputs. One of the most crucial\naspects of metaheuristic algorithms is randomization. This makes it possible for meta-\nheuristic algorithms to effectively avoid trapping in local optima and to search the whole\nsearch space. More specifically, it enables numerous metaheuristics to handle issues in-\nvolving an ambiguous search space or various local optima. Finally, because of their ex-\ntreme adaptability and flexibility, these metaheuristics can be used to solve a wide range\nof optimisation issues, including non-linear issues, issues involving non-differentiable\nvariables, and issues involving sophisticated numerical calculations and a large number\nof local minima.\n1.9\n Motivation\nThe latest trend in increase of oral cancer is having an adverse effect on health of human\nbeing. Oral cancer can be treated if detected early, with the increase in total number of\ncases of oral cancer we need an accurate and fast way to detect cancer cells. The risk\nof oral cancer is in all age groups but elder people are more prone to it due to unhealthy\nlifestyle. A lot of people have experienced financial troubles. It is crucial for the early\ndiagnosis of disease so that patients can start taking preventative measures right away. AI,\nwhich consists of machine learning and deep learning, is heavily reliant on classification,\ngrading, segmentation, and computer vision. To more or less better model optimisation is\nthe main reason for conducting research in this field.\n‚Ä¢ Deep learning concept fascinate me to learn more in this area. Deep learning based\nmodel can detect oral cancer with early signs that can be captured by modern cam-\n13\neras\n‚Ä¢ Clinical Images can give an more accurate and fast result as compare to normal\nmethods applied by Doctors\n1.10\n Problem Statement and Research Objective\nA large number of deaths were recorded from oral cancer as a result of lack of its identifi-\ncation and late treatment. Oral cavity cancer has a significant mortality rate that is rising.\nIt is crucial to develop and put into practise a method for detecting this malignancy early\non. By identifying cancer early and adopting preventative measures, it is simple to limit\nthe number of deaths brought on by the disease. Although many researchers have already\nconducted their research in the field of oral cancer disorders, there is still a great deal of\nresearch that may be done in this area owing to performance improvements.\nMachine learning has advanced to the point where getting more use out of it is all but\nimpossible during the last several years. The performance of Deep learning models have\nincreased but there is still a concern of model size, low accuracy and high computation\ntime.\n‚Ä¢ To propose and implement optimized Deep learning algorithm for detection of oral\ncancer in it‚Äôs early stages.\n‚Ä¢ To implement Metaheuristic optimization for better weight selection of clinical im-\nages.\n‚Ä¢ To conduct an analysis and compare the proposed approach with state of art models\non basic of evaluation matrices like accuracy, precision, Sensitivity and Specificity.\n1.11\n Thesis outline\nThe chapters of the thesis are organised consistently into an overview, key facts and fig-\nures, significant content, pertinent data, and a final chapter summary. All references are\nincluded at the end and each Figure, table, and piece of text is correctly referenced. The\nfive chapters that make up this thesis are arranged as follows:\n14\n‚Ä¢ Chapter 1: It provides a succinct overview of oral cancer, including its kinds, symp-\ntoms, and methods of diagnosis. It describes how the process of making medical\ndiagnoses has been transformed by machine learning, neural networks, and deep\nneural networks. Why has CNN surpassed conventional neural networks? what\nmotivated and inspired you to work in medicine. Additionally, it provides informa-\ntion about the goals and motivation.\n‚Ä¢ Chapter 2: It gives a brief overview of the literature for a number of researchers who\nworked on various methods for automatic oral cancer diagnosis, image processing,\nand texture-based categorization. Artificial neural networks, deep learning. A re-\nview of all pertinent theories and techniques for diagnosing oral cancer that are\navailable in the literature.\n‚Ä¢ Chapter 3: The chapter sheds insight on a crucial experiment study and the approach\nused to carry out our investigation. The models and various detection architectures\nemployed by CNN have been described. The proposed model is covered in this\nchapter; it has fewer parameters and a shallower learning curve than the pretrained\nmodel, but it is more accurate.\n‚Ä¢ Chapter 4: With the use of a graph, bar chart, and other presentation approaches,\nall model and performance metric results are shown. The model‚Äôs shortcomings are\nthen displayed and contrasted with the suggested model.\n‚Ä¢ Chapter 5: The entire work is concluded in the last chapter. This chapter also\ndiscusses how we might enhance our efforts in the future.","recorded":"2024-10-21 13:03:50.213154809","filePath":"null","pinned":false},{"value":"\\address[2]{Department of Computer Science \\\u0026 Engineering, DR. B.R. Ambedkar National Institute of Technology, Jalandhar -- $144027$, India}","recorded":"2024-10-21 12:58:49.563837468","filePath":"null","pinned":false},{"value":"Prognostic tools","recorded":"2024-10-21 12:57:44.248084303","filePath":"null","pinned":false},{"value":"Evolutionary optimization","recorded":"2024-10-21 12:57:36.356951912","filePath":"null","pinned":false},{"value":"Metaheuristic optimization","recorded":"2024-10-21 12:57:25.665484771","filePath":"null","pinned":false},{"value":"Manta Ray Foraging Optimization (MRFO)","recorded":"2024-10-21 12:57:16.333872857","filePath":"null","pinned":false},{"value":"Transfer learning","recorded":"2024-10-21 12:57:06.035801206","filePath":"null","pinned":false},{"value":"Clinical image analysis","recorded":"2024-10-21 12:56:58.668525204","filePath":"null","pinned":false},{"value":"Deep learning","recorded":"2024-10-21 12:56:50.072238422","filePath":"null","pinned":false},{"value":"Oral cancer detection","recorded":"2024-10-21 12:56:45.380432613","filePath":"null","pinned":false},{"value":"Oral cancer constitutes a considerable worldwide health challenge, especially in areas like India, where it is the sixth most common malignancy, resulting in roughly 130,000 deaths each year. Contemporary diagnostic methods, notwithstanding their diversity, encounter constraints in precision, especially in differentiating malignant cells. Recent advancements in deep learning have demonstrated potential in improving diagnostic accuracy, providing a means to decrease false positives and negatives, and facilitating more dependable prognostics and therapy strategies.\n\nDeep learning architectures, despite their computational complexity, have exhibited remarkable efficacy in numerous categorization tasks, including medical image analysis. Image-based deep learning algorithms for oral cancer diagnosis primarily employ two categories of datasets: clinical images and histopathological images. Due to the availability of high-quality imaging tools, clinical images are now more practical for extensive diagnostic applications. Conversely, histopathology images necessitate specialist equipment and high magnification, presenting practical hurdles for general application.\n\nThis research aims to utilize clinical picture datasets to develop a deep learning framework for the identification of oral cancer. We present an optimization-based metaheuristic strategy that integrates numerous pre-trained models, including VGG19, ResNet50, and EfficientNet, to improve prediction reliability. The novelty resides in the utilization of Manta Ray Foraging Optimization (MRFO) to optimize these models. This ensemble method markedly enhances diagnostic performance, attaining accuracy, sensitivity, and specificity ratings of 97.4%, 95.63%, and 94.12%, respectively.\n\nOur research illustrates the efficacy of deep learning in the identification of cancer through clinical imaging, providing a more accessible and efficient alternative. Future research avenues encompass the expansion of this study to multiclass cancer diagnosis and the investigation of evolutionary techniques for enhanced optimization of deep networks.","recorded":"2024-10-21 12:53:56.507944646","filePath":"null","pinned":false},{"value":"histopathological","recorded":"2024-10-21 12:52:36.198872543","filePath":"null","pinned":false},{"value":"Oral cancer remains a significant global health challenge, particularly in regions such as India, where it ranks as the fifth most prevalent cancer, leading to approximately 130,000 fatalities annually. Current diagnostic techniques, though varied, face limitations in their accuracy, particularly in distinguishing cancerous cells. Recent advances in deep learning have shown promise in enhancing diagnostic precision, offering an avenue to reduce false positives and negatives, and enabling more reliable prognostics and therapeutic interventions.\n\nDeep learning architectures, despite their computational complexity, have demonstrated exceptional performance in various classification tasks, including medical image analysis. For oral cancer diagnosis, image-based deep learning models primarily utilize two types of datasets: clinical images and histopathological images. Given the accessibility of high-quality imaging devices, clinical images have become more feasible for large-scale diagnostic applications. In contrast, histopathological images, requiring specialized equipment and high magnification, pose practical challenges in widespread usage.\n\nThis study focuses on leveraging clinical image datasets to build a deep learning framework for the detection of oral cancer. We propose a metaheuristic optimization-based approach, integrating multiple pre-trained models, including VGG19, ResNet50, and EfficientNet, to enhance prediction reliability. The innovation lies in the application of Manta Ray Foraging Optimization (MRFO) to fine-tune these models. This ensemble-based method significantly improves diagnostic performance, achieving accuracy, sensitivity, and specificity scores of 97.4%, 95.63%, and 94.12%, respectively.\n\nOur findings demonstrate the potential of deep learning in clinical image-based cancer detection, offering a more accessible and efficient solution. Future research directions include extending this work to multiclass cancer detection and exploring evolutionary algorithms for further optimization of deep networks.","recorded":"2024-10-21 12:50:40.139528367","filePath":"null","pinned":false},{"value":"Oral cancer is a prevalent and challenging cancer with a high mortality rate. It is the fifth\nmost common cancer in India, with 130,000 fatalities annually. There are a number of\ndiagnostic techniques for oral cancer, however their precision in identifying cancer cells\nis constrained.\nDeep architectures are becoming more popular as a result of their aptitude for solving\ncomplex problems. Deep architectures have proven effective in numerous classification\nproblems. Despite their incredible representational capacity, deep networks are difficult\nto train computationally. Deep learning minimises false-positive and false-negative errors\nin the detection and diagnosis of this condition, creating a new opportunity to provide\npatients with quick and safe prognostics treatments.\nDeep Learning models work on images, for oral cancer there are 2 dominant datasets\nof Clinical Images and Histopathological Images. The dataset used in study is clinical\ndataset, as now a days phones that can click a high quality images are available with ev-\nerybody therefore anyone can click a image and provide to model to check whether the\nimage id cancerous or not, on the other hand creating Histopathalogical images is a big\ntask, these images are taken at 400X magnification and such tools might not be readily\navailable. Therefore this study aims to use a model that can predict the outcome on clini-\ncal images\nHowever, the majority of research relies on a specific model prediction, and the final find-\nings may or may not be reliable. We suggested a metaheuristic optimization-based deep\nlearning technique. We employ many pre-trained models, including VGG19, ResNet50,\nand EfficientNet, using images of oral cancer. To evaluate the performance of the sug-\ngested method, all of the models‚Äô output was compared to it. Various Transfer of Learning\nmodels with Manta Ray Foraging optimisation are used in our studies, and they produce\nbetter outcomes, with accuracy, sensitivity, and specificity ratings of 97.4, 95.63, and\n94.12, respectively. Future work on this project might include detecting multiclass can-\ncer photos and investigating deep network optimisation systems based on evolutionary\ntechniques.","recorded":"2024-10-21 12:49:22.986939210","filePath":"null","pinned":false},{"value":"bind = $mainMod SHIFT, N, exec, $fileManager\n","recorded":"2024-10-21 12:45:32.578800500","filePath":"null","pinned":false},{"value":"# CNN Model\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Initialising the CNN\nclassifier = Sequential()\n\n# Step 1 - Convolution\nclassifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n\n# Step 2 - Pooling\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n# Adding a second convolutional layer\nclassifier.add(Conv2D(32, (3, 3), activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\n","recorded":"2024-10-21 12:38:38.407438932","filePath":"null","pinned":false},{"value":"def fib(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fib(n-1) + fib(n-2)\n\n\nn = int(input())\nprint(fib(n))\n","recorded":"2024-10-21 12:37:40.270187376","filePath":"null","pinned":false},{"value":"eyJhbGciOiJSUzI1NiIsImtpZCI6IjcxOGY0ZGY5MmFkMTc1ZjZhMDMwN2FiNjVkOGY2N2YwNTRmYTFlNWYiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiQ2hhZ2FudGkgIFJlZGR5IiwiaXNzIjoiaHR0cHM6Ly9zZWN1cmV0b2tlbi5nb29nbGUuY29tL2V4YTItZmIxNzAiLCJhdWQiOiJleGEyLWZiMTcwIiwiYXV0aF90aW1lIjoxNzI5NDkxMTY2LCJ1c2VyX2lkIjoiZGhFNEd1SldoUlZwM0M3Q3JxWk8yYjJ4dGtnMiIsInN1YiI6ImRoRTRHdUpXaFJWcDNDN0NycVpPMmIyeHRrZzIiLCJpYXQiOjE3Mjk0OTQwNzQsImV4cCI6MTcyOTQ5NzY3NCwiZW1haWwiOiJjaGFnYW50aXZlbmthdGFyYW1pcmVkZHkxQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJmaXJlYmFzZSI6eyJpZGVudGl0aWVzIjp7ImVtYWlsIjpbImNoYWdhbnRpdmVua2F0YXJhbWlyZWRkeTFAZ21haWwuY29tIl19LCJzaWduX2luX3Byb3ZpZGVyIjoicGFzc3dvcmQifX0.nBrwYOnQvc87QZxrIsH0YAuscaqPixgvvNfkzYu-ENc_fmAf1dVlO7Ce4T_GxTIl1yjwQz7hubzgfWSOrNe5ol9fQdAFkdhh8KFFNmMlqw5AdjZ_-rrvIkXkIVdkCb0RfBuh3zJF7QYAy0diPQMVwCwftCG_sCDsDHI7VQw_oA9nh_TuPfS8ZHbzdlzhfUFphm6K7i7M4jAhCRcqcY_SCMRH6h9Gx6lzlFrAyjQqJDVzk6USaeeBwDF2kUSw4_zdLh67H2kkiHKOrpitWMQ4iXa1CvH_COlJjiu26T-EokZBitVextI0OFz9z6qqhm1V78E9LWWY5ggFrhA4ppuDXw","recorded":"2024-10-21 12:31:16.382651156","filePath":"null","pinned":false},{"value":"e","recorded":"2024-10-21 12:28:49.982735639","filePath":"null","pinned":false},{"value":"vim.g.codeium_enabled = false","recorded":"2024-10-21 12:23:42.739653911","filePath":"null","pinned":false},{"value":"config = function ()\n    -- Change '\u003cC-g\u003e' here to any keycode you like.\n    vim.keymap.set('i', '\u003cC-g\u003e', function () return vim.fn['codeium#Accept']() end, { expr = true, silent = true })\n    vim.keymap.set('i', '\u003cc-;\u003e', function() return vim.fn['codeium#CycleCompletions'](1) end, { expr = true, silent = true })\n    vim.keymap.set('i', '\u003cc-,\u003e', function() return vim.fn['codeium#CycleCompletions'](-1) end, { expr = true, silent = true })\n    vim.keymap.set('i', '\u003cc-x\u003e', function() return vim.fn['codeium#Clear']() end, { expr = true, silent = true })\n  end","recorded":"2024-10-21 12:23:03.084152391","filePath":"null","pinned":false},{"value":"vim.g.codeium_disable_bindings = 1","recorded":"2024-10-21 12:22:42.184635808","filePath":"null","pinned":false},{"value":"Clear current suggestion \tcodeium#Clear() \t\u003cC-]\u003e\nNext suggestion \tcodeium#CycleCompletions(1) \t\u003cM-]\u003e\nPrevious suggestion \tcodeium#CycleCompletions(-1) \t\u003cM-[\u003e\nInsert suggestion \tcodeium#Accept() \t\u003cTab\u003e\nManually trigger suggestion \tcodeium#Complete() \t\u003cM-Bslash\u003e\nAccept word from suggestion \tcodeium#AcceptNextWord() \t\u003cC-k\u003e\nAccept line from suggestion \tcodeium#AcceptNextLine() \t\u003cC-l\u003e","recorded":"2024-10-21 12:21:06.836901678","filePath":"null","pinned":false},{"value":"{\n  'Exafunction/codeium.vim',\n  event = 'BufEnter'\n}","recorded":"2024-10-21 12:20:15.441259524","filePath":"null","pinned":false},{"value":"cursor.sh","recorded":"2024-10-21 11:42:12.978260554","filePath":"null","pinned":false},{"value":"workspace_swipe_direction_lock","recorded":"2024-10-21 11:39:14.431692895","filePath":"null","pinned":false},{"value":"Institute","recorded":"2024-10-20 17:03:37.368255083","filePath":"null","pinned":false},{"value":"Technology","recorded":"2024-10-20 17:03:32.239256667","filePath":"null","pinned":false},{"value":"DR. B.R. AMBEDKAR NATIONAL INSTITUTE OF TECHNOLOGY","recorded":"2024-10-20 17:03:16.693009910","filePath":"null","pinned":false},{"value":"Department of Computer Science \\\u0026 Engineering","recorded":"2024-10-20 17:03:05.839678702","filePath":"null","pinned":false},{"value":"\\address[3]{Faculty of Sciences and Mathematics, University of Ni\\v s, Vi\\v segradska 33, 18000 Ni\\v s, Serbia}","recorded":"2024-10-20 17:02:48.931873839","filePath":"null","pinned":false},{"value":"/mnt/Windows/Documents and Settings/chaga/Downloads/Transfer/(58) Evaluating Web Metrics to Enhance Web Page Quality.pptx","recorded":"2024-10-20 14:38:36.203395895","filePath":"null","pinned":false},{"value":"/mnt/Windows/Documents and Settings/chaga/Downloads/Transfer/(166) Blockchain-Driven Roundabout Production Network.pptx","recorded":"2024-10-20 14:38:25.752314342","filePath":"null","pinned":false},{"value":"Mukesh","recorded":"2024-10-19 19:05:29.731382357","filePath":"null","pinned":false},{"value":"Assistant Professor","recorded":"2024-10-19 19:05:20.119179976","filePath":"null","pinned":false},{"value":"Aman Chandra Kaushik","recorded":"2024-10-19 19:01:17.834488447","filePath":"null","pinned":false},{"value":"+86-15618987739","recorded":"2024-10-19 19:01:03.863543452","filePath":"null","pinned":false},{"value":"Jiangsu,","recorded":"2024-10-19 19:00:22.145628204","filePath":"null","pinned":false},{"value":"No. 1800, Li Lake Avenue","recorded":"2024-10-19 19:00:17.164427989","filePath":"null","pinned":false},{"value":"School of Medicine, Jiangnan University","recorded":"2024-10-19 19:00:08.021786114","filePath":"null","pinned":false},{"value":"Rakesh Prasad","recorded":"2024-10-19 18:57:45.502644311","filePath":"null","pinned":false},{"value":"Jeedimetla ","recorded":"2024-10-19 18:52:47.846436967","filePath":"null","pinned":false},{"value":"Bahadurpally","recorded":"2024-10-19 18:52:41.152402121","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Rakesh.pdf","recorded":"2024-10-19 18:42:36.771026937","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Rakesh","recorded":"2024-10-19 18:42:19.481414719","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/LoR - Venkat_Rakesh.pdf","recorded":"2024-10-19 18:41:46.134210945","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/TAMU/LoR - Venkat_Rakesh.pdf","recorded":"2024-10-19 18:39:33.254839670","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Mann.pdf","recorded":"2024-10-19 18:38:17.047426300","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Mann","recorded":"2024-10-19 18:37:48.010101730","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Aman.pdf","recorded":"2024-10-19 18:37:05.807854852","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Aman","recorded":"2024-10-19 18:36:59.564463678","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/LOR/Aman_Sir_lor.docx\n/mnt/Karna/Git/Masters-Documents/Applications/LOR/Mann_Recommendation_Letter.pdf\n/mnt/Karna/Git/Masters-Documents/Applications/LOR/Rakesh_Recommendation_Letter.pdf","recorded":"2024-10-19 18:35:00.019725221","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Venkat LoR - University of Maryland_Aman.docx\n/home/karna/Downloads/Venkat LoR - University of Maryland_Aman.pdf\n/home/karna/Downloads/Venkat LoR - University of Maryland_Mann.docx\n/home/karna/Downloads/Venkat LoR - University of Maryland_Mann.pdf\n/home/karna/Downloads/Venkat LoR - University of Maryland_Rakesh.docx\n/home/karna/Downloads/Venkat LoR - University of Maryland_Rakesh.pdf","recorded":"2024-10-19 18:34:32.765742089","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Maryland_Mukesh","recorded":"2024-10-19 18:32:38.655115174","filePath":"null","pinned":false},{"value":"Mentoring Venkat on a major project allowed me to observe his exceptional technical acumen and research skills, which have consistently impressed me.  His projects demonstrated his deep understanding of the subject matter and the ability to implement complex methodologies. His initiative to submit this work to a reputed journal reflects his dedication and ambition to contribute to bioinformatics and drug development.\n\nOne of Venkat‚Äôs most admirable qualities is his versatility in working independently and within a team. His leadership skills were evident as he effectively coordinated with team members, yet he excelled when tasked with individual responsibilities, ensuring timely and high-quality output in both scenarios. His problem-solving abilities, particularly in improving model performance and integrating diverse datasets, set him apart as a proactive and analytical thinker.\n\nThroughout our collaboration, I have seen Venkat grow immensely in tackling interdisciplinary challenges, combining his machine learning and computational biology expertise to devise innovative solutions. His proficiency in transfer learning techniques and meticulous attention to enhancing model accuracy underscore his technical depth and analytical mindset. While he has shown remarkable progress, further strengthening his statistical foundation will amplify his research potential.","recorded":"2024-10-19 18:29:02.817881970","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/UMD/Venkat LoR - University of Maryland_Aman.tex","recorded":"2024-10-19 18:25:47.485855962","filePath":"null","pinned":false},{"value":"Venkat LoR - University of Massachusetts Amherst_Aman","recorded":"2024-10-19 18:25:27.759218536","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/UMD/leadership.tex","recorded":"2024-10-19 18:23:42.862162284","filePath":"null","pinned":false},{"value":"https://www.kaggle.com/competitions/npci-credit-card-default-risk-analysis/leaderboard","recorded":"2024-10-19 17:26:31.214129086","filePath":"null","pinned":false},{"value":"https://github.com/Chaganti-Reddy/CJPR-Report.git","recorded":"2024-10-19 16:20:01.346870626","filePath":"null","pinned":false},{"value":"git@github.com:Chaganti-Reddy/CJPR-Report.git","recorded":"2024-10-19 16:19:58.597453011","filePath":"null","pinned":false},{"value":"Hyderabad@2003","recorded":"2024-10-19 13:23:16.002307804","filePath":"null","pinned":false},{"value":"venkataramireddychaganti41@gmail.com","recorded":"2024-10-19 13:23:09.066026227","filePath":"null","pinned":false},{"value":"VEN3150927","recorded":"2024-10-19 12:57:25.455648729","filePath":"null","pinned":false},{"value":"graduate@rice.edu","recorded":"2024-10-19 12:40:38.164596429","filePath":"null","pinned":false},{"value":"mcisnero@rice.edu","recorded":"2024-10-19 12:40:16.589536145","filePath":"null","pinned":false},{"value":"I wanted to let you know that I checked my ASU portal for any unfinished chores after receiving the email below, but I was unable to locate any on my application site. Since I am eager to enroll in the university, could you kindly let me know if there is anything further I need to do to complete the application?","recorded":"2024-10-19 12:13:50.368299329","filePath":"null","pinned":false},{"value":"Just wanted to inform that after receiving the below email, I have checked my ASU portal for any incomplete tasks but couldn't find any in my application portal. So, could you please confirm if anything is pending from myside so that I can finish the application as I am very keen in getting into the university.","recorded":"2024-10-19 12:13:33.715238020","filePath":"null","pinned":false},{"value":"Dear Sir/Madam,\n\nHope this email finds you well\n\n    Just wanted to inform that after receiving the below email, I have checked my ASU portal for any incomplete tasks but couldn't find any in my application portal. So, could you please confirm if anything is pending from myside so that I can finish the application as I am very keen in getting into the university.\n\nThanks \u0026 Regards\nVenkatarami Reddy Chaganti\n\n---------- Forwarded message ---------\nFrom: Chandan on behalf of ASU \u003casu@mail.kaplanpathways.com\u003e\nDate: Fri, Oct 18, 2024 at 6:34‚ÄØPM\nSubject: Your ASU application is incomplete\nTo: \u003cvenkataramireddychaganti41@gmail.com\u003e\n\n\nView in browser\nKAPLAN INTERNATIONAL IN PARTNERSHIP WITH\nVenkatarami Reddy Chaganti, your ASU application needs further action\nYour application details\nCampus: ASU Tempe campus\n\t\nStart Date: August 2025\nWe have submitted your application to Arizona State University (ASU), but there are still some tasks you need to complete before it can be evaluated.\nYour application is incomplete\nYour application is currently marked as ‚Äúincomplete.‚Äù To find out why, you‚Äôll need to log into your MyASU student portal.\nCheck MyASU student portal\nPlease complete any outstanding tasks as soon as possible. ASU will not consider you for admission until your application is complete. \n\nIf you have recently completed your application, you can ignore this message.\nBy completing your application, you‚Äôre taking the first ‚Äî and most important ‚Äî step toward joining the inspiring community of 11,000+ international students at ASU.  \n\nWhile you complete your application, check out this video on why Phoenix is a fantastic study destination. \nwhy Connecticut is a fantastic destination to study abroad. \nIf you have any questions, please contact us by replying to this email, or reach out to your agent, IMFS KP Singh education Services pvt ltd. To ensure a quick response, one of my helpful colleagues may reply.\nBest wishes,\n\nChandan Sharma\nDirector of Application Management, US\nKaplan International on behalf of Arizona State University\nABOUT US\nWe are Kaplan International. We help students follow their path to leading universities across the world. We offer access to exceptional teaching, thousands of degrees and deeply rewarding experiences.\n\nKaplan International works in partnership with Arizona State University to provide application counselling and admissions support to international students.\n\nIn the UK, Kaplan International Pathways is the trading name of Kaplan International Colleges UK Ltd.Company No. 05268303. Registered in England. Registered office: Palace House, 3 Cathedral Street London, SE1 9DE, United Kingdom.\n\t\nYOUR SUBSCRIPTION DETAILS\nVenkatarami Reddy Chaganti, you are receiving this email because you have submitted an application to study at Arizona State University.\n\nWe need to keep you informed throughout your application and admission process with important email updates. You can update your details or manage your subscription. Alternatively, you can unsubscribe if you no longer want to receive these updates.\nPrivacy Policy\nKaplan International 2024. All rights reserved.\nFacebook\nTwitter\nInstagram\nYouTube\nLinkedIn\n","recorded":"2024-10-19 12:12:41.420710595","filePath":"null","pinned":false},{"value":"asu@mail.kaplanpathways.com","recorded":"2024-10-19 12:07:06.719742161","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/aco-main\n/home/karna/Downloads/Files/CP\n/home/karna/Downloads/Files/Git\n/home/karna/Downloads/Files/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main\n/home/karna/Downloads/Files/Pratik Project GAN Generate lung images\n/home/karna/Downloads/Files/Reinforcement-learning-approach-for-prognosis-in-ICU\n/home/karna/Downloads/Files/aco-main.zip\n/home/karna/Downloads/Files/Bus_Routing_Problems__June_06th__2024_.pdf\n/home/karna/Downloads/Files/format.pptx\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-17 17:52:46.816018517","filePath":"null","pinned":false},{"value":"Karna","recorded":"2024-10-17 17:52:29.994697881","filePath":"null","pinned":false},{"value":"Backup","recorded":"2024-10-17 17:46:10.904625718","filePath":"null","pinned":false},{"value":"/run/media/karna/Xtras/EndeavourOS_Endeavour_neo-2024.09.22.iso","recorded":"2024-10-17 17:32:32.021445202","filePath":"null","pinned":false},{"value":"/mnt/Karna/aco-main\n/mnt/Karna/CP\n/mnt/Karna/Git\n/mnt/Karna/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main\n/mnt/Karna/Pratik Project GAN Generate lung images\n/mnt/Karna/Reinforcement-learning-approach-for-prognosis-in-ICU\n/mnt/Karna/aco-main.zip\n/mnt/Karna/Bus_Routing_Problems__June_06th__2024_.pdf\n/mnt/Karna/format.pptx\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-17 17:30:49.088953163","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/chaganti-reddy.github.io/static/uploads/resume.tex","recorded":"2024-10-17 12:33:52.255110051","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Books/Quantum Computing/Griffiths - Introduction to quantum mechanics.pdf","recorded":"2024-10-17 12:31:59.750544890","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Evaluating_Web_Metrics_for_Enhancing_WebPage_Quality.pptx\n/home/karna/Downloads/venkat_blockchain_6321.pptx","recorded":"2024-10-17 12:31:09.426441434","filePath":"null","pinned":false},{"value":"haskell-skylighting-format-latex-0.1-135-x86_64                                       29.8 KiB   109 KiB/s 00:00 [--------------------------------------------------------------------] 100%","recorded":"2024-10-17 12:11:28.291771970","filePath":"null","pinned":false},{"value":"haskell-hslua-marshalling-2.3.1-13-x86_64","recorded":"2024-10-17 11:55:35.322241289","filePath":"null","pinned":false},{"value":"haskell-indexed-traversable-0.1.4-4-x86_64","recorded":"2024-10-17 11:55:34.657888545","filePath":"null","pinned":false},{"value":"haskell-fast-logger-3.1.2-85-x86_64","recorded":"2024-10-17 11:55:33.917732938","filePath":"null","pinned":false},{"value":"haskell-hslua-module-doclayout-1.1.0-70-x86_64","recorded":"2024-10-17 11:55:32.836506099","filePath":"null","pinned":false},{"value":"                                       73.6 KiB   116 KiB/s 00:01 [--------------------------------------------------------------------] 100%\n haskell-fas","recorded":"2024-10-17 11:55:31.438995978","filePath":"null","pinned":false},{"value":"haskell-hslua-module-docla","recorded":"2024-10-17 11:55:30.101757100","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/HITA2024/Blockchain-driven Roundabout Production Network/Blockchain_empowered_Roundabout_Production_Network_in_the_Agri-food_Supply_Chain_for_Spanning_Trust_T.pptx","recorded":"2024-10-16 20:38:56.068859882","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/HITA2024/Empirical Validation on Web Pages/Evaluating_Web_Metrics_for_Enhancing_WebPage_Quality_.pptx","recorded":"2024-10-16 20:38:51.148275063","filePath":"null","pinned":false},{"value":"/mnt/Karna/format.pptx","recorded":"2024-10-16 20:37:49.806377121","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/format.pptx","recorded":"2024-10-16 20:37:45.510997482","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/aco-main\n/home/karna/Downloads/Files/CP\n/home/karna/Downloads/Files/Git\n/home/karna/Downloads/Files/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main\n/home/karna/Downloads/Files/Pratik Project GAN Generate lung images\n/home/karna/Downloads/Files/Reinforcement-learning-approach-for-prognosis-in-ICU\n/home/karna/Downloads/Files/aco-main.zip\n/home/karna/Downloads/Files/Bus_Routing_Problems__June_06th__2024_.pdf\n/home/karna/Downloads/Files/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-16 20:37:19.476187140","filePath":"null","pinned":false},{"value":"Windows","recorded":"2024-10-16 20:35:35.566081274","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/Git/HITA2024/Blockchain-driven Roundabout Production Network/Blockchain_empowered_Roundabout_Production_Network_in_the_Agri-food_Supply_Chain_for_Spanning_Trust_T.pptx","recorded":"2024-10-16 16:42:48.866499802","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/Git/HITA2024/Empirical Validation on Web Pages/Evaluating_Web_Metrics_for_Enhancing_WebPage_Quality_.pptx","recorded":"2024-10-16 16:42:41.637230516","filePath":"null","pinned":false},{"value":"This study focuses on evaluating key web metrics to enhance the quality of web pages.","recorded":"2024-10-16 16:39:15.007246002","filePath":"null","pinned":false},{"value":"overview","recorded":"2024-10-16 16:33:46.897316240","filePath":"null","pinned":false},{"value":"This study focuses on evaluating key web metrics to enhance the quality of web pages.\nData is sourced from 600 websites nominated for the Webby Awards between 2017 and 2022.\n\nKey performance and quality metrics include:\nSpeed Index\nTotal Blocking Time\nTime to Interactive (TTI)\nFirst Contentful Paint (FCP)\n\nA Python-based automated tool was developed to analyze:\n16 quality measures\n6 performance indicators\n\nWebsites were categorized into high and low quality based on these metrics.\n\nThe findings offer insights into optimizing web design and improving user experiences on online platforms.","recorded":"2024-10-16 14:41:34.168033252","filePath":"null","pinned":false},{"value":"üì∑ 109676-797521221.png","recorded":"2024-10-16 14:40:19.840872922","filePath":"/home/karna/.config/clipse/tmp_files/109676-797521221.png","pinned":false},{"value":"üì∑ 8528-415532030.png","recorded":"2024-10-16 14:39:35.420247983","filePath":"/home/karna/.config/clipse/tmp_files/8528-415532030.png","pinned":false},{"value":"\u003cmeta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"\u003e\u003cimg src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRSSl4axzcEv9vzUQU6zJqcaknKrYeyBb7QYQ\u0026amp;s\" class=\"sFlh5c FyHeAf\" alt=\"Mahindra University | Hyderabad\" jsname=\"JuXqh\" style=\"max-width:761px;\" data-ilt=\"1729069761536\"\u003e","recorded":"2024-10-16 14:39:35.416280525","filePath":"null","pinned":false},{"value":"Mahindra University,","recorded":"2024-10-16 14:38:06.470891760","filePath":"null","pinned":false},{"value":"Rakesh Prasad Badoni","recorded":"2024-10-16 14:37:30.403429629","filePath":"null","pinned":false},{"value":"Evaluating Web Metrics for Enhancing Web Page Quality","recorded":"2024-10-16 14:29:28.853951628","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Files/Internships","recorded":"2024-10-15 13:16:32.871616056","filePath":"null","pinned":false},{"value":"üì∑ 4412-465009130.png","recorded":"2024-10-15 10:55:35.468435878","filePath":"/home/karna/.config/clipse/tmp_files/4412-465009130.png","pinned":false},{"value":"https://catalog.arizona.edu/programs/COSCMS","recorded":"2024-10-15 10:55:35.466170212","filePath":"null","pinned":false},{"value":"üì∑ 3962-572380516.png","recorded":"2024-10-15 10:55:24.575993323","filePath":"/home/karna/.config/clipse/tmp_files/3962-572380516.png","pinned":false},{"value":"https://degrees.apps.asu.edu/masters-phd/major/ASU00/ESCSEBDMS/computer-science-big-data-systems-ms","recorded":"2024-10-15 10:55:24.573466893","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/Win10_22H2_EnglishInternational_x64v1.iso","recorded":"2024-10-14 19:10:15.445627020","filePath":"null","pinned":false},{"value":"/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/boot\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/efi\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/sources\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/support\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/autorun.inf\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/bootmgr\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/bootmgr.efi\n/run/media/karna/CCCOMA_X64FRE_EN-GB_DV9/setup.exe","recorded":"2024-10-14 18:59:16.981858978","filePath":"null","pinned":false},{"value":"exo-open","recorded":"2024-10-14 18:53:55.910158476","filePath":"null","pinned":false},{"value":"TerminalEmulator","recorded":"2024-10-14 18:53:06.282987859","filePath":"null","pinned":false},{"value":"afc://00008120-0010246C11E8201E:3/org.mozilla.ios.Firefox/Downloads/www.5MovieRulz.top%20-%20Mangalavaaram%20(2023)%201080p%20Telugu%20HQ%20HDRip%20-%20HEVC%20-%20%20(DD%205.1%20-%20192kbps%20_%20AAC)%20-%201.8GB%20-%20ESub.mkv.zip","recorded":"2024-10-14 18:48:38.054554017","filePath":"null","pinned":false},{"value":"/mnt/Karna/aco-main\n/mnt/Karna/CP\n/mnt/Karna/Git\n/mnt/Karna/ISRO\n/mnt/Karna/Motor Imagery Classification Performance Enhancement with EEG Data Augmentation\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main\n/mnt/Karna/Pratik Project GAN Generate lung images\n/mnt/Karna/Reinforcement-learning-approach-for-prognosis-in-ICU\n/mnt/Karna/1.mp3\n/mnt/Karna/acknowledgementSlip_S1858416595000.pdf\n/mnt/Karna/aco-main.zip\n/mnt/Karna/Oral-cancer-detection-using-deep-learning-main.zip","recorded":"2024-10-14 18:45:39.154727189","filePath":"null","pinned":false},{"value":"/run/media/karna/Xtras/Win10_22H2_EnglishInternational_x64v1.iso","recorded":"2024-10-14 18:45:22.361748592","filePath":"null","pinned":false},{"value":"#!/bin/bash\nnotify_levels=(3 5 10 20)\nBAT=$(ls /sys/class/power_supply |grep BAT |head -n 1)\nlast_notify=100\n\nwhile true; do\n    bat_lvl=$(cat /sys/class/power_supply/${BAT}/capacity)\n    if [ $bat_lvl -gt $last_notify ]; then\n            last_notify=$bat_lvl\n    fi\n    for notify_level in ${notify_levels[@]}; do\n        if [ $bat_lvl -le $notify_level ]; then\n            if [ $notify_level -lt $last_notify ]; then\n                notify-send -u critical \"Low Battery\" \"$bat_lvl% battery remaining.\"\n                last_notify=$bat_lvl\n            fi\n        fi\n    done\nsleep 60\ndone","recorded":"2024-10-11 14:07:39.939515930","filePath":"null","pinned":false},{"value":"/mnt/Karna/Pratik Project GAN Generate lung images/archive.zip","recorded":"2024-10-11 14:04:00.079000123","filePath":"null","pinned":false},{"value":"exec-once = numlockx on \u0026\n","recorded":"2024-10-11 11:12:16.372495951","filePath":"null","pinned":false},{"value":"AA2005233711@@","recorded":"2024-10-10 18:03:28.843983252","filePath":"null","pinned":false},{"value":"ramchaganti200@gmail.com","recorded":"2024-10-10 18:03:24.922993123","filePath":"null","pinned":false},{"value":"https://leetcode.com/discuss/interview-question/5886397/DSA-Patterns-you-need-to-know-!!!","recorded":"2024-10-10 18:01:00.267913418","filePath":"null","pinned":false},{"value":"Minor_Project_Generating_Lung_Images_GAN_network","recorded":"2024-10-10 17:46:32.731696132","filePath":"null","pinned":false},{"value":"            // Bluetooth Devices Bluetooth icon \n","recorded":"2024-10-10 17:30:19.699745624","filePath":"null","pinned":false},{"value":"            \n","recorded":"2024-10-10 17:30:18.833974826","filePath":"null","pinned":false},{"value":"            \"Bluetooth Devices\": \"\u003cspan foreground='#a6adc8'\u003eÛ∞§Ø \u003c/span\u003e Bluetooth Devices\",\n","recorded":"2024-10-10 17:30:17.426970677","filePath":"null","pinned":false},{"value":"            \"(.*)Bluetooth Devices\": \"\u003cspan foreground='#a6adc8'\u003eÛ∞§Ø \u003c/span\u003e $1\",\n","recorded":"2024-10-10 17:30:16.423901188","filePath":"null","pinned":false},{"value":"            \"(.*) - Bluetooth Devices\": \"\u003cspan foreground='#f38ba8'\u003eÛ∞ÖÇ \u003c/span\u003e $1\",\n","recorded":"2024-10-10 17:28:37.921056718","filePath":"null","pinned":false},{"value":"            \"Bluetooth Devices\": \"\u003cspan foreground='#f38ba8'\u003eÛ∞ÖÇ \u003c/span\u003e Bluetooth Devices\",\n","recorded":"2024-10-10 17:28:37.615069268","filePath":"null","pinned":false},{"value":"        \"custom/kernelinfo\",\n","recorded":"2024-10-10 17:23:27.113485597","filePath":"null","pinned":false},{"value":"Generating Radiologically Realistic Lung Images\nwith Generative Adversarial Networks","recorded":"2024-10-10 16:20:58.922037573","filePath":"null","pinned":false},{"value":"train","recorded":"2024-10-10 16:10:55.268502000","filePath":"null","pinned":false},{"value":"img_width, img_height = 64, 64\nchannels = 1  # Grayscale\n\ndataset_dir = \"/content/drive/My Drive/Lung dataset/archive\"\n!ls \"/content/drive/My Drive/Lung dataset/archive\"\n# Load and preprocess images from the dataset\nnormal_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/NORMAL\"), img_width, img_height)\npneumonia_images = load_images(os.path.join(dataset_dir, \"chest_xray/train/PNEUMONIA\"), img_width, img_height)","recorded":"2024-10-10 16:06:31.174340605","filePath":"null","pinned":false},{"value":"archive","recorded":"2024-10-10 16:04:57.653750846","filePath":"null","pinned":false},{"value":"Lung dataset","recorded":"2024-10-10 16:04:48.029545501","filePath":"null","pinned":false},{"value":"Reviewer #1: The authors could provide good work. However, there are some concerns to be resolved.","recorded":"2024-10-10 16:01:56.372229701","filePath":"null","pinned":false},{"value":"The abstract needs to be improved. The first sentence in the abstract, it is necessary for the authors to add a sentence to describe the problem or motivation to focus on this topic. The second sentence should provide the literature gap. In the third sentence, the authors should say what you are doing, and then provide the empirical findings. Finally, the significance of the finding should be offered.","recorded":"2024-10-10 16:01:50.999127571","filePath":"null","pinned":false},{"value":"This study addresses the vehicle routing problem (VRP) en-\ncountered by transportation bus service providers with the goal of opti-\nmizing their bus routes. The hypothesis posits that each vehicle should\nreach a pickup point, considered a boarding location, before proceed-\ning to the designated destination. At each pickup point, a list of one or\nmore passengers departing from that specific location exists. The primary\nobjective of this study is to minimize the overall transportation cost by\nenhancing routing efficiency while accounting for application constraints,\ncapacity limitations, and time constraints. To achieve this, we propose a\nmathematical model encompassing various boarding and dropping points\nwith diverse time periods to enhance vehicle routing. The subsequent\nphase focuses on the ant colony optimization algorithm, addressing the\nproblem comprehensively within its expansive scope. Finally, to validate\nthe effectiveness of the proposed algorithm, we conduct tests on real-\nworld data to ascertain its practical viability.","recorded":"2024-10-10 16:01:11.156488946","filePath":"null","pinned":false},{"value":"/home/karna/Downloads/LOR check list .docx","recorded":"2024-10-10 15:58:11.341078197","filePath":"null","pinned":false},{"value":"The strength of Mr. Venkatarami Reddy lies in his ability to adapt and embed his knowledge into various real world problems with his research ability. The same has done under my guidance as well, embedded machine learning in predicting protein structures using Deep Learning techniques. One of the most important and unique skill that I had observed is his ability to work independently as well as working with team. He can complete the work on time in both the scenario, which resembles his team work and leadership.\n\tYes, his work on protein structure prediction using deep learning, specifically with CNNs and transformer-based models, would certainly stand out. He achieved high precision and accuracy which has practical applications in bioinformatics and drug development. His initiative to submit this work to a prestigious journal further underscores his dedication to advancing knowledge in the field.\n\tI would highlight Venkatarami‚Äôs problem-solving skills, as demonstrated by his ability to improve model performance and integrate complex datasets. \n\tI have seen significant growth in Venkatarami‚Äôs ability to tackle interdisciplinary problems, blending his knowledge of machine learning with computational biology. His technical depth and understanding of advanced models have matured throughout our collaboration, as evidenced by his innovative solutions to complex biological challenges.\n\tStrengthening his statistical knowledge would further enhance his research capabilities, especially in fields requiring rigorous validation and testing.\nCertainly, his proficiency in implementing and optimizing advanced machine learning models, along with his ability to apply techniques like transfer learning, highlights his strong technical and problem-solving skills. He paid close attention to detail in improving model performance and accuracy, which reflects his analytical mindset.","recorded":"2024-10-10 14:58:16.623405641","filePath":"null","pinned":false},{"value":"    11. Can you recall any specific examples of the student's strengths in your class or projects?\n\t\n\tThe strength of Mr. Venkatarami Reddy lies in his ability to adapt and embed his knowledge into various real world problems with his research ability. The same has done under my guidance as well, embedded machine learning in predicting protein structures using Deep Learning techniques.\n\n    12. What qualities do you think graduate admissions committees value most, and how does the student compare?\n\n\tOne of the most important and unique skill that I had observed is his ability to work independently as well as working with team. He can complete the work on time in both the scenario, which resembles his team work and leadership.\n \n    13. Are there any examples of the student's work that you believe would stand out to an admissions committee?\n\t\n\tYes, his work on protein structure prediction using deep learning, specifically with CNNs and transformer-based models, would certainly stand out. He achieved high precision and accuracy which has practical applications in bioinformatics and drug development. His initiative to submit this work to a prestigious journal further underscores his dedication to advancing knowledge in the field.\n\n    14. What specific traits, like problem-solving or teamwork, would you highlight in the student's recommendation?\n\n\tI would highlight Venkatarami‚Äôs problem-solving skills, as demonstrated by his ability to improve model performance and integrate complex datasets. \n\n    15. Where have you seen the most academic or professional growth in the student?\n\n\tI have seen significant growth in Venkatarami‚Äôs ability to tackle interdisciplinary problems, blending his knowledge of machine learning with computational biology. His technical depth and understanding of advanced models have matured throughout our collaboration, as evidenced by his innovative solutions to complex biological challenges.\n\n    16. What areas should the student work on before starting this program?\n\n\tStrengthening his statistical knowledge would further enhance his research capabilities, especially in fields requiring rigorous validation and testing.\n\n    17. Would you be able to provide examples to support the key skills the student plans to emphasize in their application?\n\nCertainly, his proficiency in implementing and optimizing advanced machine learning models, along with his ability to apply techniques like transfer learning, highlights his strong technical and problem-solving skills. He paid close attention to detail in improving model performance and accuracy, which reflects his analytical mindset. ","recorded":"2024-10-10 14:57:24.794451561","filePath":"null","pinned":false},{"value":"The strength of Mr. Venkatarami Reddy lies in his ability to adapt and embed his knowledge into various real world problems with his research ability. The same has done under my guidance as well, embedded machine learning in predicting protein structures using Deep Learning techniques.","recorded":"2024-10-10 14:57:19.428796595","filePath":"null","pinned":false},{"value":"I've known Mr. Venkatarami Reddy Chaganti for the past 9 months. He worked as a research assistant under my supervision. I asked him to work on a research topic of his interest and he choose AI and it's working in various fields. \nHe chose the project titled \"Court Judgement Prediction and Recommendation\"\nBeing impressed by his research work, I gave him an opportunity to work as a full time research assistant under my guidance.\n\nDuring his project I noticed that he has always been very passionate about learning new things and tried implementing them. He always completed the task in the given time. He has an eye to detail and is fastidious about the results for every minor task that he does.\n\nOne of the most important and unique quality that I've observed in Venkat Is time management and his arduous zeal to finish the given task on time with accuracy. \n\nExamples: 1) Submission of multiple manuscripts on time for the conference \n2) Conducting research and creating pipelines accurately and precisely in given time.\n\nHe has submitted 4 research manuscripts within this span of 9-10 months and Not just restricted to a single domain, he worked on various domains and he  always wants to understand the usage of a domain in multiple domains, such as incorporating AI in law and Reinforcement learning in traffic signals and also AI In Computational Biology.  This shows his quest to knowledge and consistency which makes him unique.\n\nIn this tenure there has always been an exponential growth in both his academic performance and research work managing both equally.\n\nHe always wants to take the research work to the next level in an advanced way , \nbut due to limited resources \u0026 limited guidance he faces small obstacles that hinders him from achieving great things.\n\nI believe Computer Science course work at your esteemed institution and advanced facilities helps him to hone his skills.","recorded":"2024-10-10 14:33:58.874633602","filePath":"null","pinned":false},{"value":"I take the opportunity to write this letter to support Mr. Venkatarami Reddy Chaganti to pursue his master‚Äôs at your esteemed university. I am a professor at the Computer Science department at Indian Institute of Information Technology Sonepat. I have taught several students in my 9 years of experience, and I confidently say that Venkat is one of the brightest among them. I have known him for the past 4 years, during which I taught him Computer Programming, DBMS, Data Structures, Software Engineering, Soft Computing. I have also mentored him during his Smart Indian Hackathon 2023 and two of his major final year projects. During this time, I observed Venkat picking a team of students with relevant skills to get the job done. This unique skill, combined with his academic brilliance, helped him complete the projects quickly and efficiently. \n\nFrom an academic perspective, Venkat is a gold medalist and a brilliant student with innate curiosity and a willingness to understand academic concepts by any means possible. These qualities helped him rank in the top 2% of his batch. This is a testament to his high academic caliber. I've seen Venkat be well prepared for lectures‚Äîcompleting pre-reads, being up to speed with course content, and completing assignments on time. His enthusiasm towards the subject has always made the class more interactive as he would ask questions and introduce discussions. His interpersonal skills allow him to get along with his peers. He illustrated eminent verbal articulation in the project seminar, which was an integral part of the course.\n\nDuring his second year he started AI \u0026 ML club.  His main intention was to share the profound knowledge that he has on AI \u0026 ML to his peers and his juniors. Initially, he did not get facilitated with enough resources \u0026 support. However, with an unwavering determination and commitment he formed a club \u0026 actively participated and engaged all the members of the club to participate in many hackathons. Not just restricted to active participation. He made his team win some of them. This shows his team work \u0026 excellent leadership skills. During these 4 years I've noticed he has developed overall not just academically but also personally. I had the privilege to recommend him to multiple internship positions.\n\nObserving Venkat over four years, I firmly believe that he has the potential to succeed in all of his future endeavors, both academically and professionally. His academic knowledge, perseverance, and attitude will ensure his ascent to greater heights. I am confident that he/she will be a valuable asset to your cohort. I would strongly recommend his candidature for the program at your esteemed university.","recorded":"2024-10-10 14:32:06.873296071","filePath":"null","pinned":false},{"value":"Certainly. The student‚Äôs proficiency in implementing and optimizing advanced machine learning models, along with their ability to apply techniques like transfer learning, highlights their strong technical and problem-solving skills. They pay close attention to detail in improving model performance and accuracy, which reflects their analytical mindset. ","recorded":"2024-10-10 14:31:01.220676018","filePath":"null","pinned":false},{"value":"I have seen significant growth in Venkatarami‚Äôs ability to tackle interdisciplinary problems, blending his knowledge of machine learning with computational biology. His technical depth and understanding of advanced models have matured throughout our collaboration, as evidenced by his innovative solutions to complex biological challenges.","recorded":"2024-10-10 14:30:25.875194023","filePath":"null","pinned":false},{"value":"I would highlight Venkatarami‚Äôs problem-solving skills, as demonstrated by his ability to improve model performance and integrate complex datasets. ","recorded":"2024-10-10 14:30:05.042907396","filePath":"null","pinned":false},{"value":"Would you be able to provide examples to support the key skills the student plans to emphasize in their application? Certainly. Venkatarami‚Äôs proficiency in implementing and optimizing deep learning models for biological data, his use of transfer learning, and his attention to detail in model performance metrics are all excellent examples of his technical and problem-solving skills. His leadership in managing a lab and his collaborative efforts with peers demonstrate his ability to work in a team-oriented research environment, which he plans to highlight in his application.","recorded":"2024-10-10 14:29:37.105899628","filePath":"null","pinned":false},{"value":"Strengthening his statistical knowledge would further enhance his research capabilities, especially in fields requiring rigorous validation and testing.","recorded":"2024-10-10 14:28:59.542457827","filePath":"null","pinned":false},{"value":"which has practical applications in bioinformatics and drug development. His initiative to submit this work to a prestigious journal further underscores his dedication to advancing knowledge in the field.","recorded":"2024-10-10 14:28:13.779178443","filePath":"null","pinned":false},{"value":"Yes, his work on protein structure prediction using deep learning, specifically with CNNs and transformer-based models, would certainly stand out. He achieved high precision and ","recorded":"2024-10-10 14:28:00.298402300","filePath":"null","pinned":false},{"value":"    1. Can you recall any specific examples of the student's strengths in your class or projects?\n\n    2. How has the student demonstrated leadership, critical thinking, or problem-solving in your course?\n\n    3. What qualities do you think graduate admissions committees value most, and how does the student compare?\n\n    4. How does the student's academic performance align with the requirements of the program they're applying to?\n \n    5. Are there any examples of the student's work that you believe would stand out to an admissions committee?\n\n    6. What specific traits, like problem-solving or teamwork, would you highlight in the student's recommendation?\n\n    7. Where have you seen the most academic or professional growth in the student?\n\n    8. What areas should the student work on before starting this program?\n\n    9. How do the student's goals align with the work you‚Äôve seen them do in your class?\n\n    10. Would you be able to provide examples to support the key skills the student plans to emphasize in their application?\n","recorded":"2024-10-10 14:19:27.351344754","filePath":"null","pinned":false},{"value":"It is a pleasure to write a recommendation for a student as dynamic and bright as Venkatarami Reddy Chaganti. I have had the privilege of evaluating his work as an advisor for his undergraduate final-year project. I taught him Machine Learning, Deep Learning, and Computational Biology in UG's 3rd and 4th years. ¬†Throughout our association, I have witnessed his unwavering passion for his studies. His academic performance in our college is a testament to his excellence and his potential to excel in the MS in Computer Science.\nVenkatarami is a person who can combine knowledge and diligence to accomplish his goals. He is a keen student with high level of acumen and has a good grasp of his subjects. He is highly motivated and is always on the lookout to learn something new. Venkatarami is receptive to new ideas. He can work well in a team and has been managing the computational biology laboratory proficiently since last two years.\nFurthermore, Venkatarami developed and trained advanced deep learning models using convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformer-based models. He employed techniques like transfer learning and data augmentation to enhance the performance and accuracy of these models. His efforts led to high precision in predicting protein structures, reflecting his deep understanding of machine learning algorithms and computational biology principles.  Venkatarami's final product, notable for its accuracy and user-friendly interface, has significant practical applications in bioinformatics and drug development. He is now preparing to submit it to a prestigious journal, highlighting his commitment to contributing to the field through scholarly research.  \nBased on his excellent academic performance and well-honed interpersonal skills, I am confident that Mr. Venkatarami Reddy Chaganti will not only meet but also surpass your expectations. Therefore, I strongly recommend him for graduate studies at your university and financial assistance in the form of a research assistantship. I am certain he will be a source of pride for your institution.¬†¬†¬†","recorded":"2024-10-10 14:17:42.444380643","filePath":"null","pinned":false},{"value":"sir what about the mail?","recorded":"2024-10-10 14:08:53.810105481","filePath":"null","pinned":false},{"value":"/mnt/Karna/Git/Masters-Documents/Applications/Certificates BTech/CMM.pdf","recorded":"2024-10-10 13:45:20.125020557","filePath":"null","pinned":false},{"value":"ASU ID: 1236373673","recorded":"2024-10-10 13:42:12.437698105","filePath":"null","pinned":false},{"value":"transcripts@asu.edu","recorded":"2024-10-10 13:41:53.969405003","filePath":"null","pinned":false},{"value":"Can you recall any specific examples of the student's strengths in your class or projects?","recorded":"2024-10-10 12:55:57.286988587","filePath":"null","pinned":false},{"value":"    1. Can you recall any specific examples of the student's strengths in your class or projects?\n\n    2. How has the student demonstrated leadership, critical thinking, or problem-solving in your course?\n\n    3. What qualities do you think graduate admissions committees value most, and how does the student compare?\n\n    4. How does the student's academic performance align with the requirements of the program they're applying to?\n \n    5. Are there any examples of the student's work that you believe would stand out to an admissions committee?\n\n    6. What specific traits, like problem-solving or teamwork, would you highlight in the student's recommendation?\n\n    7. Where have you seen the most academic or professional growth in the student?\n\n    8. What areas should the student work on before starting this program?\n\n    9. How do the student's goals align with the work you‚Äôve seen them do in your class?\n\n    10. Would you be able to provide examples to support the key skills the student plans to emphasize in their application?","recorded":"2024-10-10 12:45:11.895827843","filePath":"null","pinned":false},{"value":"known","recorded":"2024-10-10 12:43:00.213948367","filePath":"null","pinned":false},{"value":"I have had the privilege of evaluating his work as an advisor for his undergraduate final-year project.","recorded":"2024-10-10 12:37:03.802921821","filePath":"null","pinned":false},{"value":"I've known Mr. Venkatarami Reddy Chaganti for the past 9 months. He worked as a research assistant under my supervision.","recorded":"2024-10-10 12:36:21.374666062","filePath":"null","pinned":false},{"value":"LOR ‚Äì 2 Mann Sir","recorded":"2024-10-10 12:24:33.253439128","filePath":"null","pinned":false},{"value":"Observing XXXX over four years, I firmly believe that he/she has the potential to succeed in all of his/her future endeavors, both academically and professionally. His/Her academic knowledge, perseverance, and attitude will ensure his/her ascent to greater heights. I am confident that he/she will be a valuable asset to your cohort. I would strongly recommend his/her candidature for the program at your esteemed university.","recorded":"2024-10-10 12:23:27.316832739","filePath":"null","pinned":false},{"value":"During these 4 years I've noticed he has developed overall not just academically but also personally. I had the privilege to recommend him to multiple internship positions.","recorded":"2024-10-10 12:18:19.067274552","filePath":"null","pinned":false},{"value":"This shows his team work \u0026 excellent leadership skills","recorded":"2024-10-10 12:18:13.680667210","filePath":"null","pinned":false},{"value":"During his second year he started AI \u0026 ML club. \nHis main intention was to share the profound knowledge that he has on AI \u0026 ML to his peers and his juniors.\n\nInitially, he did not get facilitated with enough resources \u0026 support.\nHowever, with an unwavering determination and commitment he formed a club \u0026 actively participated and engaged all the members of the club to participate in many hackathons.\nNot just restricted to active participation. He made his team win many hackathons.","recorded":"2024-10-10 12:17:59.516196225","filePath":"null","pinned":false},{"value":"From an academic perspective, Venkat is a gold medalist and a brilliant student with innate curiosity and a willingness to understand academic concepts by any means possible. These qualities helped him rank in the top 5% of his batch. This is a testament to his high academic caliber. I've seen Venkat be well prepared for lectures‚Äîcompleting pre-reads, being up to speed with course content, and completing assignments on time. His enthusiasm towards the subject has always made the class more interactive as he would ask questions and introduce discussions. His interpersonal skills allow him to get along with his peers. He illustrated eminent verbal articulation in the project seminar, which was an integral part of the course.","recorded":"2024-10-10 12:06:38.744252873","filePath":"null","pinned":false},{"value":"I take the opportunity to write this letter to support Mr. Venkatarami Reddy Chaganti to pursue his master‚Äôs at your esteemed university. I am a professor at the Computer Science department at Indian Institute of Information Technology Sonepat. I have taught several students in my 9 years of experience, and I confidently say that Venkat is one of the brightest among them. I have known him for the past 4 years, during which I taught him Computer Programming, DBMS, Data Structures, Software Engineering, Soft Computing. I have also mentored him during his Smart Indian Hackathon 2023 and two of his major final year projects. During this time, I observed Venkat picking a team of students with relevant skills to get the job done. This unique skill, combined with his academic brilliance, helped him complete the projects quickly and efficiently. ","recorded":"2024-10-10 12:04:30.458736208","filePath":"null","pinned":false},{"value":" LOR ‚Äì 1 Rakesh Sir","recorded":"2024-10-10 11:55:23.365785403","filePath":"null","pinned":false},{"value":"Not just restricted to a single domain, he worked on various domains ","recorded":"2024-10-10 11:54:10.384414976","filePath":"null","pinned":false},{"value":"I've known venkat for the past 9 months. He worked as a research assistant under my supervision. I asked him to work on a research topic of his interest and he choose AI and it's working in various fields. \nHe chose the project titled \" \"\nBeing impressed by his research work, I gave him an opportunity to work as a full time research assistant under my guidance.\n\n\nDuring his project I noticed that he has always been very passionate about learning new things and tried implementing them. He always completed the task in the given time. He has an eye to detail and is fastidious about the results for every minor task that he does.\n\nOne of the most important quality that I've observed in VRR\nIs time management and his arduous zeal to finish the given task on time with accuracy. \nEg's\n\n\nHe has submitted 4 research papers on various topics like :\n1.\n2.\n3.\nNot just restricted to a single domain, he worked on various domains \n\nThis shows his quest to knowledge and consistency which makes him unique.\n\nIn this 9 months there has always been an exponential growth in both his academic performance and research work managing both equally.\n\nHe always wants to take the research work to the next level in an advanced way , \nbut due to limited resources \u0026 limited guidance he faces small obstacles that hinders him from achieving great things.\nI believe XYZ course work at your esteemed institution and advanced facilities helps him to hone his skills.","recorded":"2024-10-10 11:46:53.979828630","filePath":"null","pinned":false},{"value":"https://meet.google.com/kju-thvw-pze","recorded":"2024-10-10 10:38:13.959460561","filePath":"null","pinned":false}]}